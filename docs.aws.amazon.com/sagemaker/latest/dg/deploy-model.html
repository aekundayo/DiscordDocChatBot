<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Deploy models for inference - Amazon SageMaker</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="deploy-model" /><meta name="default_state" content="deploy-model" /><link rel="icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="canonical" href="deploy-model.html" /><meta name="description" content="Learn more about how to get inferences from your Amazon SageMaker models and deploy your models for serving inference." /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon SageMaker" /><meta name="guide" content="Developer Guide" /><meta name="abstract" content="Use Internet Monitor to build, train, and host machine learning models in AWS." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="deploy-model.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/deploy-model.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/deploy-model.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/deploy-model.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/deploy-model.html" hreflang="de" /><link rel="alternative" href="deploy-model.html" hreflang="en-us" /><link rel="alternative" href="deploy-model.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/deploy-model.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/deploy-model.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/deploy-model.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/deploy-model.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/deploy-model.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/deploy-model.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/deploy-model.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/deploy-model.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/deploy-model.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/deploy-model.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/deploy-model.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/deploy-model.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/deploy-model.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/deploy-model.html" hreflang="zh-tw" /><link rel="alternative" href="deploy-model.html" hreflang="x-default" /><meta name="feedback-item" content="SageMaker" /><meta name="this_doc_product" content="Amazon SageMaker" /><meta name="this_doc_guide" content="Developer Guide" /><script defer="" src="../../../assets/r/vendor4.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor3.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor1.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-common.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-doc-page.js@version=2021.12.02"></script><link href="../../../assets/r/vendor4.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-common.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-doc-page.css@version=2021.12.02.css" rel="stylesheet" /><script async="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'sagemaker'}"></script><meta id="panorama-serviceSubSection" value="Developer Guide" /><meta id="panorama-serviceConsolePage" value="Deploy models for inference" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>Deploy models for inference - Amazon SageMaker</title><meta name="pdf" content="/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#deploy-model" /><meta name="rss" content="amazon-sagemaker-release-notes.rss" /><meta name="forums" content="http://forums.aws.amazon.com/forum.jspa?forumID=285" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/deploy-model.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/deploy-model.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/deploy-model.html" /><meta name="keywords" content="SageMaker,Amazon SageMaker,machine learning,notebook instance,inference,deploy model,endpoint,prediction,ML application,serverless machine learning,multi model deployment,inference pipelines,ML model" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "Developer Guide",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Deploy models for inference",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#deploy-model" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="https://aws.amazon.com">AWS</a><a href="https://docs.aws.amazon.com/index.html">Documentation</a><a href="https://docs.aws.amazon.com/sagemaker/index.html">Amazon SageMaker</a><a href="whatis.html">Developer Guide</a></div><div id="page-toc-src"><a href="deploy-model.html#deploy-model-prereqs">Before you begin</a><a href="deploy-model.html#deploy-model-steps">Steps for model deployment</a><a href="deploy-model.html#deploy-model-options">Inference options</a><a href="deploy-model.html#deploy-model-advanced">Advanced endpoint options</a><a href="deploy-model.html#deploy-model-steps-byom">Bring your own model</a><a href="deploy-model.html#deploy-model-next-steps">Next steps</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="deploy-model">Deploy models for inference</h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>With Amazon SageMaker, you can deploy your machine learning (ML) models to make predictions, also known as <em>inference</em>. SageMaker provides a broad selection of ML
        infrastructure and model deployment options to help meet all your ML inference needs. It is
        a fully managed service and integrates with MLOps tools, so you can scale your model
        deployment, reduce inference costs, manage models more effectively in production, and reduce
        operational burden.</p><p>After youâ€™ve built and trained a machine learning model, you can use SageMaker Inference to start getting predictions, or
        <em>inferences</em>, from your model. With SageMaker Inference, you can either set up an endpoint that
        returns inferences or run batch inferences from your model.</p><p>To get started with SageMaker Inference, see the following sections and review the <a href="deploy-model.html#deploy-model-options">Inference options</a>
        to determine which feature best fits your use case.</p><p>You can refer to the <a href="inference-resources.html">Resources</a> section for more troubleshooting and reference information,
        blogs and examples to help you get started, and common FAQs.</p><div class="itemizedlist">
        <h6><b>Topics</b></h6>
         
         
         
         
         
         
    <ul class="itemizedlist"><li class="listitem">
            <p><a href="deploy-model.html#deploy-model-prereqs">Before you begin</a></p>
        </li><li class="listitem">
            <p><a href="deploy-model.html#deploy-model-steps">Steps for model deployment</a></p>
        </li><li class="listitem">
            <p><a href="deploy-model.html#deploy-model-options">Inference options</a></p>
        </li><li class="listitem">
            <p><a href="deploy-model.html#deploy-model-advanced">Advanced endpoint options</a></p>
        </li><li class="listitem">
            <p><a href="deploy-model.html#deploy-model-steps-byom">Bring your own model</a></p>
        </li><li class="listitem">
            <p><a href="deploy-model.html#deploy-model-next-steps">Next steps</a></p>
        </li></ul></div>
        <h2 id="deploy-model-prereqs">Before you begin</h2>
        <p>These topics assume that you have built and trained one or more machine learning
            models and are ready to deploy them. You don't need to train your model in SageMaker in order
            to deploy your model in SageMaker and get inferences. If you don't have your own model, you
            can also use SageMakerâ€™s <a href="algos.html">built-in algorithms or pre-trained models</a>.</p>
        <p>If you are new to SageMaker and haven't picked out a model to deploy, work through the steps in the <a href="gs.html">Get Started with
                Amazon SageMaker</a> tutorial to familiarize yourself with an example of how SageMaker
            manages the data science process and how it handles model deployment. For more
            information about training a model, see <a href="train-model.html">Train Models</a>.</p>
        <p>For additional information, reference, and examples, see the <a href="inference-resources.html">Resources</a>.</p>
     
        <h2 id="deploy-model-steps">Steps for model deployment</h2>
        <p>For inference endpoints, the general workflow consists of the following:</p>
        <div class="itemizedlist">
             
             
             
             
             
        <ul class="itemizedlist"><li class="listitem"><p>Create a model in SageMaker Inference by pointing to model artifacts stored in Amazon S3 and a container image.</p></li><li class="listitem"><p>Select an inference option. For more information, see <a href="deploy-model.html#deploy-model-options">Inference options</a>.</p></li><li class="listitem"><p>Create a SageMaker Inference endpoint configuration by choosing the instance type and number of
                instances you need behind the endpoint. You can use <a href="inference-recommender.html">Amazon SageMaker Inference Recommender</a> to
                get recommendations for instance types. For Serverless Inference, you only need to provide the
                memory configuration you need based on your model size. </p></li><li class="listitem"><p>Create a SageMaker Inference endpoint.</p></li><li class="listitem"><p>Invoke your endpoint to receive an inference as a response.</p></li></ul></div>
        
        <p>The following diagram shows the preceding workflow.</p>
        <div class="mediaobject">
             
                <img src="../../../images/sagemaker/latest/dg/images/inference-workflow-flowchart.png" class="aws-docs-img-whiteBg aws-docs-img-padding" alt="&#xA;                A diagram of the workflow described in the preceding paragraph showing how to get inferences from SageMaker.&#xA;            " style="max-width:100%" />
             
             
        </div>
        
        <p>You can perform these actions using the AWS console, the AWS SDKs, the SageMaker Python SDK, AWS CloudFormation or the AWS CLI.</p>
        <p>For batch inference with batch transform, point to your model artifacts and input data
            and create a batch inference job. Instead of hosting an endpoint for inference, SageMaker
            outputs your inferences to an Amazon S3 location of your choice.</p>
     
        <h2 id="deploy-model-options">Inference options</h2>
        <p>SageMaker provides multiple inference options so that you can pick the option that best suits your workload:</p>
        <div class="itemizedlist">
             
             
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p><a href="realtime-endpoints.html">Real-Time Inference</a>: <em>Real-time
                        inference</em> is ideal for online inferences that have low latency or
                    high throughput requirements. Use real-time inference for a persistent and fully
                    managed endpoint (REST API) that can handle sustained traffic, backed by the
                    instance type of your choice. Real-time inference can support payload sizes up to 6 MB
                    and processing times of 60 seconds.</p>
            </li><li class="listitem">
                <p><a href="serverless-endpoints.html">Serverless Inference</a>: <em>Serverless inference</em> is ideal when you have
                    intermittent or unpredictable traffic patterns. SageMaker manages all of the
                    underlying infrastructure, so thereâ€™s no need to manage instances or scaling
                    policies. You pay only for what you use and not for idle time. It can support payload sizes up to 4 MB
                    and processing times up to 60 seconds.</p>
            </li><li class="listitem">
                <p><a href="batch-transform.html">Batch Transform</a>: <em>Batch transform</em>
                    is suitable for offline processing when large amounts of data are available
                    upfront and you donâ€™t need a persistent endpoint. You can also use batch
                    transform for pre-processing datasets. It can support large datasets that are GBs in size
                    and processing times of days.</p>
            </li><li class="listitem">
                <p><a href="async-inference.html">Asynchronous Inference</a>: <em>Asynchronous inference</em> is
                    ideal when you want to queue requests and have large payloads with long
                    processing times. Asynchronous Inference can support payloads up to 1 GB and long processing
                    times up to one hour. You can also scale down your endpoint to 0 when there are
                    no requests to process.</p>
            </li></ul></div>
        
        <p>The following diagram shows the preceding information in a flowchart and can help you choose the option that best fits your use case.</p>
        <div class="mediaobject">
             
                <img src="../../../images/sagemaker/latest/dg/images/inference-workflow-options.png" class="aws-docs-img-whiteBg aws-docs-img-padding" alt="&#xA;                A diagram listing the benefits of each SageMaker Inference option. The full benefits are described in the preceding paragraphs.&#xA;            " style="max-width:100%" />
             
             
        </div>
     
        <h2 id="deploy-model-advanced">Advanced endpoint options</h2>
        <p>With real-time inference, you can further optimize for performance and cost with the
            following advanced inference options:</p>
        <div class="itemizedlist">
             
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p>If you have multiple models that use the same framework and can share a container, then use
                    <a href="multi-model-endpoints.html">Host multiple models in one container behind one endpoint</a>. This option helps you optimize costs by improving
                    endpoint utilization and reducing deployment overhead.</p>
            </li><li class="listitem">
                <p>If you have multiple models that use different frameworks and require their
                    own containers, then use <a href="multi-container-endpoints.html">Host multiple models which use different containers behind one endpoint</a>. With this option, you get many
                    of the benefits of Multi-Model Endpoints and can deploy a variety of frameworks and
                    models.</p>
            </li><li class="listitem">
                <p>If you want to host models with pre-processing and post-processing logic behind an endpoint, then use
                    <a href="inference-pipelines.html">Serial Inference Pipelines</a>. Inference pipelines
                    are fully managed by SageMaker and provide lower latency because all of the containers are hosted on the same Amazon EC2 instances.</p>
            </li></ul></div>
     
            <h2 id="deploy-model-steps-byom">Bring your own model</h2>
            <p>To use an existing Docker container in SageMaker, see <a href="docker-containers-adapt-your-own.html">Adapting your own Docker container to
                work with SageMaker</a>.</p>
            <p>To create a new Docker container and receive more advanced guidance on how to run
                your own inference code, see the following links.</p>
            <div class="itemizedlist">
                 
                 
            <ul class="itemizedlist"><li class="listitem">
                    <p>To run your own inference code hosting services, see <a href="your-algorithms-inference-code.html">Use Your Own Inference Code with Hosting
            Services</a>.</p>
                </li><li class="listitem">
                    <p>To run your own inference code for batch inference, see <a href="your-algorithms-batch-code.html">Use Your Own Inference Code with Batch
            Transform</a>.</p>
                </li></ul></div>
         
        <h2 id="deploy-model-next-steps">Next steps</h2>
        <p>After you have an endpoint and understand the general inference workflow, you can use
            the following features within SageMaker Inference to improve your inference workflow.</p>
        
         
            <h3 id="deploy-model-next-steps-monitoring">Monitoring</h3>
            <p>To track your model over time through metrics such as model accuracy and drift, you can use Model Monitor. With Model Monitor,
                you can set alerts that notify you when there are deviations in your modelâ€™s quality. To learn more, see the
                <a href="model-monitor.html">Model Monitor documentation</a>.
                To learn more about tools that can be used to monitor model deployments and events that change your endpoint,
                see <a href="monitoring-overview.html">Monitor Amazon SageMaker</a>.
                For example, you can monitor your endpointâ€™s health through metrics such as invocation errors and model latency using Amazon CloudWatch metrics.
                The <a href="monitoring-cloudwatch.html#cloudwatch-metrics-endpoint-invocation">SageMaker endpoint invocation metrics</a>
                can provide you with valuable information about your endpointâ€™s performance.</p>
         
        
         
            <h3 id="deploy-model-next-steps-cicd">CI/CD for model deployment</h3>
            <p>To put together machine learning solutions in SageMaker, you can use <a href="sagemaker-projects.html">SageMaker
                    MLOps</a>. You can use this feature to automate the steps in your machine
                learning workflow and practice CI/CD. You can use <a href="sagemaker-projects-templates.html">MLOps Project
                    Templates</a> to help with the setup and implementation of SageMaker MLOps
                projects. SageMaker also supports using your own <a href="sagemaker-projects-walkthrough-3rdgit.html">third-party
                    Git repo</a> for creating a CI/CD system.</p>
            <p>For your ML pipelines, use <a href="model-registry.html">Model Registry</a>
                to manage your model versions and the deployment and automation of your models.</p>
         
        
         
            <h3 id="deploy-model-next-steps-guardrails">Deployment guardrails</h3>
            <p>If you want to update your model while itâ€™s in production without impacting production, you can use deployment guardrails.
                Deployment guardrails are a set of model deployment options in SageMaker Inference to update your machine learning models in production.
                Using the fully managed deployment options, you can control the switch from the current model in production to a new one. Traffic shifting
                modes give you granular control over the traffic shifting process, and built-in safeguards like auto-rollbacks help you catch issues early on.
                To learn more about deployment guardrails, see the <a href="deployment-guardrails.html">deployment guardrails documentation</a>.</p>
         
        
        
        
         
            <h3 id="deploy-model-next-steps-inferentia">Inferentia</h3>
            <p>If you need to run large scale machine learning and deep learning applications for
                use cases such as image or speech recognition, natural language processing (NLP),
                personalization, forecasting, or fraud detection, you can use an <code class="code">Inf1</code> instance with
                a real-time endpoint.</p>
            <p><code class="code">Inf1</code> instances are built to support machine learning inference applications and feature the AWS Inferentia chips. <code class="code">Inf1</code> instances provide higher
                throughput and lower cost per inference than GPU-based instances.</p>
            <p>To deploy a model on <code class="code">Inf1</code> instances, compile your model with SageMaker Neo and choose an <code class="code">Inf1</code> instance for your deployment option.
                To learn more, see <a href="neo.html">Optimize model performance using SageMaker Neo</a>.</p>
         
        
         
            <h3 id="deploy-model-next-steps-optimize">Optimize model performance</h3>
            <p>SageMaker provides features to manage resources and optimize inference performance when
                deploying machine learning models. You can use SageMakerâ€™s <a href="algos.html">built-in algorithms and pre-built
                    models</a>, as well as <a href="docker-containers-prebuilt.html">prebuilt Docker
                    images</a>, which are developed for machine learning. To train TensorFlow, Apache
                MXNet, PyTorch, ONNX, and XGBoost models once and optimize them to deploy on ARM,
                Intel, and Nvidia processors, see <a href="neo.html">Optimize model performance using SageMaker
                Neo</a>.</p>
         
        
         
            <h3 id="deploy-model-next-steps-autoscaling">Autoscaling</h3>
            <p>If you have varying amounts of traffic to your endpoints, you might want to try autoscaling. For example, during peak hours, you might require
                more instances to process requests, but during periods of low traffic, you might want to reduce your use of computing resources. To dynamically adjust
                the number of instances provisioned in response to changes in your workload, see <a href="endpoint-auto-scaling.html">Automatically Scale Amazon SageMaker Models</a>.</p>
            <p>If you have unpredictable traffic patterns or donâ€™t want to set up scaling policies, you can also use Serverless Inference for an endpoint where SageMaker manages autoscaling for you.
                During periods of low traffic, SageMaker scales down your endpoint, and if traffic increases, then SageMaker scales your endpoint up. For more information, see the
                <a href="serverless-endpoints.html">Serverless Inference</a> documentation.</p>
         
        
    <awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="https://docs.aws.amazon.com/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./model-checkpoints.html">Use Checkpoints</div><div id="next" class="next-link" accesskey="n" href="./how-it-works-deployment.html">Model Deployment</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/deploy-model.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/deploy-model.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>