<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Object Detection - MXNet - Amazon SageMaker</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="object-detection" /><meta name="default_state" content="object-detection" /><link rel="icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="canonical" href="object-detection.html" /><meta name="description" content="The Amazon SageMaker Object Detection - MXNet algorithm identifies object instances in an image." /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon SageMaker" /><meta name="guide" content="Developer Guide" /><meta name="abstract" content="Use Internet Monitor to build, train, and host machine learning models in AWS." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="object-detection.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/object-detection.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/object-detection.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/object-detection.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/object-detection.html" hreflang="de" /><link rel="alternative" href="object-detection.html" hreflang="en-us" /><link rel="alternative" href="object-detection.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/object-detection.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/object-detection.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/object-detection.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/object-detection.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/object-detection.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/object-detection.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/object-detection.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/object-detection.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/object-detection.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/object-detection.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/object-detection.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/object-detection.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/object-detection.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/object-detection.html" hreflang="zh-tw" /><link rel="alternative" href="object-detection.html" hreflang="x-default" /><meta name="feedback-item" content="SageMaker" /><meta name="this_doc_product" content="Amazon SageMaker" /><meta name="this_doc_guide" content="Developer Guide" /><script defer="" src="../../../assets/r/vendor4.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor3.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor1.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-common.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-doc-page.js@version=2021.12.02"></script><link href="../../../assets/r/vendor4.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-common.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-doc-page.css@version=2021.12.02.css" rel="stylesheet" /><script async="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'sagemaker'}"></script><meta id="panorama-serviceSubSection" value="Developer Guide" /><meta id="panorama-serviceConsolePage" value="Object Detection - MXNet" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>Object Detection - MXNet - Amazon SageMaker</title><meta name="pdf" content="/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#object-detection" /><meta name="rss" content="amazon-sagemaker-release-notes.rss" /><meta name="forums" content="http://forums.aws.amazon.com/forum.jspa?forumID=285" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/object-detection.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/object-detection.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/object-detection.html" /><meta name="keywords" content="SageMaker,Amazon SageMaker,machine learning,notebook instance" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "Developer Guide",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Train machine learning models",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/train-model.html"
      },
      {
        "@type" : "ListItem",
        "position" : 5,
        "name" : "Choose an Algorithm",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-choose.html"
      },
      {
        "@type" : "ListItem",
        "position" : 6,
        "name" : "Use Amazon SageMaker Built-in Algorithms or Pre-trained Models",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"
      },
      {
        "@type" : "ListItem",
        "position" : 7,
        "name" : "Built-in SageMaker Algorithms for Computer Vision",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-vision.html"
      },
      {
        "@type" : "ListItem",
        "position" : 8,
        "name" : "Object Detection - MXNet",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-vision.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#object-detection" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="https://aws.amazon.com">AWS</a><a href="https://docs.aws.amazon.com/index.html">Documentation</a><a href="https://docs.aws.amazon.com/sagemaker/index.html">Amazon SageMaker</a><a href="whatis.html">Developer Guide</a></div><div id="page-toc-src"><a href="object-detection.html#object-detection-inputoutput">Input/Output Interface for the Object
                Detection Algorithm</a><a href="object-detection.html#object-detection-instances">EC2 Instance Recommendation for the Object
                Detection Algorithm</a><a href="object-detection.html#object-detection-sample-notebooks">Sample Notebooks</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="object-detection">Object Detection - MXNet</h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>The Amazon SageMaker Object Detection - MXNet algorithm detects and classifies objects in images using a
        single deep neural network. It is a supervised learning algorithm that takes images as input
        and identifies all instances of objects within the image scene. The object is categorized
        into one of the classes in a specified collection with a confidence score that it belongs to
        the class. Its location and scale in the image are indicated by a rectangular bounding box.
        It uses the <a href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener noreferrer" target="_blank"><span>Single Shot multibox Detector
            (SSD)</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> framework and supports two base networks: <a href="https://arxiv.org/pdf/1409.1556.pdf" rel="noopener noreferrer" target="_blank"><span>VGG</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> and <a href="https://arxiv.org/pdf/1603.05027.pdf" rel="noopener noreferrer" target="_blank"><span>ResNet</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. The network can be trained
        from scratch, or trained with models that have been pre-trained on the <a href="http://www.image-net.org/" rel="noopener noreferrer" target="_blank"><span>ImageNet</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> dataset.</p><div class="highlights"><h6>Topics</h6><ul><li><a href="object-detection.html#object-detection-inputoutput">Input/Output Interface for the Object
                Detection Algorithm</a></li><li><a href="object-detection.html#object-detection-instances">EC2 Instance Recommendation for the Object
                Detection Algorithm</a></li><li><a href="object-detection.html#object-detection-sample-notebooks">Object Detection Sample
                Notebooks</a></li><li><a href="algo-object-detection-tech-notes.html">How Object Detection Works</a></li><li><a href="object-detection-api-config.html">Object Detection Hyperparameters</a></li><li><a href="object-detection-tuning.html">Tune an Object Detection Model</a></li><li><a href="object-detection-in-formats.html">Object Detection Request and Response
                Formats</a></li></ul></div>
        <h2 id="object-detection-inputoutput">Input/Output Interface for the Object
                Detection Algorithm</h2>
        <p>The SageMaker Object Detection algorithm supports both RecordIO
                (<code class="code">application/x-recordio</code>) and image (<code class="code">image/png</code>,
                <code class="code">image/jpeg</code>, and <code class="code">application/x-image</code>) content types for
            training in file mode and supports RecordIO (<code class="code">application/x-recordio</code>) for
            training in pipe mode. However you can also train in pipe mode using the image files
                (<code class="code">image/png</code>, <code class="code">image/jpeg</code>, and
                <code class="code">application/x-image</code>), without creating RecordIO files, by using the
            augmented manifest format. The recommended input format for the Amazon SageMaker object
            detection algorithms is <a href="https://mxnet.apache.org/api/architecture/note_data_loading" rel="noopener noreferrer" target="_blank"><span>Apache MXNet
                RecordIO</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. However, you can also use raw images in .jpg or .png format. The
            algorithm supports only <code class="code">application/x-image</code> for inference.</p>
        <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>To maintain better interoperability with existing deep learning frameworks, this
                differs from the protobuf data formats commonly used by other Amazon SageMaker
                algorithms.</p></div></div>
        <p>See the <a href="object-detection.html#object-detection-sample-notebooks">Object Detection Sample
                Notebooks</a> for more details on data
            formats.</p>

         
            <h3 id="object-detection-recordio-training">Train with the RecordIO
                    Format</h3>
            <p>If you use the RecordIO format for training, specify both train and validation
                channels as values for the <code class="code">InputDataConfig</code> parameter of the <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html"><code class="code">CreateTrainingJob</code></a> request. Specify one RecordIO (.rec) file
                in the train channel and one RecordIO file in the validation channel. Set the
                content type for both channels to <code class="code">application/x-recordio</code>. An example of
                how to generate RecordIO file can be found in the object detection sample notebook.
                You can also use tools from the <a href="https://gluon-cv.mxnet.io/build/examples_datasets/recordio.html" rel="noopener noreferrer" target="_blank"><span>MXNet's
                    GluonCV</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> to generate RecordIO files for popular datasets like the <a href="http://host.robots.ox.ac.uk/pascal/VOC/" rel="noopener noreferrer" target="_blank"><span>PASCAL Visual Object
                    Classes</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> and <a href="http://cocodataset.org/#home" rel="noopener noreferrer" target="_blank"><span>Common Objects in
                    Context (COCO)</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
         
         
            <h3 id="object-detection-image-training">Train with the Image
                    Format</h3>
            <p>If you use the image format for training, specify <code class="code">train</code>,
                    <code class="code">validation</code>, <code class="code">train_annotation</code>, and
                    <code class="code">validation_annotation</code> channels as values for the
                    <code class="code">InputDataConfig</code> parameter of <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html"><code class="code">CreateTrainingJob</code></a> request. Specify the individual image
                data (.jpg or .png) files for the train and validation channels. For annotation
                data, you can use the JSON format. Specify the corresponding .json files in the
                    <code class="code">train_annotation</code> and <code class="code">validation_annotation</code> channels.
                Set the content type for all four channels to <code class="code">image/png</code> or
                    <code class="code">image/jpeg</code> based on the image type. You can also use the content
                type <code class="code">application/x-image</code> when your dataset contains both .jpg and .png
                images. The following is an example of a .json file.</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class=""><span>{</span>
   "file": "your_image_directory/sample_image1.jpg",
   "image_size": [
      <span>{</span>
         "width": 500,
         "height": 400,
         "depth": 3
      }
   ],
   "annotations": [
      <span>{</span>
         "class_id": 0,
         "left": 111,
         "top": 134,
         "width": 61,
         "height": 128
      },
      <span>{</span>
         "class_id": 0,
         "left": 161,
         "top": 250,
         "width": 79,
         "height": 143
      },
      <span>{</span>
         "class_id": 1,
         "left": 101,
         "top": 185,
         "width": 42,
         "height": 130
      }
   ],
   "categories": [
      <span>{</span>
         "class_id": 0,
         "name": "dog"
      },
      <span>{</span>
         "class_id": 1,
         "name": "cat"
      }
   ]
}</code></pre>
            <p>Each image needs a .json file for annotation, and the .json file should have the
                same name as the corresponding image. The name of above .json file should be
                "sample_image1.json". There are four properties in the annotation .json file. The
                property "file" specifies the relative path of the image file. For example, if your
                training images and corresponding .json files are stored in
                    s3://<code class="replaceable">your_bucket</code>/train/sample_image and
                    s3://<code class="replaceable">your_bucket</code>/train_annotation, specify the path
                for your train and train_annotation channels as
                    s3://<code class="replaceable">your_bucket</code>/train and
                    s3://<code class="replaceable">your_bucket</code>/train_annotation, respectively. </p>
            <p>In the .json file, the relative path for an image named sample_image1.jpg should
                be sample_image/sample_image1.jpg. The <code class="code">"image_size"</code> property specifies
                the overall image dimensions. The SageMaker object detection algorithm currently only
                supports 3-channel images. The <code class="code">"annotations"</code> property specifies the
                categories and bounding boxes for objects within the image. Each object is annotated
                by a <code class="code">"class_id"</code> index and by four bounding box coordinates
                    (<code class="code">"left"</code>, <code class="code">"top"</code>, <code class="code">"width"</code>,
                    <code class="code">"height"</code>). The <code class="code">"left"</code> (x-coordinate) and
                    <code class="code">"top"</code> (y-coordinate) values represent the upper-left corner of the
                bounding box. The <code class="code">"width"</code> (x-coordinate) and <code class="code">"height"</code>
                (y-coordinate) values represent the dimensions of the bounding box. The origin (0,
                0) is the upper-left corner of the entire image. If you have multiple objects within
                one image, all the annotations should be included in a single .json file. The
                    <code class="code">"categories"</code> property stores the mapping between the class index
                and class name. The class indices should be numbered successively and the numbering
                should start with 0. The <code class="code">"categories"</code> property is optional for the
                annotation .json file</p>
         

         
            <h3 id="object-detection-augmented-manifest-training">Train with Augmented
                    Manifest Image Format</h3>
            <p>The augmented manifest format enables you to do training in pipe mode using image
                files without needing to create RecordIO files. You need to specify both train and
                validation channels as values for the <code class="code">InputDataConfig</code> parameter of the
                    <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html"><code class="code">CreateTrainingJob</code></a> request. While using the format, an S3
                manifest file needs to be generated that contains the list of images and their
                corresponding annotations. The manifest file format should be in <a href="http://jsonlines.org/" rel="noopener noreferrer" target="_blank"><span>JSON Lines</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> format in which each line
                represents one sample. The images are specified using the <code class="code">'source-ref'</code>
                tag that points to the S3 location of the image. The annotations are provided under
                the <code class="code">"AttributeNames"</code> parameter value as specified in the <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html"><code class="code">CreateTrainingJob</code></a> request. It can also contain additional
                metadata under the <code class="code">metadata</code> tag, but these are ignored by the
                algorithm. In the following example, the <code class="code">"AttributeNames</code> are contained
                in the list <code class="code">["source-ref", "bounding-box"]</code>:</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class=""><span>{</span>"source-ref": "s3://your_bucket/image1.jpg", "bounding-box":<span>{</span>"image_size":[<span>{</span> "width": 500, "height": 400, "depth":3}], "annotations":[<span>{</span>"class_id": 0, "left": 111, "top": 134, "width": 61, "height": 128}, <span>{</span>"class_id": 5, "left": 161, "top": 250, "width": 80, "height": 50}]}, "bounding-box-metadata":<span>{</span>"class-map":<span>{</span>"0": "dog", "5": "horse"}, "type": "groundtruth/object-detection"}}
<span>{</span>"source-ref": "s3://your_bucket/image2.jpg", "bounding-box":<span>{</span>"image_size":[<span>{</span> "width": 400, "height": 300, "depth":3}], "annotations":[<span>{</span>"class_id": 1, "left": 100, "top": 120, "width": 43, "height": 78}]}, "bounding-box-metadata":<span>{</span>"class-map":<span>{</span>"1": "cat"}, "type": "groundtruth/object-detection"}}</code></pre>

            <p>The order of <code class="code">"AttributeNames"</code> in the input files matters when
                training the Object Detection algorithm. It accepts piped data in a specific order,
                with <code class="code">image</code> first, followed by <code class="code">annotations</code>. So the
                "AttributeNames" in this example are provided with <code class="code">"source-ref"</code> first,
                followed by <code class="code">"bounding-box"</code>. When using Object Detection with Augmented
                Manifest, the value of parameter <code class="code">RecordWrapperType</code> must be set as
                    <code class="code">"RecordIO"</code>.</p>
            <p>For more information on augmented manifest files, see <a href="augmented-manifest.html">Provide Dataset Metadata to Training Jobs with an
            Augmented Manifest File</a>.</p>
         

         
            <h3 id="object-detection-incremental-training">Incremental Training</h3>

            <p>You can also seed the training of a new model with the artifacts from a model that
                you trained previously with SageMaker. Incremental training saves training time when you
                want to train a new model with the same or similar data. SageMaker object detection
                models can be seeded only with another built-in object detection model trained in
                SageMaker.</p>

            <p>To use a pretrained model, in the <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html"><code class="code">CreateTrainingJob</code></a> request, specify the
                    <code class="code">ChannelName</code> as "model" in the <code class="code">InputDataConfig</code>
                parameter. Set the <code class="code">ContentType</code> for the model channel to
                    <code class="code">application/x-sagemaker-model</code>. The input hyperparameters of both
                the new model and the pretrained model that you upload to the model channel must
                have the same settings for the <code class="code">base_network</code> and
                    <code class="code">num_classes</code> input parameters. These parameters define the network
                architecture. For the pretrained model file, use the compressed model artifacts (in
                .tar.gz format) output by SageMaker. You can use either RecordIO or image formats for
                input data.</p>

            
            
            <p>For more
                information on incremental training and for instructions on how to use it, see <a href="incremental-training.html">Use Incremental Training in Amazon SageMaker</a>. </p>
         


     
        <h2 id="object-detection-instances">EC2 Instance Recommendation for the Object
                Detection Algorithm</h2>

        <p>The object detection algorithm supports P2, P3, G4dn, and G5 GPU instance families. 
            We recommend using GPU instances with more memory for training with large batch sizes. 
            You can run the object detection algorithm on multi-GPU and mult-machine settings for distributed training.</p>
        <p>You can use both CPU (such as C5 and M5) and GPU (such as P3 and G4dn) instances for inference.</p>
     
        <h2 id="object-detection-sample-notebooks">Object Detection Sample
                Notebooks</h2>
        <p>For a sample notebook that shows how to use the SageMaker Object Detection algorithm to
            train and host a model on the </p>
        <p><a href="http://www.vision.caltech.edu/datasets/cub_200_2011/" rel="noopener noreferrer" target="_blank"><span>Caltech Birds
                (CUB 200 2011)</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> dataset using the Single Shot multibox Detector algorithm,
            see <a href="https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/object_detection_birds/object_detection_birds.html" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker Object Detection for Bird Species</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. For instructions how to create
            and access Jupyter notebook instances that you can use to run the example in SageMaker, see
                <a href="nbi.html">Amazon SageMaker Notebook Instances</a>. Once you have created a notebook instance
            and opened it, select the <b>SageMaker Examples</b> tab to see a
            list of all the SageMaker samples. The object detection example notebook using the Object
            Detection algorithm is located in the <b>Introduction to Amazon
                Algorithms</b> section. To open a notebook, click on its <b>Use</b> tab and select <b>Create
                copy</b>.</p>
    <awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="https://docs.aws.amazon.com/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./IC-TF-tuning.html">Model Tuning</div><div id="next" class="next-link" accesskey="n" href="./algo-object-detection-tech-notes.html">How It Works</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/object-detection.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/object-detection.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>