<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Semantic Segmentation Algorithm - Amazon SageMaker</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="semantic-segmentation" /><meta name="default_state" content="semantic-segmentation" /><link rel="icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="canonical" href="semantic-segmentation.html" /><meta name="description" content="The Amazon SageMaker semantic segmentation algorithm identifies and locates objects in an image by tagging every pixel with a class label. ." /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon SageMaker" /><meta name="guide" content="Developer Guide" /><meta name="abstract" content="Use Internet Monitor to build, train, and host machine learning models in AWS." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="semantic-segmentation.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/semantic-segmentation.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/semantic-segmentation.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/semantic-segmentation.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/semantic-segmentation.html" hreflang="de" /><link rel="alternative" href="semantic-segmentation.html" hreflang="en-us" /><link rel="alternative" href="semantic-segmentation.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/semantic-segmentation.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/semantic-segmentation.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/semantic-segmentation.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/semantic-segmentation.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/semantic-segmentation.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/semantic-segmentation.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/semantic-segmentation.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/semantic-segmentation.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/semantic-segmentation.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/semantic-segmentation.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/semantic-segmentation.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/semantic-segmentation.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/semantic-segmentation.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/semantic-segmentation.html" hreflang="zh-tw" /><link rel="alternative" href="semantic-segmentation.html" hreflang="x-default" /><meta name="feedback-item" content="SageMaker" /><meta name="this_doc_product" content="Amazon SageMaker" /><meta name="this_doc_guide" content="Developer Guide" /><script defer="" src="../../../assets/r/vendor4.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor3.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor1.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-common.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-doc-page.js@version=2021.12.02"></script><link href="../../../assets/r/vendor4.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-common.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-doc-page.css@version=2021.12.02.css" rel="stylesheet" /><script async="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'sagemaker'}"></script><meta id="panorama-serviceSubSection" value="Developer Guide" /><meta id="panorama-serviceConsolePage" value="Semantic Segmentation Algorithm" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>Semantic Segmentation Algorithm - Amazon SageMaker</title><meta name="pdf" content="/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#semantic-segmentation" /><meta name="rss" content="amazon-sagemaker-release-notes.rss" /><meta name="forums" content="http://forums.aws.amazon.com/forum.jspa?forumID=285" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/semantic-segmentation.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/semantic-segmentation.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/semantic-segmentation.html" /><meta name="keywords" content="SageMaker,Amazon SageMaker,machine learning,notebook instance" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "Developer Guide",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Train machine learning models",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/train-model.html"
      },
      {
        "@type" : "ListItem",
        "position" : 5,
        "name" : "Choose an Algorithm",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-choose.html"
      },
      {
        "@type" : "ListItem",
        "position" : 6,
        "name" : "Use Amazon SageMaker Built-in Algorithms or Pre-trained Models",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"
      },
      {
        "@type" : "ListItem",
        "position" : 7,
        "name" : "Built-in SageMaker Algorithms for Computer Vision",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-vision.html"
      },
      {
        "@type" : "ListItem",
        "position" : 8,
        "name" : "Semantic Segmentation Algorithm",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-vision.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#semantic-segmentation" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="https://aws.amazon.com">AWS</a><a href="https://docs.aws.amazon.com/index.html">Documentation</a><a href="https://docs.aws.amazon.com/sagemaker/index.html">Amazon SageMaker</a><a href="whatis.html">Developer Guide</a></div><div id="page-toc-src"><a href="semantic-segmentation.html#semantic-segmentation-sample-notebooks">Sample Notebooks</a><a href="semantic-segmentation.html#semantic-segmentation-inputoutput">Input/Output Interface for the
                Semantic Segmentation Algorithm</a><a href="semantic-segmentation.html#semantic-segmentation-instances">EC2 Instance Recommendation for the
                Semantic Segmentation Algorithm</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="semantic-segmentation">Semantic Segmentation Algorithm</h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>The SageMaker semantic segmentation algorithm provides a fine-grained, pixel-level approach to
        developing computer vision applications. It tags every pixel in an image with a class label
        from a predefined set of classes. Tagging is fundamental for understanding scenes, which is
        critical to an increasing number of computer vision applications, such as self-driving
        vehicles, medical imaging diagnostics, and robot sensing. </p><p>For comparison, the SageMaker <a href="image-classification.html">Image Classification - MXNet</a> is a supervised learning algorithm that analyzes
        only whole images, classifying them into one of multiple output categories. The <a href="object-detection.html">Object Detection - MXNet</a> is a supervised learning
        algorithm that detects and classifies all instances of an object in an image. It indicates
        the location and scale of each object in the image with a rectangular bounding box. </p><p>Because the semantic segmentation algorithm classifies every pixel in an image, it also
        provides information about the shapes of the objects contained in the image. The
        segmentation output is represented as a grayscale image, called a <em>segmentation mask</em>. A segmentation mask is a grayscale image with the same
        shape as the input image.</p><p>The SageMaker semantic segmentation algorithm is built using the <a href="https://github.com/dmlc/gluon-cv" rel="noopener noreferrer" target="_blank"><span>MXNet Gluon framework and the Gluon CV
            toolkit</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. It provides you with a choice of three built-in algorithms to train a deep
        neural network. You can use the <a href="https://arxiv.org/abs/1605.06211" rel="noopener noreferrer" target="_blank"><span>Fully-Convolutional Network (FCN) algorithm </span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>, <a href="https://arxiv.org/abs/1612.01105" rel="noopener noreferrer" target="_blank"><span>Pyramid Scene Parsing (PSP) algorithm</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>, or
            <a href="https://arxiv.org/abs/1706.05587" rel="noopener noreferrer" target="_blank"><span>DeepLabV3</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. </p><p>Each of the three algorithms has two distinct components: </p><div class="itemizedlist">
         
         
    <ul class="itemizedlist"><li class="listitem">
            <p>The <em>backbone</em> (or <em>encoder</em>)—A
                network that produces reliable activation maps of features.</p>
        </li><li class="listitem">
            <p>The <em>decoder</em>—A network that constructs the segmentation
                mask from the encoded activation maps.</p>
        </li></ul></div><p>You also have a choice of backbones for the FCN, PSP, and DeepLabV3 algorithms:
            <a href="https://arxiv.org/abs/1512.03385" rel="noopener noreferrer" target="_blank"><span>ResNet50 or ResNet101</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.
        These backbones include pretrained artifacts that were originally trained on the <a href="http://www.image-net.org/" rel="noopener noreferrer" target="_blank"><span>ImageNet</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> classification task. You can fine-tune
        these backbones for segmentation using your own data. Or, you can initialize and train these
        networks from scratch using only your own data. The decoders are never pretrained. </p><p>To deploy the trained model for inference, use the SageMaker hosting service. During inference,
        you can request the segmentation mask either as a PNG image or as a set of probabilities for
        each class for each pixel. You can use these masks as part of a larger pipeline that
        includes additional downstream image processing or other applications.</p><div class="highlights"><h6>Topics</h6><ul><li><a href="semantic-segmentation.html#semantic-segmentation-sample-notebooks">Semantic Segmentation Sample
                Notebooks</a></li><li><a href="semantic-segmentation.html#semantic-segmentation-inputoutput">Input/Output Interface for the
                Semantic Segmentation Algorithm</a></li><li><a href="semantic-segmentation.html#semantic-segmentation-instances">EC2 Instance Recommendation for the
                Semantic Segmentation Algorithm</a></li><li><a href="segmentation-hyperparameters.html">Semantic Segmentation
                Hyperparameters</a></li><li><a href="semantic-segmentation-tuning.html">Tuning a Semantic Segmentation Model</a></li></ul></div>
        <h2 id="semantic-segmentation-sample-notebooks">Semantic Segmentation Sample
                Notebooks</h2>
        <p>For a sample Jupyter notebook that uses the SageMaker semantic segmentation algorithm to
            train a model and deploy it to perform inferences, see the <a href="https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/semantic_segmentation_pascalvoc/semantic_segmentation_pascalvoc.html" rel="noopener noreferrer" target="_blank"><span>Semantic Segmentation Example</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. For instructions on how to create and access
            Jupyter notebook instances that you can use to run the example in SageMaker, see <a href="nbi.html">Amazon SageMaker Notebook Instances</a>. </p>
        <p>To see a list of all of the SageMaker samples, create and open a notebook instance, and
            choose the <b>SageMaker Examples</b> tab. The example semantic
            segmentation notebooks are located under <b>Introduction to Amazon
                algorithms</b>. To open a notebook, choose its <b>Use</b> tab, and choose <b>Create copy</b>.</p>
     
        <h2 id="semantic-segmentation-inputoutput">Input/Output Interface for the
                Semantic Segmentation Algorithm</h2>
        <p>SageMaker semantic segmentation expects the customer's training dataset to be on <a href="https://aws.amazon.com/s3/" rel="noopener noreferrer" target="_blank"><span>Amazon Simple Storage Service (Amazon S3)</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.
            Once trained, it produces the resulting model artifacts on Amazon S3. The input interface
            format for the SageMaker semantic segmentation is similar to that of most standardized
            semantic segmentation benchmarking datasets. The dataset in Amazon S3 is expected to be
            presented in two channels, one for <code class="code">train</code> and one for
                <code class="code">validation</code> using four directories, two for images and two for
            annotations. Annotations are expected to be uncompressed PNG images. The dataset might
            also have a label map that describes how the annotation mappings are established. If
            not, the algorithm uses a default. It also supports the augmented manifest image format
                (<code class="code">application/x-image</code>)
            for training in Pipe input mode straight from Amazon S3. For inference, an
            endpoint accepts images with an <code class="code">image/jpeg</code> content type. </p>
         
            <h3 id="semantic-segmentation-inputoutput-training">How Training
                    Works</h3>
            <p>The training data is split into four directories: <code class="code">train</code>,
                    <code class="code">train_annotation</code>, <code class="code">validation</code>, and
                    <code class="code">validation_annotation</code>. There is a channel for each of these
                directories. The dataset also expected to have one
                    <code>label_map.json</code> file per channel for
                    <code class="code">train_annotation</code> and <code class="code">validation_annotation</code>
                respectively. If you don't provide these JSON files, SageMaker provides the default set
                label map.</p>
            <p>The dataset specifying these files should look similar to the following
                example:</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="">s3://bucket_name
    |
    |- train
                 |
                 | - 0000.jpg
                 | - coffee.jpg
    |- validation
                 |
                 | - 00a0.jpg
                 | - bananna.jpg
    |- train_annotation
                 |
                 | - 0000.png
                 | - coffee.png
    |- validation_annotation
                 |
                 | - 00a0.png
                 | - bananna.png
    |- label_map
                 | - train_label_map.json
                 | - validation_label_map.json </code></pre>
            <p>Every JPG image in the train and validation directories has a corresponding PNG
                label image with the same name in the <code class="code">train_annotation</code> and
                    <code class="code">validation_annotation</code> directories. This naming convention helps the
                algorithm to associate a label with its corresponding image during training. The
                    <code class="code">train</code>, <code class="code">train_annotation</code>, <code class="code">validation</code>, and
                    <code class="code">validation_annotation</code> channels are mandatory. The annotations are
                single-channel PNG images. The format works as long as the metadata (modes) in the
                image helps the algorithm read the annotation images into a single-channel 8-bit
                unsigned integer. For more information on our support for modes, see the <a href="https://pillow.readthedocs.io/en/stable/handbook/concepts.html#modes" rel="noopener noreferrer" target="_blank"><span>Python
                    Image Library documentation</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. We recommend using the 8-bit pixel, true
                color <code class="code">P</code> mode. </p>
            <p>The image that is encoded is a simple 8-bit integer when using modes. To get from
                this mapping to a map of a label, the algorithm uses one mapping file per channel,
                called the <em>label map</em>. The label map is used to map the values
                in the image with actual label indices. In the default label map, which is provided
                by default if you don’t provide one, the pixel value in an annotation matrix (image)
                directly index the label. These images can be grayscale PNG files or 8-bit indexed
                PNG files. The label map file for the unscaled default case is the following: </p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class=""><span>{</span>
  "scale": "1"
}  </code></pre>
            <p>To provide some contrast for viewing, some annotation software scales the label
                images by a constant amount. To support this, the SageMaker semantic segmentation
                algorithm provides a rescaling option to scale down the values to actual label
                values. When scaling down doesn’t convert the value to an appropriate integer, the
                algorithm defaults to the greatest integer less than or equal to the scale value.
                The following code shows how to set the scale value to rescale the label
                values:</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class=""><span>{</span>
  "scale": "3"
}  </code></pre>
            <p>The following example shows how this <code class="code">"scale"</code> value is used to rescale
                the <code class="code">encoded_label</code> values of the input annotation image when they are
                mapped to the <code class="code">mapped_label</code> values to be used in training. The label
                values in the input annotation image are 0, 3, 6, with scale 3, so they are mapped
                to 0, 1, 2 for training:</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="">encoded_label = [0, 3, 6]
mapped_label = [0, 1, 2]</code></pre>
            <p>In some cases, you might need to specify a particular color mapping for each
                class. Use the map option in the label mapping as shown in the following example of
                a <code>label_map</code> file:</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class=""><span>{</span>
    "map": <span>{</span>
        "0": 5,
        "1": 0,
        "2": 2
    }
}</code></pre>
            <p>This label mapping for this example is:</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="">encoded_label = [0, 5, 2]
mapped_label = [1, 0, 2]</code></pre>
            <p>With label mappings, you can use different annotation systems and annotation
                software to obtain data without a lot of preprocessing. You can provide one label
                map per channel. The files for a label map in the <code class="code">label_map</code> channel
                must follow the naming conventions for the four directory structure. If you don't
                provide a label map, the algorithm assumes a scale of 1 (the default).</p>

         
         
            <h3 id="semantic-segmentation-inputoutput-training-augmented-manifest">Training with the Augmented Manifest Format</h3>
            <p>The augmented manifest format enables you to do training in Pipe mode using image
                files without needing to create RecordIO files. The augmented manifest file contains
                data objects and should be in <a href="http://jsonlines.org/" rel="noopener noreferrer" target="_blank"><span>JSON Lines</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>
                format, as described in the <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html"><code class="code">CreateTrainingJob</code></a> request. Each line in the manifest
                is an entry containing the Amazon S3 URI for the image and the URI for the annotation
                image.</p>
            <p>Each JSON object in the manifest file must contain a <code class="code">source-ref</code> key.
                The <code class="code">source-ref</code> key should contain the value of the Amazon S3 URI to the
                image. The labels are provided under the <code class="code">AttributeNames</code> parameter value
                as specified in the <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html"><code class="code">CreateTrainingJob</code></a> request. It can also contain additional
                metadata under the metadata tag, but these are ignored by the algorithm. In the
                example below, the <code class="code">AttributeNames</code> are contained in the list of image
                and annotation references <code class="code">["source-ref", "city-streets-ref"]</code>. These
                names must have <code class="code">-ref</code> appended to them. When using the Semantic
                Segmentation algorithm with Augmented Manifest, the value of the
                    <code class="code">RecordWrapperType</code> parameter must be <code class="code">"RecordIO"</code> and
                value of the <code class="code">ContentType</code> parameter must be
                    <code class="code">application/x-recordio</code>.</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class=""><span>{</span>"source-ref": "S3 bucket location", "city-streets-ref": "S3 bucket location", "city-streets-metadata": <span>{</span>"job-name": "label-city-streets", }}</code></pre>
            <p>For more information on augmented manifest files, see <a href="augmented-manifest.html">Provide Dataset Metadata to Training Jobs with an
            Augmented Manifest File</a>.</p>
         
         
            <h3 id="semantic-segmentation-inputoutput-incremental-training">Incremental
                    Training</h3>
            <p>You can also seed the training of a new model with a model that you trained
                previously using SageMaker. This incremental training saves training time when you want
                to train a new model with the same or similar data. Currently, incremental training
                is supported only for models trained with the built-in SageMaker Semantic
                Segmentation.</p>
            <p>To use your own pre-trained model, specify the <code class="code">ChannelName</code> as "model"
                in the <code class="code">InputDataConfig</code> for the <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html"><code class="code">CreateTrainingJob</code></a> request. Set the <code class="code">ContentType</code>
                for the model channel to <code class="code">application/x-sagemaker-model</code>. The
                    <code class="code">backbone</code>, <code class="code">algorithm</code>, <code class="code">crop_size</code>, and
                    <code class="code">num_classes</code> input parameters that define the network architecture
                must be consistently specified in the input hyperparameters of the new model and the
                pre-trained model that you upload to the model channel. For the pretrained model
                file, you can use the compressed (.tar.gz) artifacts from SageMaker outputs. You can only
                use Image formats for input data. For more information on incremental training and
                for instructions on how to use it, see <a href="incremental-training.html">Use Incremental Training in Amazon SageMaker</a>. </p>
         
         
            <h3 id="semantic-segmentation-inputoutput-inference">Produce
                    Inferences</h3>
            <p>To query a trained model that is deployed to an endpoint, you need to provide an
                image and an <code class="code">AcceptType</code> that denotes the type of output required. The
                endpoint takes JPEG images with an <code class="code">image/jpeg</code> content type. If you
                request an <code class="code">AcceptType</code> of <code class="code">image/png</code>, the algorithm outputs
                a PNG file with a segmentation mask in the same format as the labels themselves. If
                you request an accept type of<code class="code">application/x-recordio-protobuf</code>, the
                algorithm returns class probabilities encoded in recordio-protobuf format. The
                latter format outputs a 3D tensor where the third dimension is the same size as the
                number of classes. This component denotes the probability of each class label for
                each pixel.</p>
         
     
        <h2 id="semantic-segmentation-instances">EC2 Instance Recommendation for the
                Semantic Segmentation Algorithm</h2>
        <p>The SageMaker semantic segmentation algorithm only supports GPU instances for training, and
            we recommend using GPU instances with more memory for training with large batch sizes.
            The algorithm can be trained using P2, P3, G4dn, or G5 instances in single machine configurations.</p>
        <p>For inference, you can use either CPU instances (such as C5 and M5) and GPU instances
            (such as P3 and G4dn) or both. For information about the instance types that provide
            varying combinations of CPU, GPU, memory, and networking capacity for inference, see
                <a href="https://aws.amazon.com/sagemaker/pricing/instance-types/" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker ML Instance Types</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
    <awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="https://docs.aws.amazon.com/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./object-detection-tensorflow-tuning.html">Model Tuning</div><div id="next" class="next-link" accesskey="n" href="./segmentation-hyperparameters.html">Hyperparameters</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/semantic-segmentation.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/semantic-segmentation.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>