<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Import - Amazon SageMaker</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="data-wrangler-import" /><meta name="default_state" content="data-wrangler-import" /><link rel="icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="canonical" href="data-wrangler-import.html" /><meta name="description" content="Import your data into Data Wrangler to run transformations and analyses." /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon SageMaker" /><meta name="guide" content="Developer Guide" /><meta name="abstract" content="Use Internet Monitor to build, train, and host machine learning models in AWS." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="data-wrangler-import.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/data-wrangler-import.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/data-wrangler-import.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/data-wrangler-import.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/data-wrangler-import.html" hreflang="de" /><link rel="alternative" href="data-wrangler-import.html" hreflang="en-us" /><link rel="alternative" href="data-wrangler-import.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/data-wrangler-import.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/data-wrangler-import.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/data-wrangler-import.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/data-wrangler-import.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/data-wrangler-import.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/data-wrangler-import.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/data-wrangler-import.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/data-wrangler-import.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/data-wrangler-import.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/data-wrangler-import.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/data-wrangler-import.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/data-wrangler-import.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/data-wrangler-import.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/data-wrangler-import.html" hreflang="zh-tw" /><link rel="alternative" href="data-wrangler-import.html" hreflang="x-default" /><meta name="feedback-item" content="SageMaker" /><meta name="this_doc_product" content="Amazon SageMaker" /><meta name="this_doc_guide" content="Developer Guide" /><script defer="" src="../../../assets/r/vendor4.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor3.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor1.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-common.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-doc-page.js@version=2021.12.02"></script><link href="../../../assets/r/vendor4.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-common.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-doc-page.css@version=2021.12.02.css" rel="stylesheet" /><script async="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'sagemaker'}"></script><meta id="panorama-serviceSubSection" value="Developer Guide" /><meta id="panorama-serviceConsolePage" value="Import" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>Import - Amazon SageMaker</title><meta name="pdf" content="/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#data-wrangler-import" /><meta name="rss" content="amazon-sagemaker-release-notes.rss" /><meta name="forums" content="http://forums.aws.amazon.com/forum.jspa?forumID=285" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/data-wrangler-import.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/data-wrangler-import.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/data-wrangler-import.html" /><meta name="keywords" content="SageMaker,Amazon SageMaker,machine learning,notebook instance" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "Developer Guide",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Prepare data",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html"
      },
      {
        "@type" : "ListItem",
        "position" : 5,
        "name" : "Prepare ML Data with Amazon SageMaker Data Wrangler",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler.html"
      },
      {
        "@type" : "ListItem",
        "position" : 6,
        "name" : "Import",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#data-wrangler-import" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="https://aws.amazon.com">AWS</a><a href="https://docs.aws.amazon.com/index.html">Documentation</a><a href="https://docs.aws.amazon.com/sagemaker/index.html">Amazon SageMaker</a><a href="whatis.html">Developer Guide</a></div><div id="page-toc-src"><a href="data-wrangler-import.html#data-wrangler-import-s3">Import data from Amazon S3</a><a href="data-wrangler-import.html#data-wrangler-import-athena">Import data from Athena</a><a href="data-wrangler-import.html#data-wrangler-import-redshift">Import data from Amazon Redshift</a><a href="data-wrangler-import.html#data-wrangler-emr">Import data from Amazon EMR</a><a href="data-wrangler-import.html#data-wrangler-databricks">Import data from Databricks (JDBC)</a><a href="data-wrangler-import.html#data-wrangler-import-salesforce-data-cloud">Import data from Salesforce Data Cloud</a><a href="data-wrangler-import.html#data-wrangler-snowflake">Import data from Snowflake</a><a href="data-wrangler-import.html#data-wrangler-import-saas">Import Data from SaaS Platforms</a><a href="data-wrangler-import.html#data-wrangler-import-storage">Imported Data Storage</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="data-wrangler-import">Import</h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>You can use Amazon SageMaker Data Wrangler to import data from the following <em>data sources</em>:
        Amazon Simple Storage Service (Amazon S3), Amazon Athena, Amazon Redshift, and Snowflake. The dataset that you import can include
        up to 1000 columns.</p><div class="highlights" id="inline-topiclist"><h6>Topics</h6><ul><li><a href="data-wrangler-import.html#data-wrangler-import-s3">Import data from Amazon S3</a></li><li><a href="data-wrangler-import.html#data-wrangler-import-athena">Import data from Athena</a></li><li><a href="data-wrangler-import.html#data-wrangler-import-redshift">Import data from Amazon Redshift</a></li><li><a href="data-wrangler-import.html#data-wrangler-emr">Import data from Amazon EMR</a></li><li><a href="data-wrangler-import.html#data-wrangler-databricks">Import data from Databricks (JDBC)</a></li><li><a href="data-wrangler-import.html#data-wrangler-import-salesforce-data-cloud">Import data from Salesforce Data Cloud</a></li><li><a href="data-wrangler-import.html#data-wrangler-snowflake">Import data from Snowflake</a></li><li><a href="data-wrangler-import.html#data-wrangler-import-saas">Import Data From Software as a Service (SaaS) Platforms</a></li><li><a href="data-wrangler-import.html#data-wrangler-import-storage">Imported Data Storage</a></li></ul></div><p>Some data sources allow you to add multiple <em>data
            connections</em>:</p><div class="itemizedlist">
         
         
    <ul class="itemizedlist"><li class="listitem">
            <p>You can connect to multiple Amazon Redshift clusters. Each cluster becomes a data source.
            </p>
        </li><li class="listitem">
            <p>You can query any Athena database in your account to import data from that
                database.</p>
        </li></ul></div><p></p><p>When you import a dataset from a data source, it appears in your data flow. Data Wrangler
        automatically infers the data type of each column in your dataset. To modify these types,
        select the <b>Data types</b> step and select <b>Edit data
            types</b>.</p><p>When you import data from Athena or Amazon Redshift, the imported data is automatically stored in the
        default SageMaker S3 bucket for the AWS Region in which you are using Studio. Additionally,
        Athena stores data you preview in Data Wrangler in this bucket. To learn more, see <a href="data-wrangler-import.html#data-wrangler-import-storage">Imported Data Storage</a>.</p><div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>The default Amazon S3 bucket may not have the least permissive security settings, such as
            bucket policy and server-side encryption (SSE). We strongly recommend that you <a href="data-wrangler-security.html#data-wrangler-security-bucket-policy"> Add a Bucket Policy To Restrict Access to Datasets Imported to Data Wrangler</a>.
        </p></div></div><div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>In addition, if you use the managed policy for SageMaker, we strongly recommend that you
            scope it down to the most restrictive policy that allows you to perform your use case.
            For more information, see <a href="data-wrangler-security.html#data-wrangler-security-iam-policy">Grant an IAM Role Permission
                    to Use Data Wrangler</a>.</p></div></div><p>All data sources except for Amazon Simple Storage Service (Amazon S3) require you to specify a SQL query to import your data. For each query, you must specify the following:</p><div class="itemizedlist">
         
         
         
    <ul class="itemizedlist"><li class="listitem">
            <p><b>Data catalog</b></p>
        </li><li class="listitem">
            <p><b>Database</b></p>
        </li><li class="listitem">
            <p><b>Table</b></p>
        </li></ul></div><p>You can specify the name of the database or the data catalog in either the drop down menus or within the query. The following are example queries:</p><div class="itemizedlist">
         
         
         
    <ul class="itemizedlist"><li class="listitem">
            <p><code class="code">select * from <code class="replaceable">example-data-catalog-name</code>.<code class="replaceable">example-database-name</code>.<code class="replaceable">example-table-name</code></code> – The query doesn't use anything specified in the dropdown menus of the user-interface (UI) to run. 
                It queries <code class="code">example-table-name</code> within <code class="code">example-database-name</code> within <code class="code">example-data-catalog-name</code>.</p>
        </li><li class="listitem">
            <p><code class="code">select * from <code class="replaceable">example-database-name</code>.<code class="replaceable">example-table-name</code></code> – The query uses the data catalog that you've specified in the <b>Data catalog</b> dropdown menu to run. It queries <code class="code">example-table-name</code> within <code class="code">example-database-name</code> within the data catalog that you've specified.</p>
        </li><li class="listitem">
            <p><code class="code">select * from <code class="replaceable">example-table-name</code></code> – The query requires you to select fields for both the <b>Data catalog</b> and <b>Database name</b> dropdown menus. It queries <code class="code">example-table-name</code> within the data catalog within the database and data catalog that you've specified.</p>
        </li></ul></div><p>The link between Data Wrangler and the data source is a <em>connection</em>. You use the connection to import data from your data source.</p><p>There are the following types of connections:</p><div class="itemizedlist">
         
         
    <ul class="itemizedlist"><li class="listitem">
            <p>Direct</p>
        </li><li class="listitem">
            <p>Cataloged</p>
        </li></ul></div><p>Data Wrangler always has access to the most recent data in a direct connection. If the data in the data source has been updated, you can use the connection to import the data. For example, if someone adds a file to one of your Amazon S3 buckets, you can import the file.</p><p>A cataloged connection is the result of a data transfer. The data in the cataloged connection doesn't necessarily have the most recent data. For example, you might set up a data transfer between Salesforce and Amazon S3. If there's an update to the Salesforce data, you must transfer the data again.
        You can automate the process of transferring data. For more information about data transfers, see <a href="data-wrangler-import.html#data-wrangler-import-saas">Import Data From Software as a Service (SaaS) Platforms</a>.</p>
        <h2 id="data-wrangler-import-s3">Import data from Amazon S3</h2>
        <p>You can use Amazon Simple Storage Service (Amazon S3) to store and retrieve any amount of data, at any time,
            from anywhere on the web. You can accomplish these tasks using the AWS Management Console, which is a
            simple and intuitive web interface, and the Amazon S3 API. If you've stored your dataset
            locally, we recommend that you add it to an S3 bucket for import into Data Wrangler. To learn
            how, see <a href="https://docs.aws.amazon.com/AmazonS3/latest/gsg/PuttingAnObjectInABucket.html">Uploading an object to a bucket</a> in the Amazon Simple Storage Service User Guide. </p>
        <p>Data Wrangler uses <a href="http://aws.amazon.com/s3/features/#s3-select" rel="noopener noreferrer" target="_blank"><span>S3 Select</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> to
            allow you to preview your Amazon S3 files in Data Wrangler. You incur standard charges for each file
            preview. To learn more about pricing, see the <b>Requests &amp; data
                retrievals</b> tab on <a href="http://aws.amazon.com/s3/pricing/" rel="noopener noreferrer" target="_blank"><span>Amazon S3
                pricing</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. </p>
        <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>If you plan to export a data flow and launch a Data Wrangler job, ingest data into a SageMaker
                feature store, or create a SageMaker pipeline, be aware that these integrations require
                Amazon S3 input data to be located in the same AWS region.</p></div></div>
        <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>If you're importing a CSV file, make sure it meets the following
                requirements:</p><div class="itemizedlist">
                 
                 
                 
            <ul class="itemizedlist"><li class="listitem">
                    <p>A record in your dataset can't be longer than one line.</p>
                </li><li class="listitem">
                    <p>A backslash, <code class="code">\</code>, is the only valid escape character.</p>
                </li><li class="listitem">
                    <p>Your dataset must use one of the following delimiters:</p>
                    <div class="itemizedlist">
                         
                         
                         
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p>Comma – <code class="code">,</code></p>
                        </li><li class="listitem">
                            <p>Colon – <code class="code">:</code></p>
                        </li><li class="listitem">
                            <p>Semicolon – <code class="code">;</code></p>
                        </li><li class="listitem">
                            <p>Pipe – <code class="code">|</code></p>
                        </li><li class="listitem">
                            <p>Tab – <code class="code">[TAB]</code></p>
                        </li></ul></div>
                </li></ul></div><p>To save space, you can import compressed CSV files.</p></div></div>
        <p>Data Wrangler gives you the ability to either import the entire dataset or sample a portion of
            it. For Amazon S3, it provides the following sampling options:</p>
        <div class="itemizedlist">
             
             
             
             

        <ul class="itemizedlist"><li class="listitem">
                <p>None – Import the entire dataset.</p>
            </li><li class="listitem">
                <p>First K – Sample the first K rows of the dataset, where K is an integer
                    that you specify.</p>
            </li><li class="listitem">
                <p>Randomized – Takes a random sample of a size that you specify.</p>
            </li><li class="listitem">
                <p>Stratified – Takes a stratified random sample. A stratified sample
                    preserves the ratio of values in a column.</p>
            </li></ul></div>
        <p>After you've imported your data, you can also use the sampling transformer to take one
            or more samples from your entire dataset. For more information about the sampling
            transformer, see <a href="data-wrangler-transform.html#data-wrangler-transform-sampling">Sampling</a>.</p>

        <p>You can use one of the following resource identifiers to import your data:</p>
        <div class="itemizedlist">
             
             
                         
        <ul class="itemizedlist"><li class="listitem">
                <p>An Amazon S3 URI that uses an Amazon S3 bucket or Amazon S3 access point</p>
            </li><li class="listitem">
                <p>An Amazon S3 access point alias</p>
            </li><li class="listitem">
                <p>An Amazon Resource Name (ARN) that uses an Amazon S3 access point or Amazon S3 bucket</p>
            </li></ul></div>
        <p>Amazon S3 access points are named network endpoints that are attached to the buckets.
            Each access point has distinct permissions and network controls that you can configure. 
            For more information about access points, see <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points.html">Managing data access with Amazon S3 access points</a>.</p>
        <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>If you're using an Amazon Resource Name (ARN) to import your data, it must be for a resource located in the same AWS Region that you're using to access Amazon SageMaker Studio.</p></div></div>
        <p>You can import either a single file or multiple files as a dataset. You can use the
            multifile import operation when you have a dataset that is partitioned into separate
            files. It takes all of the files from an Amazon S3 directory and imports them as a single
            dataset. For information on the types of files that you can import and how to import
            them, see the following sections.</p>
        <awsdocs-tabs><dl style="display: none">
            <dt>Single File Import</dt><dd tab-id="single-file-import">
                    <p>You can import single files in the following formats:</p>
                    <div class="itemizedlist">
                         
                         
                         
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p>Comma Separated Values (CSV)</p>
                        </li><li class="listitem">
                            <p>Parquet</p>
                        </li><li class="listitem">
                            <p>Javascript Object Notation (JSON)</p>
                        </li><li class="listitem">
                            <p>Optimized Row Columnar (ORC)</p>
                        </li><li class="listitem">
                            <p>Image – Data Wrangler uses OpenCV to import images. For more information about supported image formats, see <a href="https://docs.opencv.org/3.4/d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56" rel="noopener noreferrer" target="_blank"><span>Image file reading and writing</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                        </li></ul></div>
                    <p>For files formatted in JSON, Data Wrangler supports both JSON lines (.jsonl) and
                        JSON documents (.json). When you preview your data, it automatically shows
                        the JSON in tabular format. For nested JSON documents that are larger than 5
                        MB, Data Wrangler shows the schema for the structure and the arrays as values in the
                        dataset. Use the <b>Flatten structured</b> and
                            <b>Explode array</b> operators to display the nested
                        values in tabular format. For more information, see <a href="data-wrangler-transform.html#data-wrangler-transform-flatten-column">Unnest JSON Data</a> and <a href="data-wrangler-transform.html#data-wrangler-transform-explode-array">Explode Array</a>.</p>
                    <p>When you choose a dataset, you can rename it, specify the file type, and
                        identify the first row as a header.</p>
                    <p>You can import a dataset that you've partitioned into multiple files in an
                        Amazon S3 bucket in a single import step.</p>
                    <div class="procedure"><h6>To import a dataset into Data Wrangler from a single file that you've stored
                            in Amazon S3:</h6><ol><li>
                            <p>If you are not currently on the <b>Import</b> tab,
                                choose <b>Import</b>.</p>
                        </li><li>
                            <p>Under <b>Available</b>, choose
                                    <b>Amazon S3</b>.</p>
                        </li><li>
                            <p>From the <b>Import tabular, image, or time-series data from S3</b>, do one of the following:</p>
                            <ul>
                                <li>
                                    <p>Choose an Amazon S3 bucket from the tabular view and navigate to the file that you're importing.</p>
                                </li>
                                <li>
                                    <p>For <b>S3 source</b>, specify an Amazon S3 bucket or an Amazon S3 URI and select <b>Go</b>. The Amazon S3 URIs can be in one of the following formats:</p>
                                    
                                            <div class="itemizedlist">
                                                 
                                                 
                                                 
                                            <ul class="itemizedlist"><li class="listitem">
                                                    <p><code class="code">s3://<code class="replaceable"><code class="replaceable">DOC-EXAMPLE-BUCKET</code></code>/<code class="replaceable">example-prefix</code>/<code class="replaceable">example-file</code></code></p>
                                                </li><li class="listitem">
                                                    <p><code class="replaceable">example-access-point</code>-<code class="replaceable">aqfqprnstn7aefdfbarligizwgyfouse1a</code>-s3alias/datasets/<code class="replaceable">example-file</code></p>
                                                </li><li class="listitem">
                                                    <p><code class="code">s3://arn:aws:s3:<code class="replaceable">AWS-Region</code>:<code class="replaceable">111122223333</code>:accesspoint/<code class="replaceable">example-prefix</code>/<code class="replaceable">example-file</code></code></p>
                                                </li></ul></div>
                              
                                       
                                    
                                </li>
          
                            </ul>
                            
                        </li><li>
                            <p>Choose the dataset to open the <b>Import settings</b> pane.</p>
                        </li><li>
                            <p>If your CSV file has a header, select the checkbox next to
                                    <b>Add header to table</b>.</p>
                        </li><li>
                            <p>Use the <b>Preview</b> table to preview your
                                dataset. This table shows up to 100 rows. </p>
                        </li><li>
                            <p>In the <b>Details</b> pane, verify or change the
                                    <b>Name</b> and <b>File Type</b> for
                                your dataset. If you add a <b>Name</b> that contains
                                spaces, these spaces are replaced with underscores when your dataset
                                is imported. </p>
                        </li><li>
                            <p>Specify the sampling configuration that you'd like to use. </p>
                        </li><li>
                            <p>Choose <b>Import</b>.</p>
                        </li></ol></div>
                </dd>
            <dt>Multifile Import</dt><dd tab-id="multifile-import">
                    <p>The following are the requirements for importing multiple files:</p>
                    <div class="itemizedlist">
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p>The files must be in the same folder of your Amazon S3 bucket.</p>
                        </li><li class="listitem">
                            <p>The files must either share the same header or have no
                                header.</p>
                        </li></ul></div>
                    <p>Each file must be in one of the following formats:</p>
                    <div class="itemizedlist">
                         
                         
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p>CSV</p>
                        </li><li class="listitem">
                            <p>Parquet</p>
                        </li><li class="listitem">
                            <p>Optimized Row Columnar (ORC)</p>
                        </li><li class="listitem">
                            <p>Image – Data Wrangler uses OpenCV to import images. For more information about supported image formats, see <a href=" https://docs.opencv.org/3.4/d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56" rel="noopener noreferrer" target="_blank"><span>Image file reading and writing</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                        </li></ul></div>
                    <p>Use the following procedure to import multiple files.</p>
                    <div class="procedure"><h6>To import a dataset into Data Wrangler from multiple files that you've stored
                            in an Amazon S3 directory</h6><ol><li>
                            <p>If you are not currently on the <b>Import</b> tab,
                                choose <b>Import</b>.</p>
                        </li><li>
                            <p>Under <b>Available</b>, choose
                                <b>Amazon S3</b>.</p>
                        </li><li>
                            <p>From the <b>Import tabular, image, or time-series data from S3</b>, do one of the following:</p>
                            <ul>
                                <li>
                                    <p>Choose an Amazon S3 bucket from the tabular view and navigate to the folder containing the files that you're importing.</p>
                                </li>
                                <li>
                                    <p>For <b>S3 source</b>, specify the Amazon S3 bucket or an Amazon S3 URI with your files and select <b>Go</b>. 
                                        The following are valid URIs:</p>
                                   
                                            <div class="itemizedlist">
                                                 
                                                 
                                                 
                                            <ul class="itemizedlist"><li class="listitem">
                                                    <p><code class="code">s3://<code class="replaceable"><code class="replaceable">DOC-EXAMPLE-BUCKET</code></code>/<code class="replaceable">example-prefix</code>/<code class="replaceable">example-prefix</code></code></p>
                                                </li><li class="listitem">
                                                    <p><code class="code"><code class="replaceable">example-access-point</code>-<code class="replaceable">aqfqprnstn7aefdfbarligizwgyfouse1a</code>-s3alias/<code class="replaceable">example-prefix</code>/</code></p>
                                                </li><li class="listitem">
                                                    <p><code class="code">s3://arn:aws:s3:<b>AWS-Region</b>:<code class="replaceable">111122223333</code>:accesspoint/<code class="replaceable">example-prefix</code></code></p>
                                                </li></ul></div>
                                </li>
                            </ul>
                            
                        </li><li>
                            <p>Select the folder containing the files that you want to import.
                                Each file must be in one of the supported formats. Your files must
                                be the same data type.</p>
                        </li><li>
                            <p>If your folder contains CSV files with headers, select the
                                checkbox next to <b>First row is header</b>.</p>
                        </li><li>
                            <p>If your files are nested within other folders, select the checkbox
                                next to <b>Include nested directories</b>.</p>
                        </li><li>
                            <p>(Optional) Choose <b>Add filename column</b> add a
                                column to the dataset that shows the filename for each
                                observation.</p>
                        </li><li>
                            <p>(Optional) By default, Data Wrangler doesn't show you a preview of a
                                folder. You can activate previewing by choosing the blue
                                    <b>Preview off</b> button. A preview shows the
                                first 10 rows of the first 10 files in the folder.</p>
                            
                        </li><li>
                            <p>In the <b>Details</b> pane, verify or change the
                                    <b>Name</b> and <b>File Type</b> for
                                your dataset. If you add a <b>Name</b> that contains
                                spaces, these spaces are replaced with underscores when your dataset
                                is imported. </p>
                        </li><li>
                            <p>Specify the sampling configuration that you'd like to use. </p>
                        </li><li>
                            <p>Choose <b>Import dataset</b>.</p>
                        </li></ol></div>
                </dd>
        </dl></awsdocs-tabs>
        <p>You can also use parameters to import a subset of files that match a pattern. Parameters help you more selectively pick the files that you're importing. 
            To start using parameters, edit the data source and apply them to the path that you're using to import the data. 
            For more information, see <a href="data-wrangler-parameterize.html">Reusing Data Flows for Different Datasets</a>.</p>
        
     
        <h2 id="data-wrangler-import-athena">Import data from Athena</h2>
        <p>Use Amazon Athena to import your data from Amazon Simple Storage Service (Amazon S3) into Data Wrangler. In Athena, you write standard SQL queries to select the data that you're importing from Amazon S3. For more information, see <a href="https://docs.aws.amazon.com/athena/latest/ug/what-is.html">What is Amazon Athena?</a></p>
        <p>You can use the AWS Management Console to set up Amazon Athena. You must create at least one database in Athena before you start running queries. For more information about getting started with Athena, see <a href="https://docs.aws.amazon.com/athena/latest/ug/getting-started.html">Getting started</a>.</p>
        <p>Athena is directly integrated with Data Wrangler. You can write Athena queries without having to leave the Data Wrangler UI.</p>
        <p>In addition to writing simple Athena queries in Data Wrangler, you can also use:</p>
        <div class="itemizedlist">
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p>Athena workgroups for query result management. For more information about workgroups, see <a href="data-wrangler-import.html#data-wrangler-import-manage-results">Managing query results</a>.</p>
            </li><li class="listitem">
                <p>Lifecycle configurations for setting data retention periods. For more
                    information about data retention, see <a href="data-wrangler-import.html#data-wrangler-import-athena-retention">Setting data retention periods</a>.</p>
            </li></ul></div>
        
         
            <h3 id="data-wrangler-import-athena-query">Query Athena within Data Wrangler</h3>
            
            
            <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Data Wrangler does not support federated queries.</p></div></div>
            
            <p>If you use AWS Lake Formation with Athena, make sure your Lake Formation IAM permissions do not override
                IAM permissions for the database <code class="code">sagemaker_data_wrangler</code>.</p>
            
            <p>Data Wrangler gives you the ability to either import the entire dataset or sample a portion of
                it. For Athena, it provides the following sampling options:</p>
            <div class="itemizedlist">
                 
                 
                 
                 
                
                
            <ul class="itemizedlist"><li class="listitem">
                    <p>None – Import the entire dataset.</p>
                </li><li class="listitem">
                    <p>First K – Sample the first K rows of the dataset, where K is an integer
                        that you specify.</p>
                </li><li class="listitem">
                    <p>Randomized – Takes a random sample of a size that you specify.</p>
                </li><li class="listitem">
                    <p>Stratified – Takes a stratified random sample. A stratified sample
                        preserves the ratio of values in a column.</p>
                </li></ul></div>
            <p>The following procedure shows how to import a dataset from Athena into Data Wrangler.</p>
            
            <div class="procedure"><h6>To import a dataset into Data Wrangler from Athena</h6><ol><li>
                    <p>Sign into <a href="https://console.aws.amazon.com/sagemaker" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker
                        Console</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                </li><li>
                    <p>Choose <b>Studio</b>.</p>
                </li><li>
                    <p>Choose <b>Launch app</b>.</p>
                </li><li>
                    <p>From the dropdown list, select <b>Studio</b>.</p>
                </li><li>
                    <p>Choose the Home icon.</p>
                </li><li>
                    <p>Choose <b>Data</b>.</p>
                </li><li>
                    <p>Choose <b>Data Wrangler</b>.</p>
                </li><li>
                    <p>Choose <b>Import data</b>.</p>
                </li><li>
                    <p>Under <b>Available</b>, choose
                        <b>Amazon Athena</b>.</p>
                </li><li>
                    <p>For <b>Data Catalog</b>, choose a data catalog.</p>
                </li><li>
                    <p>Use the <b>Database</b> dropdown list to select the database
                        that you want to query. When you select a database, you can preview all tables
                        in your database using the <b>Tables</b> listed under
                        <b>Details</b>.</p>
                </li><li>
                    <p>(Optional) Choose <b>Advanced configuration</b>.</p>
                    <ol><li>
                            <p>Choose a <b>Workgroup</b>.</p>
                        </li><li>
                            <p>If your workgroup hasn't enforced the Amazon S3 output location or if you
                                don't use a workgroup, specify a value for <b>Amazon S3 location of
                                    query results</b>.</p>
                        </li><li>
                            <p>(Optional) For <b>Data retention period</b>, select the checkbox 
                                to set a data retention period and specify the number of days to store the data before it's deleted.</p>
                        </li><li>
                            <p>(Optional) By default, Data Wrangler saves the connection. You can choose to deselect the checkbox and not save the connection.</p>
                        </li></ol>
                </li><li>
                    <p>For <b>Sampling</b>, choose a sampling method. Choose
                        <b>None</b> to turn off sampling.</p>
                </li><li>
                    <p>Enter your query in the query editor and use the <b>Run</b>
                        button to run the query. After a successful query, you can preview your result
                        under the editor.</p>
                    <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Salesforce data uses the <code class="code">timestamptz</code> type. 
                           If you're querying the timestamp column that you've imported to Athena from Salesforce, cast the data in the column to the <code class="code">timestamp</code> type.
                        The following query casts the timestamp column to the correct type.</p><pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="sql ">
# cast column timestamptz_col as timestamp type, and name it as timestamp_col
select cast(timestamptz_col as timestamp) as timestamp_col from table
                        </code></pre></div></div>
                </li><li>
                    <p>To import the results of your query, select
                        <b>Import</b>.</p>
                </li></ol></div>
            <p>After you complete the preceding procedure, the dataset that you've queried and
                imported appears in the Data Wrangler flow.</p>
            <p>By default, Data Wrangler saves the connection settings as a new connection. When you import your data, the query that you've already specified appears as a new connection.
                The saved connections store information about the Athena workgroups and Amazon S3 buckets that you're using. When you're connecting to the data source again, you can choose the saved connection.</p>
            
         
         
            <h3 id="data-wrangler-import-manage-results">Managing query results</h3>
            
            <p>Data Wrangler supports using Athena workgroups to manage the query results within an AWS
                account. You can specify an Amazon S3 output location for each workgroup. You can also
                specify whether the output of the query can go to different Amazon S3 locations. For more
                information, see <a href="https://docs.aws.amazon.com/athena/latest/ug/manage-queries-control-costs-with-workgroups.html">Using
                    Workgroups to Control Query Access and Costs</a>.</p>
            <p>Your workgroup might be configured to enforce the Amazon S3 query output location. You can't change the output location of the query results for those workgroups.</p>
            <p>If you don't use a workgroup or specify an output location for your queries, Data Wrangler uses the default Amazon S3 bucket in the same AWS Region in which your Studio
                instance is located to store Athena query results. It creates temporary tables in this
                database to move the query output to this Amazon S3 bucket. It deletes these tables after
                data has been imported; however the database, <code class="code">sagemaker_data_wrangler</code>,
                persists. To learn more, see <a href="data-wrangler-import.html#data-wrangler-import-storage">Imported Data Storage</a>.</p>
            
            <p>To use Athena workgroups, set up the IAM policy that gives access to workgroups. If
                you're using a <code class="code">SageMaker-Execution-Role</code>, we recommend adding the policy to the
                role. For more information about IAM policies for workgroups, see <a href="https://docs.aws.amazon.com/athena/latest/ug/workgroups-iam-policy.html">IAM
                    policies for accessing workgroups</a>. For example workgroup policies, see
                <a href="https://docs.aws.amazon.com/athena/latest/ug/example-policies-workgroup.html">Workgroup example policies</a>.</p>
         
        
     
        
         
            <h3 id="data-wrangler-import-athena-retention">Setting data retention periods</h3>
            <p>Data Wrangler automatically sets a data retention period for the query results. The results are deleted after the length of the retention period. For example, the default retention period is five days. 
                The results of the query are deleted after five days. This configuration is designed to help you clean up data that you're no longer using.
                Cleaning up your data prevents unauthorized users from gaining access. It also helps control the costs of storing your data on Amazon S3.</p>
            <p>If you don't set a retention period, the Amazon S3 lifecycle configuration determines
                the duration that the objects are stored. The data retention policy that you've
                specified for the lifecycle configuration removes any query results that are older
                than the lifecycle configuration that you've specified. For more information, see
                    <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/how-to-set-lifecycle-configuration-intro.html">Setting lifecycle configuration on a bucket</a>.</p>
            <p>Data Wrangler uses Amazon S3 lifecycle configurations to manage data retention and expiration. You
                must give your Amazon SageMaker Studio IAM execution role permissions to manage bucket
                lifecycle configurations. Use the following procedure to give permissions.</p>
         
            
            <div class="procedure"><p>To give permissions to manage the lifecycle configuration do the
                    following.</p><ol><li> <p>Sign in to the AWS Management Console and open the IAM console at <a href="https://console.aws.amazon.com/iam/" rel="noopener noreferrer" target="_blank"><span>https://console.aws.amazon.com/iam/</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p> </li><li>
                    <p>Choose <b>Roles</b>.</p>
                </li><li>
                    <p>In the search bar, specify the Amazon SageMaker execution role that Amazon SageMaker Studio is
                        using.</p>
                </li><li>
                    <p>Choose the role.</p>
                </li><li>
                    <p>Choose <b>Add permissions</b>.</p>
                </li><li>
                    <p>Choose <b>Create inline policy</b>.</p>
                </li><li>
                    <p>For <b>Service</b>, specify <b>S3</b>
                        and choose it.</p>
                </li><li>
                    <p>Under the <b>Read</b> section, choose
                            <b>GetLifecycleConfiguration</b>.</p>
                </li><li>
                    <p>Under the <b>Write</b> section, choose
                            <b>PutLifecycleConfiguration</b>.</p>
                </li><li>
                    <p>For <b>Resources</b>, choose <b>Specific</b>.</p>
                </li><li>
                    <p>For <b>Actions</b>, select the arrow icon next to
                        <b>Permissions management</b>.</p>
                </li><li>
                    <p>Choose <b>PutResourcePolicy</b>.</p>
                </li><li>
                    <p>For <b>Resources</b>, choose
                        <b>Specific</b>.</p>
                </li><li>
                    <p>Choose the checkbox next to <b>Any in this account</b>.</p>
                </li><li>
                    <p>Choose <b>Review policy</b>.</p>
                </li><li>
                    <p>For <b>Name</b>, specify a name.</p>
                </li><li>
                    <p>Choose <b>Create policy</b>.</p>
                </li></ol></div>
            
         
        
      
  

      


     
        <h2 id="data-wrangler-import-redshift">Import data from Amazon Redshift</h2>
        <p>Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. The first
            step to create a data warehouse is to launch a set of nodes, called an Amazon Redshift cluster.
            After you provision your cluster, you can upload your dataset and then perform data
            analysis queries. </p>
        <p>You can connect to and query one or more Amazon Redshift clusters in Data Wrangler. To use this import
            option, you must create at least one cluster in Amazon Redshift. To learn how, see <a href="https://docs.aws.amazon.com/redshift/latest/gsg/getting-started.html">Getting started
                with Amazon Redshift</a>.</p>
        <p>You can output the results of your Amazon Redshift query in one of the following
            locations:</p>
        <div class="itemizedlist">
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p>The default Amazon S3 bucket</p>
            </li><li class="listitem">
                <p>An Amazon S3 output location that you specify</p>
            </li></ul></div>
        <p>You can either import the entire dataset or sample a portion of it. For Amazon Redshift, it
            provides the following sampling options:</p>
        <div class="itemizedlist">
             
             
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p>None – Import the entire dataset.</p>
            </li><li class="listitem">
                <p>First K – Sample the first K rows of the dataset, where K is an integer
                    that you specify.</p>
            </li><li class="listitem">
                <p>Randomized – Takes a random sample of a size that you specify.</p>
            </li><li class="listitem">
                <p>Stratified – Takes a stratified random sample. A stratified sample
                    preserves the ratio of values in a column.</p>
            </li></ul></div>
        <p>The default Amazon S3 bucket is in the same AWS Region in which your Studio instance
            is located to store Amazon Redshift query results. For more information, see <a href="data-wrangler-import.html#data-wrangler-import-storage">Imported Data Storage</a>.</p>
        <p>For either the default Amazon S3 bucket or the bucket that you specify, you have the
            following encryption options:</p>
        <div class="itemizedlist">
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p>The default AWS service-side encryption with an Amazon S3 managed key
                    (SSE-S3)</p>
            </li><li class="listitem">
                <p> An AWS Key Management Service (AWS KMS) key that you specify</p>
            </li></ul></div>
        <p>An AWS KMS key is an encryption key that you create and manage. For more information on
            KMS keys, see <a href="https://docs.aws.amazon.com/kms/latest/developerguide/overview.html">AWS Key Management Service</a>.</p>
        <p>You can specify an AWS KMS key using either the key ARN or the ARN of your AWS
            account.</p>
        <p>If you use the IAM managed policy, <code class="code">AmazonSageMakerFullAccess</code>, to grant
            a role permission to use Data Wrangler in Studio, your <b>Database User</b>
            name must have the prefix <code class="code">sagemaker_access</code>.</p>
        <p>Use the following procedures to learn how to add a new cluster. </p>
        <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Data Wrangler uses the Amazon Redshift Data API with temporary credentials. To learn more about this
                API, refer to <a href="https://docs.aws.amazon.com/redshift/latest/mgmt/data-api.html">Using the Amazon Redshift Data API</a> in the Amazon Redshift Management Guide. </p></div></div>
        <div class="procedure"><h6>To connect to a Amazon Redshift cluster</h6><ol><li>
                <p>Sign into <a href="https://console.aws.amazon.com/sagemaker" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker
                    Console</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
            </li><li>
                <p>Choose <b>Studio</b>.</p>
            </li><li>
                <p>Choose <b>Launch app</b>.</p>
            </li><li>
                <p>From the dropdown list, select <b>Studio</b>.</p>
            </li><li>
                <p>Choose the Home icon.</p>
            </li><li>
                <p>Choose <b>Data</b>.</p>
            </li><li>
                <p>Choose <b>Data Wrangler</b>.</p>
            </li><li>
                <p>Choose <b>Import data</b>.</p>
            </li><li>
                <p>Under <b>Available</b>, choose
                    <b>Amazon Athena</b>.</p>
            </li><li>
                <p>Choose <b>Amazon Redshift</b>.</p>
            </li><li>
                <p>Choose <b>Temporary credentials (IAM)</b> for
                        <b>Type</b>.</p>
            </li><li>
                <p>Enter a <b>Connection Name</b>. This is a name used by Data Wrangler to
                    identify this connection. </p>
            </li><li>
                <p>Enter the <b>Cluster Identifier</b> to specify to which cluster
                    you want to connect. Note: Enter only the cluster identifier and not the full
                    endpoint of the Amazon Redshift cluster.</p>
            </li><li>
                <p>Enter the <b>Database Name</b> of the database to which you want
                    to connect.</p>
            </li><li>
                <p>Enter a <b>Database User</b> to identify the user you want to
                    use to connect to the database. </p>
            </li><li>
                <p>For <b>UNLOAD IAM Role</b>, enter the IAM role ARN of the role
                    that the Amazon Redshift cluster should assume to move and write data to Amazon S3. For more
                    information about this role, see <a href="https://docs.aws.amazon.com/redshift/latest/mgmt/authorizing-redshift-service.html">Authorizing
                        Amazon Redshift to access other AWS services on your behalf</a> in the
                    Amazon Redshift Management Guide. </p>
            </li><li>
                <p>Choose <b>Connect</b>.</p>
            </li><li>
                <p>(Optional) For <b>Amazon S3 output location</b>, specify the S3 URI
                    to store the query results.</p>
            </li><li>
                <p>(Optional) For <b>KMS key ID</b>, specify the ARN of the AWS KMS
                    key or alias. The following image shows you where you can find either key in the
                    AWS Management Console.</p>
                <div class="mediaobject">
                     
                        <img src="../../../images/sagemaker/latest/dg/images/studio/mohave/kms-alias-redacted.png" class="aws-docs-img-whiteBg aws-docs-img-padding" />
                     
                </div>
            </li></ol></div>
        <p>The following image shows all the fields from the preceding procedure.</p>
        <div class="mediaobject">
             
                <img src="../../../images/sagemaker/latest/dg/images/studio/mohave/redshift-connection.png" class="aws-docs-img-whiteBg aws-docs-img-padding" style="max-width:90%" />
             
        </div>
        <p>After your connection is successfully established, it appears as a data source under
                <b>Data Import</b>. Select this data source to query your database and
            import data.</p>
        <div class="procedure"><h6>To query and import data from Amazon Redshift</h6><ol><li>
                <p>Select the connection that you want to query from <b>Data
                        Sources</b>.</p>
            </li><li>
                <p>Select a <b>Schema</b>. To learn more about Amazon Redshift Schemas, see
                        <a href="https://docs.aws.amazon.com/redshift/latest/dg/r_Schemas_and_tables.html">Schemas</a> in the
                    Amazon Redshift Database Developer Guide.</p>
            </li><li>
                <p>(Optional) Under <b>Advanced configuration</b>, specify the
                        <b>Sampling</b> method that you'd like to use.</p>
            </li><li>
                <p>Enter your query in the query editor and choose <b>Run</b> to
                    run the query. After a successful query, you can preview your result under the
                    editor.</p>
            </li><li>
                <p>Select <b>Import dataset</b> to import the dataset that has been
                    queried. </p>
            </li><li>
                <p>Enter a <b>Dataset name</b>. If you add a <b>Dataset
                        name</b> that contains spaces, these spaces are replaced with
                    underscores when your dataset is imported. </p>
            </li><li>
                <p>Choose <b>Add</b>.</p>
            </li></ol></div>
        
        <div class="procedure"><p>To edit a dataset, do the following.</p><ol><li>
                <p>Navigate to your Data Wrangler flow.</p>
            </li><li>
                <p>Choose the + next to <b>Source - Sampled</b>.</p>
            </li><li>
                <p>Change the data that you're importing.</p>
            </li><li>
                <p>Choose <b>Apply</b></p>
            </li></ol></div>
     
        <h2 id="data-wrangler-emr">Import data from Amazon EMR</h2>
        <p>You can use Amazon EMR as a data source for your Amazon SageMaker Data Wrangler flow. Amazon EMR is a managed cluster
            platform that you can use process and analyze large amounts of data. For more
            information about Amazon EMR, see <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html">What is Amazon EMR?</a>. To
            import a dataset from EMR, you connect to it and query it. </p>
        <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>You must meet the following prerequisites to connect to an Amazon EMR cluster:</p><div class="itemizedlist">
                <h6>Prerequisites</h6>
                 
                 
                 
                 
           
                
               
            <ul class="itemizedlist"><li class="listitem">
                    <div class="itemizedlist">
                        <h6>Network configurations</h6>
                         
                         
                         
                         
                         
                         
                        
                    <ul class="itemizedlist"><li class="listitem">
                            <p>You have an Amazon VPC in the Region that you're using to launch Amazon SageMaker Studio
                                and Amazon EMR.</p>
                        </li><li class="listitem">
                            <p>Both Amazon EMR and Amazon SageMaker Studio must be launched in private subnets. They can be in the same subnet or in different ones.</p>
                        </li><li class="listitem">
                            <p>Amazon SageMaker Studio must be in VPC-only mode.</p>
                            <p>For more information about creating a VPC, see <a href="https://docs.aws.amazon.com/vpc/latest/userguide/working-with-vpcs.html#Create-VPC">Create a
                                VPC</a>.</p>
                            <p>For more information about creating a VPC, see <a href="https://docs.aws.amazon.com/vpc/latest/userguide/studio-notebooks-and-internet-access.html">Connect SageMaker Studio Notebooks in a VPC to External Resources</a>.</p>
                        </li><li class="listitem">
                            <p>The Amazon EMR clusters that you're running must be in the same Amazon VPC.</p>
                        </li><li class="listitem">
                            <p>The Amazon EMR clusters and the Amazon VPC must be in the same AWS account.</p>
                        </li><li class="listitem">
                            <p>Your Amazon EMR clusters are running Hive or Presto.</p>
                            <div class="itemizedlist">
                                 
                                 
                            <ul class="itemizedlist"><li class="listitem">
                                    <p>Hive clusters must allow inbound traffic from Studio security groups on port 10000.</p>
                                </li><li class="listitem">
                                    <p>Presto clusters must allow inbound traffic from Studio security groups on port 8889.</p>
                                    <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>The port number is different for Amazon EMR clusters using IAM roles. Navigate to the end of the prerequisites section for more information.</p></div></div>
                                </li></ul></div>
                        </li></ul></div>
                
                </li><li class="listitem">
                    <div class="itemizedlist">
                        <h6>SageMaker Studio</h6>
                         
                         
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p>Amazon SageMaker Studio must run Jupyter Lab Version 3. For information about updating the Jupyter Lab Version, see <a href="studio-jl.html#studio-jl-view">View and update the JupyterLab version of an application from the
        console</a>.</p>
                        </li><li class="listitem">
                            <p>Amazon SageMaker Studio has an IAM role that controls user access. The default IAM role that you're using to run Amazon SageMaker Studio doesn't have policies that can give you access to Amazon EMR clusters.
                                You must attach the policy granting permissions to the IAM role. For more information, see <a href="studio-notebooks-configure-discoverability-emr-cluster.html">Configure
                    the discoverability of Amazon EMR clusters (for administrators)</a>.</p>
                        </li><li class="listitem">
                            <p>The IAM role must also have the following policy attached <code class="code">secretsmanager:PutResourcePolicy</code>.</p>
                        </li><li class="listitem">
                            <p>If you're using a Studio domain that you've already created, make sure that its <code class="code">AppNetworkAccessType</code> is in VPC-only mode. For information about updating a domain to use VPC-only mode, see <a href="studio-tasks-update-studio.html">Shut down and Update SageMaker Studio</a>.</p>
                        </li></ul></div>
                </li><li class="listitem">
                    <div class="itemizedlist">
                        <h6>Amazon EMR clusters</h6>
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p>You must have Hive or Presto installed on your cluster.</p>
                        </li><li class="listitem">
                            <p>The Amazon EMR release must be version 5.5.0 or later.</p>
                            <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Amazon EMR supports auto termination. Auto termination stops idle clusters from running and prevents you from incurring costs. The following are the releases that support auto termination:</p><div class="itemizedlist">
                                     
                                     
                                <ul class="itemizedlist"><li class="listitem">
                                        <p>For 6.x releases, version 6.1.0 or later.</p>
                                    </li><li class="listitem">
                                        <p>For 5.x releases, version 5.30.0 or later.</p>
                                    </li></ul></div></div></div>
                        </li></ul></div>
                </li><li class="listitem">
                    <div class="itemizedlist">
                        <h6>Amazon EMR clusters using IAM runtime roles</h6>
                         
                         
                         
                         
                         
                        
                    <ul class="itemizedlist"><li class="listitem">
                            <p>Use the following pages to set up IAM runtime roles for the
                                Amazon EMR cluster. You must enable in-transit encryption when you're using runtime roles:</p>
                            <div class="itemizedlist">
                                 
                                 
                            <ul class="itemizedlist"><li class="listitem">
                                    <p><a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-steps-runtime-roles.html#emr-steps-runtime-roles-configure">Prerequisites for launching an Amazon EMR cluster with a
                                            runtime role</a></p>
                                </li><li class="listitem">
                                    <p><a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-steps-runtime-roles.html#emr-steps-runtime-roles-launch">Launch an Amazon EMR cluster with role-based access
                                            control</a></p>
                                </li></ul></div>
                        </li><li class="listitem">
                            <p>You must Lake Formation as a governance tool for the data within your
                                databases. You must also use external data filtering for access
                                control.</p>
                            <div class="itemizedlist">
                                 
                                 
                            <ul class="itemizedlist"><li class="listitem">
                                    <p>For more information about Lake Formation, see <a href="https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html">What is AWS Lake Formation?</a></p>
                                </li><li class="listitem">
                                    <p>For more information about integrating Lake Formation into Amazon EMR, see <a href="https://docs.aws.amazon.com/lake-formation/latest/dg/Integrating-with-LakeFormation.html">Integrating third-party services with Lake Formation</a>.</p>
                                </li></ul></div>
                        </li><li class="listitem">
                            <p>The version of your cluster must be 6.9.0 or later.</p>
                        </li><li class="listitem">
                            <p>Access to AWS Secrets Manager. For more information about Secrets Manager see
                                <a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html">What is AWS Secrets Manager?</a></p>
                        </li><li class="listitem">
                            <p>Hive clusters must allow inbound traffic from Studio security
                                groups on port 10000.</p>
                        </li></ul></div>
                </li></ul></div></div></div>
        <p>An Amazon VPC is a virtual network that is logically isolated from other networks on the AWS cloud. Amazon SageMaker Studio and your Amazon EMR cluster only exist within the Amazon VPC.</p>
        <p>Use the following procedure to launch Amazon SageMaker Studio in an Amazon VPC.</p>
        <div class="procedure"><p>To launch Studio within a VPC, do the following.</p><ol><li>
                <p>Navigate to the SageMaker console at <a href="https://console.aws.amazon.com/sagemaker/" rel="noopener noreferrer" target="_blank"><span>https://console.aws.amazon.com/sagemaker/</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
            </li><li>
                <p>Choose <b>Launch SageMaker Studio</b>.</p>
            </li><li>
                <p>Choose <b>Standard setup</b>.</p>
            </li><li>
                <p>For <b>Default execution role</b>, choose the IAM role to set
                    up Studio.</p>
            </li><li>
                <p>Choose the VPC where you've launched the Amazon EMR clusters.</p>
            </li><li>
                <p>For <b>Subnet</b>, choose a private subnet.</p>
            </li><li>
                <p>For <b>Security group(s)</b>, specify the security groups that
                    you're using to control between your VPC.</p>
            </li><li>
                <p>Choose <b>VPC Only</b>.</p>
            </li><li>
                <p>(Optional) AWS uses a default encryption key. You can specify an AWS Key Management Service
                    key to encrypt your data.</p>
            </li><li>
                <p>Choose <b>Next</b>.</p>
            </li><li>
                <p>Under <b>Studio settings</b>, choose the configurations that are
                    best suited to you.</p>
            </li><li>
                <p>Choose <b>Next</b> to skip the SageMaker Canvas settings.</p>
            </li><li>
                <p>Choose <b>Next</b> to skip the RStudio settings.</p>
            </li></ol></div>
        <p>If you don't have an Amazon EMR cluster ready, you can use the following procedure to create
            one. For more information about Amazon EMR, see <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html">What is Amazon EMR?</a></p>

        <div class="procedure"><p>To create a cluster, do the following.</p><ol><li>
                <p>Navigate to the AWS Management Console.</p>
            </li><li>
                <p>In the search bar, specify <code class="userinput">Amazon EMR</code>.</p>
            </li><li>
                <p>Choose <b>Create cluster</b>.</p>
            </li><li>
                <p>For <b>Cluster name</b>, specify the name of your
                    cluster.</p>
            </li><li>
                <p>For <b>Release</b>, select the release version of the
                    cluster.</p>
                <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Amazon EMR supports auto termination for the following releases:</p><div class="itemizedlist">
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p>For 6.x releases, releases 6.1.0 or later</p>
                        </li><li class="listitem">
                            <p>For 5.x releases, releases 5.30.0 or later</p>
                        </li></ul></div><p>Auto termination stops idle clusters from running and prevents you from
                        incurring costs.</p></div></div>
            </li><li>
                <p>(Optional) For <b>Applications</b>, choose <b>Presto</b>.</p>
            </li><li>
                <p>Choose the application that you're running on the cluster.</p>
            </li><li>
                <p>Under <b>Networking</b>, for <b>Hardware configuration</b>, specify the hardware
                    configuration settings.</p>
                <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>For <b>Networking</b>, choose the VPC that is running Amazon SageMaker Studio and choose a private subnet.</p></div></div>
            </li><li>
                <p>Under <b>Security and access</b>, specify the security
                    settings.</p>
            </li><li>
                <p>Choose <b>Create</b>.</p>
            </li></ol></div>

        <p>For a tutorial about creating an Amazon EMR cluster, see <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-gs.html">Getting started with Amazon EMR</a>. 
            For information about best practices for configuring a cluster, see <a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-ha-considerations.html">Considerations and best practices</a>.</p>
      
        <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>For security best practices, Data Wrangler can only connect to VPCs on private subnets. 
                You can't connect to the master node unless you use AWS Systems Manager for your Amazon EMR instances. For more information, see <a href="http://aws.amazon.com/blogs/big-data/securing-access-to-emr-clusters-using-aws-systems-manager/" rel="noopener noreferrer" target="_blank"><span>Securing access to EMR clusters using AWS Systems Manager</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p></div></div>
        
        
        <p>You can currently use the following methods to access an Amazon EMR cluster:</p>
        <div class="itemizedlist">
             
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p>No authentication</p>
            </li><li class="listitem">
                <p>Lightweight Directory Access Protocol (LDAP)</p>
            </li><li class="listitem">
                <p>IAM (Runtime role)</p>
            </li></ul></div>
        <p>Not using authentication or using LDAP can require you to create multiple clusters and Amazon EC2 instance profiles. 
            If you’re an administrator, you might need to provide groups of users with different levels of access to the data. These methods can result in administrative overhead that makes it more difficult to manage your users.</p>
        <p>We recommend using an IAM runtime role that gives multiple users the ability to connect to the same Amazon EMR cluster.
            A runtime role is an IAM role that you can assign to a user who is connecting to an Amazon EMR cluster. You can configure the runtime IAM role to have permissions that are specific to each group of users.</p>
        <p>Use the following sections to create a Presto or Hive Amazon EMR cluster with LDAP activated.</p>

       <awsdocs-tabs><dl style="display: none">
           <dt>Presto</dt><dd tab-id="presto">
                   <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>To use AWS Glue as a metastore for Presto tables, select <b>Use</b> for <b>Presto table metadata</b> to store the results of your Amazon EMR queries in a AWS Glue data catalog when you're launching an EMR cluster.
                           Storing the query results in a AWS Glue data catalog can save you from incurring charges.</p><p>To query large datasets on Amazon EMR clusters, you must add the following properties to the Presto configuration file on your Amazon EMR clusters:</p><pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="nohighlight">
[<span>{</span>"classification":"presto-config","properties":<span>{</span>
"http-server.max-request-header-size":"5MB",
"http-server.max-response-header-size":"5MB"}}]                
            </code></pre><p>You can also modify the configuration settings when you launch the Amazon EMR cluster.</p><p>The configuration file for your Amazon EMR cluster is located under the following path: <code class="code">/etc/presto/conf/config.properties</code>.</p></div></div>
                   <p>Use the following
                       procedure to create a Presto cluster with LDAP activated.</p>
                   <div class="procedure"><p>To create a cluster, do the following.</p><ol><li>
                           <p>Navigate to the AWS Management Console.</p>
                       </li><li>
                           <p>In the search bar, specify <code class="userinput">Amazon EMR</code>.</p>
                       </li><li>
                           <p>Choose <b>Create cluster</b>.</p>
                       </li><li>
                           <p>For <b>Cluster name</b>, specify the name of your
                               cluster.</p>
                       </li><li>
                           <p>For <b>Release</b>, select the release version of the
                               cluster.</p>
                           <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Amazon EMR supports auto termination for the following releases:</p><div class="itemizedlist">
                                    
                                    
                               <ul class="itemizedlist"><li class="listitem">
                                       <p>For 6.x releases, releases 6.1.0 or later</p>
                                   </li><li class="listitem">
                                       <p>For 5.x releases, releases 5.30.0 or later</p>
                                   </li></ul></div><p>Auto termination stops idle clusters from running and prevent you from
                                   incurring costs.</p></div></div>
                       </li><li>
                           <p>Choose the application that you're running on the cluster.</p>
                       </li><li>
                           <p>Under <b>Networking</b>, for <b>Hardware configuration</b>, specify the hardware
                               configuration settings.</p>
                           <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>For <b>Networking</b>, choose the VPC that is running Amazon SageMaker Studio and choose a private subnet.</p></div></div>
                       </li><li>
                           <p>Under <b>Security and access</b>, specify the security
                               settings.</p>
                       </li><li>
                           <p>Choose <b>Create</b>.</p>
                       </li></ol></div>
               </dd>
           <dt>Hive</dt><dd tab-id="hive">
                   <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>To use AWS Glue as a metastore for Hive tables, select <b>Use</b> for <b>Hive table metadata</b> to store the results of your Amazon EMR queries in a AWS Glue data catalog when you're launching an EMR cluster.
                           Storing the query results in a AWS Glue data catalog can save you from incurring charges.</p><p>To be able to query large datasets on Amazon EMR clusters, add the following properties to Hive configuration file on your Amazon EMR clusters:</p><pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="nohighlight">
[<span>{</span>"classification":"hive-site", "properties"
:<span>{</span>"hive.resultset.use.unique.column.names":"false"}}]            
            </code></pre><p>You can also modify the configuration settings when you launch the Amazon EMR cluster.</p><p>The configuration file for your Amazon EMR cluster is located under the following path: <code class="code">/etc/hive/conf/hive-site.xml</code>. You can specify the following property and restart the cluster:</p><pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="xml ">
&lt;property&gt;
    &lt;name&gt;hive.resultset.use.unique.column.names&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;
                       </code></pre></div></div>
                   <p>Use the following
                       procedure to create a Hive cluster with LDAP activated.</p>
                   <div class="procedure"><p>To create a Hive cluster with LDAP activated, do the following.</p><ol><li>
                           <p>Navigate to the AWS Management Console.</p>
                       </li><li>
                           <p>In the search bar, specify <code class="userinput">Amazon EMR</code>.</p>
                       </li><li>
                           <p>Choose <b>Create cluster</b>.</p>
                       </li><li>
                           <p>Choose <b>Go to advanced options</b>.</p>
                       </li><li>
                           <p>For <b>Release</b>, select an Amazon EMR release
                               version.</p>
                       </li><li>
                           <p>The <b>Hive</b> configuration option is selected by
                               default. Make sure the <b>Hive</b> option has a
                               checkbox next to it.</p>
                       </li><li>
                           <p>(Optional) You can also select <b>Presto</b> as a
                               configuration option to activate both Hive and Presto on your
                               cluster.</p>
                       </li><li>
                           <p>(Optional) Select <b>Use for Hive table metadata</b>
                               to store the results of your Amazon EMR queries in a AWS Glue data catalog.
                               Storing the query results in a AWS Glue catalog can save you from
                               incurring charges. For more information, see <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html">Using the AWS Glue Data Catalog as the metastore for Hive</a>.</p>
                           <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Storing the query results in a data catalog requires Amazon EMR version 5.8.0 or later.</p></div></div>
                       </li><li>
                           <p>Under <b>Enter configuration</b>, specify the
                               following JSON:</p>
                           <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="json ">
[
  <span>{</span>
    "classification": "hive-site",
    "properties": <span>{</span>
      "hive.server2.authentication.ldap.baseDN": "dc=<code class="replaceable">example</code>,dc=<code class="replaceable">org</code>",
      "hive.server2.authentication": "LDAP",
      "hive.server2.authentication.ldap.url": "ldap://<code class="replaceable">ldap-server-dns-name</code>:389"
    }
  }
]
</code></pre>
                           <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>As a security best practice, we recommend enabling SSL for HiveServer by adding a few properties in the preceding hive-site JSON.
                                   For more information, see <a href="https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.0.1/configuring-wire-encryption/content/enable_ssl_on_hiveserver2.html" rel="noopener noreferrer" target="_blank"><span>Enable SSL on HiveServer2</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p></div></div>
                       </li><li>
                           <p>Specify the remaining cluster settings and create a
                               cluster.</p>
                       </li></ol></div>
               </dd>
       </dl></awsdocs-tabs>
        
        <p>Use the following sections to use LDAP authentication for Amazon EMR clusters that you've already created.</p>
        <awsdocs-tabs><dl style="display: none">
            <dt>LDAP for Presto</dt><dd tab-id="ldap-for-presto">
                    <p>Using LDAP on a cluster running Presto requires access to the Presto coordinator through HTTPS. Do the following to provide access:</p>
                    <div class="itemizedlist">
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p>Activate access on port 636</p>
                        </li><li class="listitem">
                            <p>Enable SSL for the Presto coordinator</p>
                        </li></ul></div>
                    
                    <p>Use the following template to configure Presto:</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="nohighlight">
- Classification: presto-config
     ConfigurationProperties:
        http-server.authentication.type: 'PASSWORD'
        http-server.https.enabled: 'true'
        http-server.https.port: '8889'
        http-server.http.port: '8899'
        node-scheduler.include-coordinator: 'true'
        http-server.https.keystore.path: '/path/to/keystore/path/for/presto'
        http-server.https.keystore.key: <code class="replaceable">'keystore-key-password'</code>
        discovery.uri: 'http://<code class="replaceable">master-node-dns-name</code>:8899'
- Classification: presto-password-authenticator
     ConfigurationProperties:
        password-authenticator.name: 'ldap'
        ldap.url: !Sub 'ldaps://<code class="replaceable">ldap-server-dns-name</code>:636'
        ldap.user-bind-pattern: "uid=$<span>{</span>USER},dc=example,dc=org"
        internal-communication.authentication.ldap.user: <code class="replaceable">"ldap-user-name"</code>
        internal-communication.authentication.ldap.password: <code class="replaceable">"ldap-password"</code>                       
                    </code></pre>
                    <p>For information about setting up LDAP in Presto, see the following
                        resources:</p>
                    <div class="itemizedlist">
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p><a href="https://prestodb.io/docs/current/security/ldap.html" rel="noopener noreferrer" target="_blank"><span>LDAP Authentication</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                        </li><li class="listitem">
                            <p><a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-presto-ldap.html">Using LDAP
                                Authentication for Presto on Amazon EMR</a></p>
                        </li></ul></div>
                    <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>As a security best practice, we recommend enabling SSL for Presto. For more information, see <a href="https://prestodb.io/docs/current/security/internal-communication.html" rel="noopener noreferrer" target="_blank"><span>Secure Internal Communication</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p></div></div>
                </dd>
            <dt>LDAP for Hive</dt><dd tab-id="ldap-for-hive">
                    <p>To use LDAP for Hive for a cluster that you've created, use the following procedure <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-configure-apps-running-cluster.html#emr-configure-apps-running-cluster-considerations">Reconfigure an instance group in the console.</a></p>
                    <p>You're specifying the name of the cluster to which you're connecting.</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="json ">
[
  <span>{</span>
    "classification": "hive-site",
    "properties": <span>{</span>
      "hive.server2.authentication.ldap.baseDN": "dc=<code class="replaceable">example</code>,dc=<code class="replaceable">org</code>",
      "hive.server2.authentication": "LDAP",
      "hive.server2.authentication.ldap.url": "ldap://ldap-server-dns-name:389"
    }
  }
]                       
                    </code></pre>
                </dd>
            
        </dl></awsdocs-tabs>
        
        
        
        

            
                

        
       
        <p>Use the following procedure to import data from a cluster.</p>
        <div class="procedure"><p>To import data from a cluster, do the following.</p><ol><li>
                <p>Open a Data Wrangler flow.</p>
            </li><li>
                <p>Choose <b>Create Connection</b>.</p>
            </li><li>
                <p>Choose <b>Amazon EMR</b>.</p>
            </li><li>
                <p>Do one of the following.</p>
                <ul>
                    <li>
                        <p>(Optional) For <b>Secrets ARN</b>, specify the Amazon
                            Resource Number (ARN) of the database within the cluster. Secrets provide additional security. 
                            For more information about secrets, see <a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html">What is AWS Secrets Manager?</a>
                            For information about creating a secret for your cluster, see <a href="data-wrangler-import.html#data-wrangler-emr-secrets-manager">Creating a AWS Secrets Manager secret for your cluster</a>.</p>
                        <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>You must specify a secret if you're using an IAM runtime role for authentication.</p></div></div>
                    </li>
                    <li>
                        <p>From the dropdown table, choose a cluster.</p>
                    </li>
                </ul>
            </li><li>
                <p>Choose <b>Next</b>.</p>
            </li><li>
                <p>For <b>Select an endpoint for
                            <code class="replaceable">example-cluster-name</code> cluster</b>,
                    choose a query engine.</p>
            </li><li>
                <p>(Optional) Select <b>Save connection</b>.</p>
            </li><li>
                <p>Choose <b>Next, select login</b> and choose one of the following:</p>
                <div class="itemizedlist">
                     
                     
                     
                <ul class="itemizedlist"><li class="listitem">
                        <p>No authentication</p>
                    </li><li class="listitem">
                        <p>LDAP</p>
                    </li><li class="listitem">
                        <p>IAM</p>
                    </li></ul></div>
            </li><li>
                <p>For <b>Login into <code class="replaceable">example-cluster-name</code>
                        cluster</b>, specify the <b>Username</b> and
                        <b>Password</b> for the cluster.</p>
            </li><li>
                <p>Choose <b>Connect</b>.</p>
            </li><li>
                <p>In the query editor specify a SQL query.</p>
            </li><li>
                <p>Choose <b>Run</b>.</p>
            </li><li>
                <p>Choose <b>Import</b>.</p>
            </li></ol></div>
         
            <h3 id="data-wrangler-emr-secrets-manager">Creating a AWS Secrets Manager secret for your cluster</h3>
            <p>If you're using an IAM runtime role to access your Amazon EMR cluster, you must store the credentials that you're using to access the Amazon EMR as a Secrets Manager secret. You store all the credentials that you use to access the cluster within the secret.</p>
            <p>You must store the following information in the secret:</p>
            <div class="itemizedlist">
                 
                 
                 
            <ul class="itemizedlist"><li class="listitem">
                    <p>JDBC endpoint – <code class="code">jdbc:hive2://</code></p>
                </li><li class="listitem">
                    <p>DNS name – The DNS name of your Amazon EMR cluster. It's either the endpoint for the primary node or the hostname.</p>
                </li><li class="listitem">
                    <p>Port – <code class="code">8446</code></p>
                </li></ul></div>
            <p>You can also store the following additional information within the secret:</p>
            <div class="itemizedlist">
                 
                 
                 
            <ul class="itemizedlist"><li class="listitem">
                    <p>IAM role – The IAM role that you're using to access the cluster. Data Wrangler uses your SageMaker execution role by default.</p>
                </li><li class="listitem">
                    <p>Truststore path – By default, Data Wrangler creates a truststore path for you. You can also use your own truststore path. For more information about truststore paths, see <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/hs2-encryption-intransit.html">In-transit encryption in HiveServer2</a>.</p>
                </li><li class="listitem">
                    <p>Truststore password – By default, Data Wrangler creates a truststore password for you. You can also use your own truststore path. For more information about truststore paths, see <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/hs2-encryption-intransit.html">In-transit encryption in HiveServer2</a>.</p>
                </li></ul></div>
            <p>Use the following procedure to store the credentials within a Secrets Manager secret.</p>
            <div class="procedure"><p>To store your credentials as a secret, do the following.</p><ol><li>
                    <p>Navigate to the AWS Management Console.</p>
                </li><li>
                    <p>In the search bar, specify Secrets Manager.</p>
                </li><li>
                    <p>Choose <b>AWS Secrets Manager</b>.</p>
                </li><li>
                    <p>Choose <b>Store a new secret</b>.</p>
                </li><li>
                    <p>For <b>Secret type</b>, choose <b>Other type of secret</b>.</p>
                </li><li>
                    <p>Under <b>Key/value</b> pairs, select <b>Plaintext</b>.</p>
                </li><li>
                   
                       
                            <p>For clusters running Hive, you can use the following template for IAM authentication.</p>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="json ">
<span>{</span>"jdbcURL": ""
 "iam_auth": <span>{</span>"endpoint": "jdbc:hive2://", #required
                "dns": "ip-<code class="replaceable">xx-x-xxx-xxx</code>.ec2.internal", #required 
                "port": "10000", #required
              "cluster_id": "j-<code class="replaceable">xxxxxxxxx</code>", #required
              "iam_role": "arn:aws:iam::xxxxxxxx:role/xxxxxxxxxxxx", #optional
              "truststore_path": "/etc/alternatives/jre/lib/security/cacerts", #optional
              "truststore_password": "changeit" #optional
              
              }}                               
                            </code></pre>
                            <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>After you import your data, you apply transformations to them.
                                    You then export the data that you've transformed to a specific
                                    location. If you're using a Jupyter notebook to export your
                                    transformed data to Amazon S3, you must use the truststore
                                    path specified in the preceding example.</p></div></div>
                        
                        
                   
                </li></ol></div>
            <p>A Secrets Manager secret stores the JDBC URL of the Amazon EMR cluster as a secret. Using a secret is more secure than directly entering in your credentials.</p>
            <p>Use the following procedure to store the JDBC URL as a secret.</p>
            <div class="procedure"><p>To store the JDBC URL as a secret, do the following.</p><ol><li>
                    <p>Navigate to the AWS Management Console.</p>
                </li><li>
                    <p>In the search bar, specify Secrets Manager.</p>
                </li><li>
                    <p>Choose <b>AWS Secrets Manager</b>.</p>
                </li><li>
                    <p>Choose <b>Store a new secret</b>.</p>
                </li><li>
                    <p>For <b>Secret type</b>, choose <b>Other type of secret</b>.</p>
                </li><li>
                    <p>For <b>Key/value pairs</b>, specify <code class="code">jdbcURL</code> as the key and a valid JDBC URL as the value.</p>
                    <p>The format of a valid JDBC URL depends on whether you use authentication and whether you use Hive or Presto as the query engine. The following list shows the valid JBDC URL formats for the different possible configurations.</p>
                    <div class="itemizedlist">
                         
                         
                        
                         
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p>Hive, no authentication – <code class="code">jdbc:hive2://<code class="replaceable">emr-cluster-master-public</code>-dns:10000/;</code></p>
                        </li><li class="listitem">
                            <p>Hive, LDAP authentication – <code class="code">jdbc:hive2://<code class="replaceable">emr-cluster-master-public-dns-name</code>:10000/;AuthMech=3;UID=david;PWD=welcome123;</code></p>
                        </li><li class="listitem">
                            <p>For Hive with SSL enabled, the JDBC URL format depends on whether you use a Java Keystore File for the TLS configuration. The Java Keystore File helps verify the identity of the master node of the Amazon EMR cluster. To use a Java Keystore File, generate it on an EMR cluster and upload it to Data Wrangler. To generate a file, use the following command on the Amazon EMR cluster, <code class="code">keytool -genkey -alias hive -keyalg RSA -keysize 1024 -keystore hive.jks</code>. 
                                For information about running commands on an Amazon EMR cluster, see <a href="http://aws.amazon.com/blogs/big-data/securing-access-to-emr-clusters-using-aws-systems-manager/" rel="noopener noreferrer" target="_blank"><span>Securing access to EMR clusters using AWS Systems Manager</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. To upload a file, choose the upward arrow on the left-hand navigation of the Data Wrangler UI.</p>
                            <p>The following are the valid JDBC URL formats for Hive with SSL enabled:</p>
                            <div class="itemizedlist">
                                 
                                 
                            <ul class="itemizedlist"><li class="listitem">
                                    <p>Without a Java Keystore File – <code class="code">jdbc:hive2://<code class="replaceable">emr-cluster-master-public-dns</code>:10000/;AuthMech=3;UID=<code class="replaceable">user-name</code>;PWD=<code class="replaceable">password</code>;SSL=1;AllowSelfSignedCerts=1;</code></p> 
                                </li><li class="listitem">
                                    <p>With a Java Keystore File – <code class="code">jdbc:hive2://<code class="replaceable">emr-cluster-master-public-dns</code>:10000/;AuthMech=3;UID=<code class="replaceable">user-name</code>;PWD=<code class="replaceable">password</code>;SSL=1;SSLKeyStore=/home/sagemaker-user/data/<code class="replaceable">Java-keystore-file-name</code>;SSLKeyStorePwd=<code class="replaceable">Java-keystore-file-passsword</code>;</code></p>
                                </li></ul></div>
                        </li><li class="listitem">
                            <p>Presto, no authentication – jdbc:presto://<code class="replaceable">emr-cluster-master-public-dns</code>:8889/;</p>
                        </li><li class="listitem">
                            <p>For Presto with LDAP authentication and SSL enabled, the JDBC URL format depends on whether you use a Java Keystore File for the TLS configuration. The Java Keystore File helps verify the identity of the master node of the Amazon EMR cluster. To use a Java Keystore File, generate it on an EMR cluster and upload it to Data Wrangler. To upload a file, choose the upward arrow on the left-hand navigation of the Data Wrangler UI.
                                For information about creating a Java Keystore File for Presto, see <a href="https://prestodb.io/docs/current/security/tls.html#server-java-keystore" rel="noopener noreferrer" target="_blank"><span>Java Keystore File for TLS</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. For information about running commands on an Amazon EMR cluster, see <a href="http://aws.amazon.com/blogs/big-data/securing-access-to-emr-clusters-using-aws-systems-manager/" rel="noopener noreferrer" target="_blank"><span>Securing access to EMR clusters using AWS Systems Manager</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                            <div class="itemizedlist">
                                 
                                 
                            <ul class="itemizedlist"><li class="listitem">
                                    <p>Without a Java Keystore File – <code class="code">jdbc:presto://<code class="replaceable">emr-cluster-master-public-dns</code>:8889/;SSL=1;AuthenticationType=LDAP Authentication;UID=<code class="replaceable">user-name</code>;PWD=<code class="replaceable">password</code>;AllowSelfSignedServerCert=1;AllowHostNameCNMismatch=1;</code></p> 
                                </li><li class="listitem">
                                    <p>With a Java Keystore File – <code class="code">jdbc:presto://<code class="replaceable">emr-cluster-master-public-dns</code>:8889/;SSL=1;AuthenticationType=LDAP Authentication;SSLTrustStorePath=/home/sagemaker-user/data/<code class="replaceable">Java-keystore-file-name</code>;SSLTrustStorePwd=<code class="replaceable">Java-keystore-file-passsword</code>;UID=<code class="replaceable">user-name</code>;PWD=<code class="replaceable">password</code>;</code></p>
                                </li></ul></div>
                        </li></ul></div>
                </li></ol></div>
         
    <p>Throughout the process of importing data from an Amazon EMR cluster, you might run into issues. For information about troubleshooting them, see <a href="data-wrangler-trouble-shooting.html#data-wrangler-trouble-shooting-emr">Troubleshooting issues with Amazon EMR</a>.</p>
        <h2 id="data-wrangler-databricks">Import data from Databricks (JDBC)</h2>
        <p>You can use Databricks as a data source for your Amazon SageMaker Data Wrangler flow. To import a dataset
            from Databricks, use the JDBC (Java Database Connectivity) import functionality to
            access to your Databricks database. After you access the database, specify a SQL query
            to get the data and import it.</p>
        <p>We assume that you have a running Databricks cluster and that you've configured your
            JDBC driver to it. For more information, see the following Databricks documentation
            pages:</p>
        <div class="itemizedlist">
             
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p><a href="https://docs.databricks.com/integrations/bi/jdbc-odbc-bi.html#jdbc-driver" rel="noopener noreferrer" target="_blank"><span>JDBC driver</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
            </li><li class="listitem">
                <p><a href="https://docs.databricks.com/integrations/bi/jdbc-odbc-bi.html#jdbc-configuration-and-connection-parameters" rel="noopener noreferrer" target="_blank"><span>JDBC configuration and connection parameters</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
            </li><li class="listitem">
                <p><a href="https://docs.databricks.com/integrations/bi/jdbc-odbc-bi.html#authentication-parameters" rel="noopener noreferrer" target="_blank"><span>Authentication parameters</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
            </li></ul></div>
        <p>Data Wrangler stores your JDBC URL in AWS Secrets Manager. You must give your
            Amazon SageMaker Studio IAM execution role permissions to use Secrets Manager. Use the following
            procedure to give permissions.</p>
        <div class="procedure"><p>To give permissions to Secrets Manager, do the following.</p><ol><li> <p>Sign in to the AWS Management Console and open the IAM console at <a href="https://console.aws.amazon.com/iam/" rel="noopener noreferrer" target="_blank"><span>https://console.aws.amazon.com/iam/</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p> </li><li>
                <p>Choose <b>Roles</b>.</p>
            </li><li>
                <p>In the search bar, specify the Amazon SageMaker execution role that Amazon SageMaker Studio is
                    using.</p>
            </li><li>
                <p>Choose the role.</p>
            </li><li>
                <p>Choose <b>Add permissions</b>.</p>
            </li><li>
                <p>Choose <b>Create inline policy</b>.</p>
            </li><li>
                <p>For <b>Service</b>, specify <b>Secrets Manager</b>
                    and choose it.</p>
            </li><li>
                <p>For <b>Actions</b>, select the arrow icon next to
                        <b>Permissions management</b>.</p>
            </li><li>
                <p>Choose <b>PutResourcePolicy</b>.</p>
            </li><li>
                <p>For <b>Resources</b>, choose
                    <b>Specific</b>.</p>
            </li><li>
                <p>Choose the checkbox next to <b>Any in this account</b>.</p>
            </li><li>
                <p>Choose <b>Review policy</b>.</p>
            </li><li>
                <p>For <b>Name</b>, specify a name.</p>
            </li><li>
                <p>Choose <b>Create policy</b>.</p>
            </li></ol></div>
        <p>You can use partitions to import your data more quickly. Partitions give Data Wrangler the
            ability to process the data in parallel. By default, Data Wrangler uses 2 partitions. For most
            use cases, 2 partitions give you near-optimal data processing speeds.</p>
        <p>If you choose to specify more than 2 partitions, you can also specify a column to
            partition the data. The type of the values in the column must be numeric or date.</p>
        <p>We recommend using partitions only if you understand the structure of the data and how
            it's processed.</p>
        <p>You can either import the entire dataset or sample a portion of it. For a Databricks
            database, it provides the following sampling options:</p>
        <div class="itemizedlist">
             
             
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p>None – Import the entire dataset.</p>
            </li><li class="listitem">
                <p>First K – Sample the first K rows of the dataset, where K is an integer
                    that you specify.</p>
            </li><li class="listitem">
                <p>Randomized – Takes a random sample of a size that you specify.</p>
            </li><li class="listitem">
                <p>Stratified – Takes a stratified random sample. A stratified sample
                    preserves the ratio of values in a column.</p>
            </li></ul></div>

        <p>Use the following procedure to import your data from a Databricks database.</p>
        <div class="procedure"><p>To import data from Databricks, do the following.</p><ol><li>
                <p>Sign into <a href="https://console.aws.amazon.com/sagemaker" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker
                    Console</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
            </li><li>
                <p>Choose <b>Studio</b>.</p>
            </li><li>
                <p>Choose <b>Launch app</b>.</p>
            </li><li>
                <p>From the dropdown list, select <b>Studio</b>.</p>
            </li><li>
                <p>From the <b>Import data</b> tab of your Data Wrangler flow, choose
                        <b>Databricks</b>.</p>
            </li><li>
                <p>Specify the following fields:</p>
                <div class="itemizedlist">
                     
                     
                     
                <ul class="itemizedlist"><li class="listitem">
                        <p><b>Dataset name</b> – A name that you want to use
                            for the dataset in your Data Wrangler flow.</p>
                    </li><li class="listitem">
                        <p><b>Driver</b> –
                                <b>com.simba.spark.jdbc.Driver</b>.</p>
                    </li><li class="listitem">
                        <p><b>JDBC URL</b> – The URL of the Databricks
                            database. The URL formatting can vary between Databricks instances. For
                            information about finding the URL and the specifying the parameters
                            within it, see <a href="https://docs.databricks.com/integrations/bi/jdbc-odbc-bi.html#jdbc-configuration-and-connection-parameters" rel="noopener noreferrer" target="_blank"><span>JDBC configuration and connection parameters</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. The following
                            is an example of how a URL can be formatted:
                                jdbc:spark://aws-sagemaker-datawrangler.cloud.databricks.com:443/default;transportMode=http;ssl=1;httpPath=sql/protocolv1/o/3122619508517275/0909-200301-cut318;AuthMech=3;UID=<code class="replaceable">token</code>;PWD=<code class="replaceable">personal-access-token</code>.</p>
                        <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>You can specify a secret ARN that contains the JDBC URL instead of specifying the JDBC URL itself. The secret must contain a key-value pair with the following format: <code class="code">jdbcURL:<code class="replaceable">JDBC-URL</code></code>.
                                For more information, see <a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html">What is Secrets Manager?</a>.</p></div></div>
                    </li></ul></div>
            </li><li>
                <p>Specify a SQL SELECT statement.</p>
                <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Data Wrangler doesn't support Common Table Expressions (CTE) or temporary tables within a query.</p></div></div>
            </li><li>
                <p>For <b>Sampling</b>, choose a sampling method.</p>
            </li><li>
                <p>Choose <b>Run</b>. </p>
            </li><li>
                <p>(Optional) For the <b>PREVIEW</b>, choose the gear to open the
                        <b>Partition settings</b>. </p>
                <div class="mediaobject">
                     
                </div>
                <ol><li>
                        <p>Specify the number of partitions. You can partition by column if you
                            specify the number of partitions:</p>
                        <div class="itemizedlist">
                             
                             
                        <ul class="itemizedlist"><li class="listitem">
                                <p><b>Enter number of partitions</b> –
                                    Specify a value greater than 2.</p>
                            </li><li class="listitem">
                                <p>(Optional) <b>Partition by column</b> –
                                    Specify the following fields. You can only partition by a column
                                    if you've specified a value for <b>Enter number of
                                        partitions</b>.</p>
                                <div class="itemizedlist">
                                     
                                     
                                     
                                <ul class="itemizedlist"><li class="listitem">
                                        <p><b>Select column</b> – Select the
                                            column that you're using for the data partition. The
                                            data type of the column must be numeric or date.</p>
                                    </li><li class="listitem">
                                        <p><b>Upper bound</b> – From the
                                            values in the column that you've specified, the upper
                                            bound is the value that you're using in the partition.
                                            The value that you specify doesn't change the data that
                                            you're importing. It only affects the speed of the
                                            import. For the best performance, specify an upper bound
                                            that's close to the column's maximum.</p>
                                    </li><li class="listitem">
                                        <p><b>Lower bound</b> – From the
                                            values in the column that you've specified, the lower
                                            bound is the value that you're using in the partition.
                                            The value that you specify doesn't change the data that
                                            you're importing. It only affects the speed of the
                                            import. For the best performance, specify a lower bound
                                            that's close to the column's minimum.</p>
                                    </li></ul></div>
                            </li></ul></div>
                    </li></ol>
            </li><li>
                <p>Choose <b>Import</b>.</p>
            </li></ol></div>
     
        <h2 id="data-wrangler-import-salesforce-data-cloud">Import data from Salesforce Data Cloud</h2>
        <p>You can use Salesforce Data Cloud as a data source in SageMaker Data Wrangler to prepare Salesforce Data Cloud
            machine learning.</p>
        <p>With Salesforce Data Cloud as a data source in Data Wrangler, you can quickly connect to your Salesforce data without
            writing a single line of code. You can join your Salesforce data with
            data from any other data source in Data Wrangler.</p>
        <p>After you connect to the data cloud, you can do the following:</p>
        <div class="itemizedlist">
            
             
             
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p>Visualize your data with built-in visualizations</p>
            </li><li class="listitem">
                <p>Understand data and identify
                    potential errors and extreme values</p>
            </li><li class="listitem">
                <p>Transform data with more than 300 built-in transformations</p>
            </li><li class="listitem">
                <p>Export the data that you've transformed</p>
            </li></ul></div>
       
        <div class="highlights" id="inline-topiclist"><h6>Topics</h6><ul><li><a href="data-wrangler-import.html#data-wrangler-import-salesforce-data-cloud-administrator">Administrator setup</a></li><li><a href="data-wrangler-import.html#data-wrangler-salesforce-data-cloud-ds">Data Scientist Guide</a></li></ul></div>
         
            <h3 id="data-wrangler-import-salesforce-data-cloud-administrator">Administrator setup</h3>
            <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>Before you get started, make sure that your users are running Amazon SageMaker Studio version 1.3.0 or later. For information about checking the version of Studio and updating it, see <a href="data-wrangler.html">Prepare ML Data with Amazon SageMaker Data Wrangler</a>.</p></div></div>
            <p>When you're setting up access to Salesforce Data Cloud, you must complete the
                following tasks:</p>
            <div class="itemizedlist">
                 
                 
                 
                 
                 
                 
            <ul class="itemizedlist"><li class="listitem">
                    <p>Getting your Salesforce Domain URL. Salesforce also refers to the Domain
                        URL as your org's URL.</p>
                </li><li class="listitem">
                    <p>Getting OAuth credentials from Salesforce. </p>
                </li><li class="listitem">
                    <p>Getting the authorization URL and token URL for your Salesforce Domain.</p>
                </li><li class="listitem">
                    <p>Creating a AWS Secrets Manager secret with the OAuth configuration.</p>
                </li><li class="listitem">
                    <p>Creating a lifecycle configuration that Data Wrangler uses to read the credentials
                        from the secret.</p>
                </li><li class="listitem">
                    <p>Giving Data Wrangler permissions to read the secret.</p>
                </li></ul></div>
            <p>After you perform the preceding tasks, your users can log into the Salesforce Data
                Cloud using OAuth.</p>
            <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Your users might run into issues after you've set everything up. For information about troubleshooting, see <a href="data-wrangler-trouble-shooting.html#data-wrangler-troubleshooting-salesforce-data-cloud">Troubleshooting with Salesforce</a>.</p></div></div>
            <p>Use the following procedure to get the Domain URL.</p>
            <div class="procedure"><ol><li>
                    <p>Navigate to the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/login.salesforce.com" rel="noopener noreferrer" target="_blank">Salesforce</a> login
                        page.</p>
                </li><li>
                    <p>For <b>Quick find</b>, specify <b>My
                            Domain</b>.</p>
                </li><li>
                    <p>Copy the value of <b>Current My Domain URL</b> to a text
                        file.</p>
                </li><li>
                    <p>Add <code class="code">https://</code> to the beginning of the URL. </p>
                </li></ol></div>
            <p>After you get the Salesforce Domain URL, you can use the following procedure to
                get the login credentials from Salesforce and allow Data Wrangler to access your Salesforce
                data.</p>
            <div class="procedure"><p>To get the log in credentials from Salesforce and provide access to Data Wrangler, do
                    the following.</p><ol><li>
                    <p>Navigate to your Salesforce Domain URL and log into your account.</p>
                </li><li>
                    <p>Choose the gear icon.</p>
                </li><li>
                    <p>In the search bar that appears, specify <b>App
                        Manager</b>.</p>
                </li><li>
                    <p>Select <b>New Connected App</b>.</p>
                </li><li>
                    <p>Specify the following fields:</p>
                    <div class="itemizedlist">
                         
                         
                         
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p>Connected App Name – You can specify any name, but we
                                recommend choosing a name that includes Data Wrangler. For example, you can
                                specify <b>Salesforce Data Cloud Data Wrangler
                                    Integration</b>.</p>
                        </li><li class="listitem">
                            <p>API name – Use the default value.</p>
                        </li><li class="listitem">
                            <p>Contact Email – Specify your email address.</p>
                        </li><li class="listitem">
                            <p>Under <b>API heading (Enable OAuth Settings)</b>,
                                select the checkbox to activate OAuth settings.</p>
                        </li><li class="listitem">
                            <p>For <b>Callback URL</b> specify the Amazon SageMaker Studio
                                URL. To get the URL for Studio, access it from the AWS Management Console and
                                copy the URL.</p>
                        </li></ul></div>
                </li><li>
                    <p>Under <b>Selected OAuth Scopes</b>, move the following from
                        the <b>Available OAuth Scopes</b> to <b>Selected OAuth
                            Scopes</b>:</p>
                    <div class="itemizedlist">
                         
                         
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p>Manage user data via APIs (<code class="code">api</code>)</p>
                        </li><li class="listitem">
                            <p>Perform requests at any time (<code class="code">refresh_token</code>,
                                    <code class="code">offline_access</code>)</p>
                        </li><li class="listitem">
                            <p>Perform ANSI SQL queries on Salesforce Data Cloud data
                                    (<code class="code">cdp_query_api</code>)</p>
                        </li><li class="listitem">
                            <p>Manage Salesforce Customer Data Platform profile data
                                    (<code class="code">cdp_profile_api</code>)</p>
                        </li></ul></div>
                </li><li>
                    <p>Choose <b>Save</b>. After you save your changes, Salesforce
                        opens a new page.</p>
                </li><li>
                    <p>Choose <b>Continue</b></p>
                </li><li>
                    <p>Navigate to <b>Consumer Key and Secret</b>.</p>
                </li><li>
                    <p>Choose <b>Manage Consumer Details</b>. Salesforce redirects
                        you to a new page where you might have to pass two-factor
                        authentication.</p>
                </li><li>
                    <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>Copy the Consumer Key and Consumer Secret to a text editor. You need
                            this information to connect the data cloud to Data Wrangler.</p></div></div>
                </li><li>
                    <p>Navigate back to <b>Manage Connected Apps</b>.</p>
                </li><li>
                    <p>Navigate to <b>Connected App Name </b>and the name of your
                        application.</p>
                </li><li>
                    <p>Choose <b>Manage</b>.</p>
                    <ol><li>
                            <p>Select <b>Edit Policies</b>.</p>
                        </li><li>
                            <p>Change <b>IP Relaxation</b> to <b>Relax IP
                                    restrictions</b>.</p>
                        </li><li>
                            <p>Choose <b>Save</b>.</p>
                        </li></ol>
                </li></ol></div>
            <p>After you provide access to your Salesforce Data Cloud, you need to provide
                permissions for your users. Use the following procedure to provide them with
                permissions.</p>
            <div class="procedure"><p>To provide your users with permissions, do the following.</p><ol><li>
                    <p>Navigate to the setup home page.</p>
                </li><li>
                    <p>On the left-hand navigation, search for <b>Users</b> and
                        choose the <b>Users</b> menu item.</p>
                </li><li>
                    <p>Choose the hyperlink with your user name.</p>
                </li><li>
                    <p>Navigate to <b>Permission Set Assignments</b>.</p>
                </li><li>
                    <p>Choose <b>Edit Assignments</b>.</p>
                </li><li>
                    <p>Add the following permissions:</p>
                    <div class="itemizedlist">
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p><b>Customer Data Platform Admin</b></p>
                        </li><li class="listitem">
                            <p><b>Customer Data Platform Data Aware
                                Specialist</b></p>
                        </li></ul></div>
                </li><li>
                    <p>Choose <b>Save</b>.</p>
                </li></ol></div>
            <p>After you get the information for your Salesforce Domain, you must get the authorization URL and the token URL for the AWS Secrets Manager secret that you're creating.</p>
            <p>Use the following procedure to get the authorization URL and the token URL.</p>
            <div class="procedure"><h6>To get the authorization URL and token URL</h6><ol><li>
                    <p>Navigate to your Salesforce Domain URL.</p>
                </li><li>
                    <p>Use one of the following methods to get the URLs. If you are on a Linux
                        distribution with <code class="code">curl</code> and <code class="code">jq</code> installed, we
                        recommend using the method that only works on Linux.</p>
                    <ul>
                        <li>
                            <p>(Linux only) Specify the following command in your terminal.</p>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="nohighlight">
curl <code class="replaceable">salesforce-domain-URL</code>/.well-known/openid-configuration | \
jq '. | <span>{</span> authorization_url: .authorization_endpoint, token_url: .token_endpoint }' | \
jq '.  += <span>{</span> identity_provider: "SALESFORCE", client_id: "<code class="replaceable">example-client-id</code>", client_secret: "<code class="replaceable">example-client-secret</code>" }'                             
                            </code></pre>
                        </li>
                        <li>
                            <ol><li>
                                    <p>Navigate to <code class="replaceable"><code class="replaceable">example-org-URL</code>/.well-known/openid-configuration</code> in your browser.</p>
                                </li><li>
                                    <p>Copy the <code class="code">authorization_endpoint</code> and <code class="code">token_endpoint</code> to a text editor.</p>
                                </li><li>
                                    <p>Create the following JSON object:</p>                                    
                                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="json "><span>{</span>
  "identity_provider": "SALESFORCE",
  "authorization_url": "<code class="replaceable">example-authorization-endpoint</code>", 
  "token_url": "<code class="replaceable">example-token-endpoint</code>",
  "client_id": "<code class="replaceable">example-consumer-key</code>",
  "client_secret": "<code class="replaceable">example-consumer-secret</code>"
}
                                    </code></pre>
                                </li></ol>
                        </li>
                    </ul>
                </li></ol></div>
            <p>After you create the OAuth configuration object, you can
                create a AWS Secrets Manager secret that stores it. Use the following procedure to
                create the secret.</p>
            <div class="procedure"><p>To create a secret, do the following.</p><ol><li>
                    <p>Navigate to the <a href="https://console.aws.amazon.com/secretsmanager/" rel="noopener noreferrer" target="_blank"><span>AWS Secrets Manager
                        console</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                </li><li>
                    <p>Choose <b>Store a secret</b>.</p>
                </li><li>
                    <p>Select <b>Other type of secret</b>.</p>
                </li><li>
                    <p>Under <b>Key/value</b> pairs select
                            <b>Plaintext</b>.</p>
                </li><li>
                    <p>Replace the empty JSON with the following configuration settings.</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="json "><span>{</span>
  "identity_provider": "SALESFORCE",
  "authorization_url": "<code class="replaceable">example-authorization-endpoint</code>", 
  "token_url": "<code class="replaceable">example-token-endpoint</code>",
  "client_id": "<code class="replaceable">example-consumer-key</code>",
  "client_secret": "<code class="replaceable">example-consumer-secret</code>"
}                    
                    </code></pre>
                </li><li>
                    <p>Choose <b>Next</b>.</p>
                </li><li>
                    <p>For <b>Secret Name</b>, specify the name of the
                        secret.</p>
                </li><li>
                    <p>Under <b>Tags</b>, choose <b>Add</b>.</p>
                    <ol><li>
                            <p>For the <b>Key</b>, specify
                                    <b>sagemaker:partner</b>. For
                                    <b>Value</b>, we recommend specifying a value that
                                might be useful for your use case. However, you can specify
                                anything.</p>
                        </li></ol>
                    <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>You must create the key. You can't import your data from Salesforce if
                            you don't create it.</p></div></div>
                </li><li>
                    <p>Choose <b>Next</b>.</p>
                </li><li>
                    <p>Choose <b>Store</b>.</p>
                </li><li>
                    <p>Choose the secret you've created.</p>
                </li><li>
                    <p>Make a note of the following fields:</p>
                    <div class="itemizedlist">
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p>The Amazon Resource Number (ARN) of the secret</p>
                        </li><li class="listitem">
                            <p>The name of the secret</p>
                        </li></ul></div>
                </li></ol></div>
            <p>After you've created the secret, you must add permissions for Data Wrangler to read the
                secret. Use the following procedure to add permissions.</p>
            <div class="procedure"><p>To add read permissions for Data Wrangler, do the following.</p><ol><li>
                    <p>Navigate to the <a href="https://console.aws.amazon.com/sagemaker/" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker console</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                </li><li>
                    <p>Choose <b>Domains</b>.</p>
                </li><li>
                    <p>Choose the domain that you're using to access Data Wrangler.</p>
                </li><li>
                    <p>Choose your <b>User Profile</b>.</p>
                </li><li>
                    <p>Under <b>Details</b>, find the <b>Execution
                            role</b>. Its ARN is in the following format:
                                <code class="code">arn:aws:iam::111122223333:role/<code class="replaceable">example-role</code></code>.
                        Make a note of the SageMaker execution role. Within the ARN, it's everything
                        after <code class="code">role/</code>.</p>
                </li><li>
                    <p>Navigate to the <a href="https://console.aws.amazon.com/iam" rel="noopener noreferrer" target="_blank"><span>IAM console</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                </li><li>
                    <p>In the <b>Search IAM</b> search bar, specify the name of
                        the SageMaker execution role.</p>
                </li><li>
                    <p>Choose the role.</p>
                </li><li>
                    <p>Choose <b>Add permissions</b>.</p>
                </li><li>
                    <p>Choose <b>Create inline policy</b>.</p>
                </li><li>
                    <p>Choose the JSON tab.</p>
                </li><li>
                    <p>Specify the following policy within the editor.</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="json ">
<span>{</span>
 "Version": "2012-10-17",
 "Statement": [
     <span>{</span>
         "Effect": "Allow",
         "Action": [
             "secretsmanager:GetSecretValue",
             "secretsmanager:PutSecretValue"
         ],
         "Resource": "arn:aws:secretsmanager:*:*:secret:*",
         "Condition": <span>{</span>
             "ForAnyValue:StringLike": <span>{</span>
                 "aws:ResourceTag/sagemaker:partner": "*"
             }
         }
     },
     <span>{</span>
         "Effect": "Allow",
         "Action": [
             "secretsmanager:UpdateSecret"
         ],
         "Resource": "arn:aws:secretsmanager:*:*:secret:AmazonSageMaker-*"
     }
 ]
}                        
                    </code></pre>
                </li><li>
                    <p>Choose <b>Review Policy</b>.</p>
                </li><li>
                    <p>For <b>Name</b>, specify a name.</p>
                </li><li>
                    <p>Choose <b>Create policy</b>.</p>
                </li></ol></div>
            <p>After you've given Data Wrangler permissions to read the secret, you must add a Lifecycle
                Configuration that uses your Secrets Manager secret to your Amazon SageMaker Studio user
                profile.</p>
            <p>Use the following procedure to create a lifecycle configuration and add it to the
                Studio profile.</p>
            <div class="procedure"><p>To create a lifecycle configuration and add it to the Studio profile, do
                    the following.</p><ol><li>
                    <p>Navigate to the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/console.aws.amazon.com/sagemaker" rel="noopener noreferrer" target="_blank">Amazon SageMaker console</a>.</p>
                </li><li>
                    <p>Choose <b>Domains</b>.</p>
                </li><li>
                    <p>Choose the domain that you're using to access Data Wrangler.</p>
                </li><li>
                    <p>Choose your <b>User Profile</b>.</p>
                </li><li>
                    <p>If you see the following applications, delete them:</p>
                    <div class="itemizedlist">
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p>KernelGateway</p>
                        </li><li class="listitem">
                            <p>JupyterKernel</p>
                        </li></ul></div>
                    <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Deleting the applications updates Studio. It can take a while for
                            the updates to happen.</p></div></div>
                </li><li>
                    <p>While you're waiting for updates to happen, choose <b>Lifecycle
                            configurations</b>.</p>
                </li><li>
                    <p>Make sure the page you're on says <b>Studio Lifecycle
                            configurations</b>.</p>
                </li><li>
                    <p>Choose <b>Create configuration</b>.</p>
                </li><li>
                    <p>Make sure <b>Jupyter server app</b> has been
                        selected.</p>
                </li><li>
                    <p>Choose <b>Next</b>.</p>
                </li><li>
                    <p>For <b>Name</b>, specify a name for the
                        configuration.</p>
                </li><li>
                    <p>For <b>Scripts</b>, specify the following script:</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="nohighlight">
#!/bin/bash
set -eux

cat &gt; ~/.sfgenie_identity_provider_oauth_config &lt;&lt;EOL
<span>{</span>
    "secret_arn": "<code class="replaceable">secrets-arn-containing-salesforce-credentials</code>"
}
EOL


                </code></pre>
                </li><li>
                    <p>Choose <b>Submit</b>.</p>
                </li><li>
                    <p>On the left hand navigation, choose
                        <b>Domains</b>.</p>
                </li><li>
                    <p>Choose your Domain.</p>
                </li><li>
                    <p>Choose <b>Environment</b>.</p>
                </li><li>
                    <p>Under <b>Lifecycle configurations for personal Studio
                            apps</b>, choose <b>Attach</b>. </p>
                </li><li>
                    <p>Select <b>Existing configuration</b>.</p>
                </li><li>
                    <p>Under <b>Studio Lifecycle configurations</b> select the
                        lifecycle configuration that you've created.</p>
                </li><li>
                    <p>Choose <b>Attach to domain</b>.</p>
                </li><li>
                    <p>Select the checkbox next to the lifecycle configuration that you've
                        attached.</p>
                </li><li>
                    <p>Select <b>Set as default</b>.</p>
                </li></ol></div>
            <p>You might run into issues when you set up your lifecycle configuration. For information about debugging them, see <a href="studio-lcc-debug.html">Debug lifecycle configurations</a>.</p>
         
         
            <h3 id="data-wrangler-salesforce-data-cloud-ds">Data Scientist Guide</h3>
            <p>Use the following to connect Salesforce Data Cloud and access your data in Data Wrangler.</p>
            <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>Your administrator needs to use the information in the preceding sections to set up Salesforce Data Cloud. If you're running into issues, contact them for troubleshooting help.</p></div></div>
            
            <div class="procedure"><p>To open Studio and check its version, see the following procedure.</p><ol><li>
                    <p>Use the steps in <a href="data-wrangler-getting-started.html#data-wrangler-getting-started-prerequisite">Prerequisites</a> to access
                        Data Wrangler through Amazon SageMaker Studio.</p>
                </li><li>
                    <p>Next to the user you want to use to launch Studio, select
                            <b>Launch app</b>.</p>
                </li><li>
                    <p>Choose <b>Studio</b>.</p>
                </li></ol></div>
            

            
            
             
                        <div class="procedure"><h6>To create a dataset in Data Wrangler with data from the Salesforce Data Cloud</h6><ol><li>
                                <p>Sign into <a href="https://console.aws.amazon.com/sagemaker" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker
                                    Console</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                            </li><li>
                                <p>Choose <b>Studio</b>.</p>
                            </li><li>
                                <p>Choose <b>Launch app</b>.</p>
                            </li><li>
                                <p>From the dropdown list, select <b>Studio</b>.</p>
                            </li><li>
                                <p>Choose the Home icon.</p>
                            </li><li>
                                <p>Choose <b>Data</b>.</p>
                            </li><li>
                                <p>Choose <b>Data Wrangler</b>.</p>
                            </li><li>
                                <p>Choose <b>Import data</b>.</p>
                            </li><li>
                                <p>Under <b>Available</b>, choose
                                    <b>Salesforce Data Cloud</b>.</p>
                            </li><li>
                                <p>For <b>Connection name</b>, specify a name for your connection to the Salesforce Data Cloud.</p>
                            </li><li>
                                <p>For <b>Org URL</b>, specify the organization URL in your Salesforce account. You can get the URL from your administrator.s</p>
                            </li><li>
                                <p>Choose <b>Connect</b>.</p>
                            </li><li>
                                <p>Specify your credentials to log into Salesforce.</p>
                            </li></ol></div>
            
            
            
            
           <p>You can begin creating a dataset using data from Salesforce Data Cloud after you've connected to it.</p>
            
                    <p>After you select a table, you can write
                        queries and run them. The output of your query shows under <b>Query
                            results</b>.</p>
                    
                    <p>After you have settled on the output of your query, you can then import the
                        output of your query into a Data Wrangler flow to perform data transformations. </p>
                    
                    <p>After you've created a dataset, navigate to the <b>Data flow</b> screen to
                        start transforming your data.</p>
                    
         
       
        
        
     
        <h2 id="data-wrangler-snowflake">Import data from Snowflake</h2>

        <p>You can use Snowflake as a data source in SageMaker Data Wrangler to prepare data in Snowflake for
            machine learning.</p>
        <p>With Snowflake as a data source in Data Wrangler, you can quickly connect to Snowflake without
            writing a single line of code. You can join your data in Snowflake with
            data from any other data source in Data Wrangler.</p>
        
        <p>Once connected, you can interactively query data stored in Snowflake, transform data
            with more than 300 preconfigured data transformations, understand data and identify
            potential errors and extreme values with a set of robust preconfigured visualization
            templates, quickly identify inconsistencies in your data preparation workflow, and
            diagnose issues before models are deployed into production. Finally, you can export your
            data preparation workflow to Amazon S3 for use with other SageMaker features such as Amazon SageMaker Autopilot,
            Amazon SageMaker Feature Store and Amazon SageMaker Model Building Pipelines.</p>
        <p>You can encrypt the output of your queries using an AWS Key Management Service key that you've created.
            For more information about AWS KMS, see <a href="https://docs.aws.amazon.com/kms/latest/developerguide/overview.html">AWS Key Management Service</a>.</p>
        <div class="highlights" id="inline-topiclist"><h6>Topics</h6><ul><li><a href="data-wrangler-import.html#data-wrangler-snowflake-admin">Administrator Guide</a></li><li><a href="data-wrangler-import.html#data-wrangler-snowflake-ds">Data Scientist Guide</a></li></ul></div>
  
        

         
            <h3 id="data-wrangler-snowflake-admin">Administrator Guide</h3>
            <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>To learn more about granular access control and best practices, see <a href="https://docs.snowflake.com/en/user-guide/security-access-control.html" rel="noopener noreferrer" target="_blank"><span>Security Access Control</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. </p></div></div>
            <p>This section is for Snowflake administrators who are setting up access to
                Snowflake from within SageMaker Data Wrangler.</p>
            <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>You are responsible for managing and monitoring the access
                    control within Snowflake. Data Wrangler does
                    not add a layer of access control with respect to Snowflake. </p><p>Access control includes the following:</p><div class="itemizedlist">
                     
                     
                     
                <ul class="itemizedlist"><li class="listitem">
                        <p>The data that a user accesses</p>
                    </li><li class="listitem">
                        <p>(Optional) The storage integration that provides Snowflake the ability to write query results to an Amazon S3 bucket</p>
                    </li><li class="listitem">
                        <p>The queries that a user can run</p>
                    </li></ul></div></div></div>
           

             
                <h4 id="data-wrangler-snowflake-admin-config">(Optional) Configure Snowflake Data Import Permissions</h4>
                <p>By default, Data Wrangler queries the data in Snowflake without creating a copy of it in an Amazon S3 location.
                    Use the following information if you're configuring a storage integration with Snowflake. Your users can use a storage integration to store their query results in an Amazon S3 location.</p>
                <p>Your users might have different levels of access of sensitive data. For optimal data security, provide each user with their own storage integration. 
                    Each storage integration should have its own data governance policy.</p>
                <p>This feature is currently not available in the opt-in Regions.</p>

                        <p>Snowflake requires the following permissions on an S3 bucket and
                            directory to be able to access files in the directory:</p>
                        <div class="itemizedlist">
                             
                             
                             
                             
                             
                        <ul class="itemizedlist"><li class="listitem">
                                <p><code class="code">s3:GetObject</code></p>
                            </li><li class="listitem">
                                <p><code class="code">s3:GetObjectVersion</code></p>
                            </li><li class="listitem">
                                <p><code class="code">s3:ListBucket</code></p>
                            </li><li class="listitem">
                                <p><code class="code">s3:ListObjects</code></p>
                            </li><li class="listitem">
                                <p><code class="code">s3:GetBucketLocation</code></p>
                            </li></ul></div>
                        <p><b>Create an IAM policy</b></p>
                        <p>You must create an IAM policy to configure access permissions for
                            Snowflake to load and unload data from an Amazon S3 bucket.</p>
                        <p>The following is the JSON policy document that you use to create the policy:</p>
                        
                       
                                <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="json "># Example policy for S3 write access
# This needs to be updated
<span>{</span>
"Version": "2012-10-17",
"Statement": [
  <span>{</span>
    "Effect": "Allow",
    "Action": [
        "s3:PutObject",
        "s3:GetObject",
        "s3:GetObjectVersion",
        "s3:DeleteObject",
        "s3:DeleteObjectVersion"
    ],
    "Resource": "arn:aws:s3:::<code class="replaceable">bucket</code>/<code class="replaceable">prefix</code>/*"
  },
  <span>{</span>
    "Effect": "Allow",
    "Action": [
        "s3:ListBucket"
    ],
    "Resource": "arn:aws:s3:::<code class="replaceable">bucket/</code>",
    "Condition": <span>{</span>
        "StringLike": <span>{</span>
            "s3:prefix": ["<code class="replaceable">prefix</code>/*"]
        }
    }
  }
 ]
}</code></pre>
                        <p>For information and procedures about creating policies with policy documents, see <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_create.html">Creating IAM policies</a>.</p>
                           
                    <p>For documentation that provides an overview of using IAM permissions with Snowflake, see the following resources:</p>
                        <div class="itemizedlist">
                             
                             
                             
                             
                             
                        <ul class="itemizedlist"><li class="listitem">
                                <p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html">What is IAM?</a></p>
                            </li><li class="listitem">
                                <p><a href="https://docs.snowflake.com/en/user-guide/data-load-s3-config-storage-integration.html#step-2-create-the-iam-role-in-aws" rel="noopener noreferrer" target="_blank"><span>Create the IAM Role in AWS</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                            </li><li class="listitem">
                                <p><a href="https://docs.snowflake.com/en/user-guide/data-load-s3-config-storage-integration.html#step-3-create-a-cloud-storage-integration-in-snowflake" rel="noopener noreferrer" target="_blank"><span>Create a Cloud Storage Integration in Snowflake</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                            </li><li class="listitem">
                                <p><a href="https://docs.snowflake.com/en/user-guide/data-load-s3-config-storage-integration.html#step-4-retrieve-the-aws-iam-user-for-your-snowflake-account" rel="noopener noreferrer" target="_blank"><span>Retrieve the AWS IAM User for your Snowflake
                                    Account</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                            </li><li class="listitem">
                                <p><a href="https://docs.snowflake.com/en/user-guide/data-load-s3-config-storage-integration.html#step-5-grant-the-iam-user-permissions-to-access-bucket-objects" rel="noopener noreferrer" target="_blank"><span>Grant the IAM User Permissions to Access Bucket</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                            </li></ul></div>
                        <p>To grant the data scientist's Snowflake role usage permission to the
                            storage integration, you must run <code class="code">GRANT USAGE ON INTEGRATION
                                integration_name TO snowflake_role;</code>.</p>
                        <div class="itemizedlist">
                             
                             
                        <ul class="itemizedlist"><li class="listitem">
                                <p><code class="code">integration_name</code> is the name of your
                                    storage integration.</p>
                            </li><li class="listitem">
                                <p><code class="code">snowflake_role</code> is the name of the default
                                    <a href="https://docs.snowflake.com/en/user-guide/security-access-control-overview.html#roles" rel="noopener noreferrer" target="_blank"><span>Snowflake role</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> given to the data scientist
                                    user.</p>
                            </li></ul></div>
             
                   
                                    
                  
                  
                    

                

       
             
                <h4 id="data-wrangler-snowflake-oauth-setup">Setting up Snowflake OAuth Access</h4>
                <p>Instead of having your users directly enter their credentials into Data Wrangler, you can have them use an identity provider to access Snowflake. The following are links to the Snowflake documentation for the identity providers that Data Wrangler supports.</p>
                <div class="itemizedlist">
                     
                     
                     
                <ul class="itemizedlist"><li class="listitem">
                        <p><a href="https://docs.snowflake.com/en/user-guide/oauth-azure.html" rel="noopener noreferrer" target="_blank"><span>Azure AD</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                    </li><li class="listitem">
                        <p><a href="https://docs.snowflake.com/en/user-guide/oauth-okta.html" rel="noopener noreferrer" target="_blank"><span>Okta</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                    </li><li class="listitem">
                        <p><a href="https://docs.snowflake.com/en/user-guide/oauth-pingfed.html" rel="noopener noreferrer" target="_blank"><span>Ping Federate</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                    </li></ul></div>
                <p>Use the documentation from the preceding links to set up access to your identity provider. The information and procedures in this section help you understand how to properly use the documentation to access Snowflake within Data Wrangler.</p>
                <p>Your identity provider needs to recognize Data Wrangler as an application. Use the following procedure to register Data Wrangler as an application within the identity provider:</p>
                <div class="procedure"><ol><li>
                        <p>Select the configuration that starts the process of registering Data Wrangler as an application.</p>
                    </li><li>
                        <p>Provide the users within the identity provider access to Data Wrangler.</p>
                    </li><li>
                        <p>Turn on OAuth client authentication by storing the client credentials as an AWS Secrets Manager secret.</p>
                    </li><li>
                        <p>Specify a redirect URL using the following format: https://<code class="replaceable">Domain-ID</code>.studio.<code class="replaceable">AWS Region</code>.sagemaker.aws/jupyter/default/lab</p>
                        <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>You're specifying the Amazon SageMaker Domain ID and AWS Region that you're using to run Data Wrangler.</p></div></div>
                        <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>You must register a URL for each Amazon SageMaker Domain and AWS Region where you're running Data Wrangler. Users from a Domain and AWS Region that don't have redirect URLs set up for them won't be able to authenticate with the identity provider to access the Snowflake connection.</p></div></div>                       
                    </li><li>
                        <p>Make sure that the authorization code and refresh token grant types are allowed for the Data Wrangler application.</p>
                    </li></ol></div>
                <p>Within your identity provider, you must set up a server that sends OAuth tokens to Data Wrangler at the user level. The server sends the tokens with Snowflake as the audience.</p>
                <p>Snowflake uses the concept of roles that are distinct role the IAM roles used in AWS. You must configure the identity provider to use any role to use the default role associated with the Snowflake account. For example, if a user has <code class="code">systems administrator</code> as the default role in their Snowflake profile, the connection from Data Wrangler to Snowflake uses <code class="code">systems administrator</code> as the role.</p>
                
                <p>Use the following procedure to set up the server.</p>
                <div class="procedure"><p>To set up the server, do the following. You're working within Snowflake for all steps except the last one.</p><ol><li>
                        <p>Start setting up the server or API.</p>
                    </li><li>
                        <p>Configure the authorization server to use the authorization code and refresh token grant types.</p>
                    </li><li>
                        <p>Specify the lifetime of the access token.</p>
                    </li><li>
                        <p>Set the refresh token idle timeout. The idle timeout is the time that the refresh token expires if it's not used.</p>                     
                        <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>If you're scheduling jobs in Data Wrangler, we recommend making the idle
                                timeout time greater than the frequency of the processing job.
                                Otherwise, some processing jobs might fail because the refresh token
                                expired before they could run. When the refresh token expires, the
                                user must re-authenticate  by accessing the connection that they've
                                made to Snowflake through Data Wrangler.</p></div></div>
                    </li><li>
                        <p>Specify <code class="code">session:role-any</code> as the new scope.</p>
                        <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>For Azure AD, copy the unique identifier for the scope. Data Wrangler requires you to provide it with the identifier.</p></div></div>
                    </li><li>
                        <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>Within the External OAuth Security Integration for Snowflake, enable <code class="code">external_oauth_any_role_mode</code>.</p></div></div>
                    </li></ol></div>                

                <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>Data Wrangler doesn't support rotating refresh tokens. Using rotating refresh
                        tokens might result in access failures or users needing to log in
                        frequently.</p></div></div>
                <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>If the refresh token expires, your users must reauthenticate by accessing
                        the connection that they've made to Snowflake through Data Wrangler.</p></div></div>
                <p>After you've set up the OAuth provider, you provide Data Wrangler with the information it needs to connect to the provider. You can use the documentation from your identity provider to get values for the following fields:</p>
                <div class="itemizedlist">
                     
                     
                     
                     
                     
                <ul class="itemizedlist"><li class="listitem">
                        <p>Token URL – The URL of the token that the identity provider sends to Data Wrangler.</p>
                    </li><li class="listitem">
                        <p>Authorization URL – The URL of the authorization server of the identity provider.</p>
                    </li><li class="listitem">
                        <p>Client ID – The ID of the identity provider.</p>
                    </li><li class="listitem">
                        <p>Client secret – The secret that only the authorization server or API recognizes.</p>
                    </li><li class="listitem">
                        <p>(Azure AD only) The OAuth scope credentials that you've copied.</p>
                    </li></ul></div>
                <p>You store the fields and values in a AWS Secrets Manager secret and add it to the
                    Amazon SageMaker Studio lifecycle configuration that you're using for Data Wrangler. A Lifecycle
                    Configuration is a shell script. Use it to make the Amazon Resource Name (ARN)
                    of the secret accessible to Data Wrangler. For information about creating secrets see
                        <a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/hardcoded.html">Move hardcoded secrets
                        to AWS Secrets Manager</a>. For information about using lifecycle configurations
                    in Studio, see <a href="studio-lcc.html">Use lifecycle configurations with Amazon SageMaker Studio</a>.</p>
                <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>Before you create a Secrets Manager secret, make sure that the SageMaker execution role that you're using for Amazon SageMaker Studio has permissions to create and update secrets in Secrets Manager. For more information about adding permissions, see <a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access_examples.html#auth-and-access_examples_create">Example: Permission to create secrets</a>.</p></div></div>
                <p>For Okta and Ping Federate, the following is the format of the secret:</p>
                <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="json ">
<span>{</span>
    "token_url":"https://<code class="replaceable">identityprovider</code>.com/oauth2/<code class="replaceable">example-portion-of-URL-path</code>/v2/token",
    "client_id":"<code class="replaceable">example-client-id</code>",
    "client_secret":"<code class="replaceable">example-client-secret</code>",
    "identity_provider":"<code class="replaceable">OKTA</code>"|"<code class="replaceable">PING_FEDERATE</code>",
    "authorization_url":"https://<code class="replaceable">identityprovider</code>.com/oauth2/<code class="replaceable">example-portion-of-URL-path</code>/v2/authorize"
}                   
                </code></pre>
                <p>For Azure AD, the following is the format of the secret:</p>
                <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="json ">
<span>{</span>
    "token_url":"https://<code class="replaceable">identityprovider</code>.com/oauth2/<code class="replaceable">example-portion-of-URL-path</code>/v2/token",
    "client_id":"<code class="replaceable">example-client-id</code>",
    "client_secret":"<code class="replaceable">example-client-secret</code>",
    "identity_provider":"AZURE_AD",
    "authorization_url":"https://<code class="replaceable">identityprovider</code>.com/oauth2/<code class="replaceable">example-portion-of-URL-path</code>/v2/authorize",
    "datasource_oauth_scope":"api://appuri/<code class="replaceable">session:role-any</code>)"
}                   
                </code></pre>
                <p>You must have a lifecycle configuration that uses the Secrets Manager secret that you've
                    created. You can either create the lifecycle configuration or modify one that
                    has already been created. The configuration must use the following
                    script.</p>
                <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="nohighlight">
#!/bin/bash

set -eux

## Script Body

cat &gt; ~/.snowflake_identity_provider_oauth_config &lt;&lt;EOL
<span>{</span>
    "secret_arn": "<code class="replaceable">example-secret-arn</code>"
}
EOL                   
                </code></pre>
                
                <p>For information about setting up lifecycle configurations, see <a href="studio-lcc-create.html">Create and associate a lifecycle configuration</a>. When you're
                    going through the process of setting up, do the following:</p>
                <div class="itemizedlist">
                     
                     
                     
                     
                <ul class="itemizedlist"><li class="listitem">
                        <p>Set the application type of the configuration to <code class="code">Jupyter Server</code>.</p>
                    </li><li class="listitem">
                        <p>Attach the configuration to the Amazon SageMaker Domain that has your users.</p>
                    </li><li class="listitem">
                        <p>Have the configuration run by default. It must run every time a user logs into Studio. Otherwise, the credentials saved in the configuration won't be available to your users when they're using Data Wrangler.</p>
                    </li><li class="listitem">
                        <p>The lifecycle configuration creates a file with the name,
                                <code class="code">snowflake_identity_provider_oauth_config</code> in the user's
                            home folder. The file contains the Secrets Manager secret. Make sure that it's in
                            the user's home folder every time the Jupyter Server's instance is
                            initialized.</p>
                    </li></ul></div>
                
             
             
            <h4 id="data-wrangler-security-snowflake-vpc">Private Connectivity between
                    Data Wrangler and Snowflake via AWS PrivateLink</h4>

            <p>This section explains how to use AWS PrivateLink to establish a private connection
                between Data Wrangler and Snowflake. The steps are explained in the following sections. </p>
             
                <h5 id="data-wrangler-snowflake-snowflake-vpc-setup">Create a VPC</h5>
                <p>If you do not have a VPC set up, then follow the <a href="https://docs.aws.amazon.com/directoryservice/latest/admin-guide/gsg_create_vpc.html#create_vpc">Create a new VPC</a> instructions to create one.</p>
                <p>Once you have a chosen VPC you would like to use for establishing a private
                    connection, provide the following credentials to your Snowflake Administrator to
                    enable AWS PrivateLink:</p>
                <div class="itemizedlist">
                     
                     
                     
                <ul class="itemizedlist"><li class="listitem">
                        <p>VPC ID</p>
                    </li><li class="listitem">
                        <p>AWS Account ID</p>
                    </li><li class="listitem">
                        <p>Your corresponding account URL you use to access Snowflake</p>
                    </li></ul></div>
                <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>As described in Snowflake's documentation, enabling your Snowflake account
                        can take up to two business days. </p></div></div>
             
             
                <h5 id="data-wrangler-snowflake-snowflake-vpc-privatelink-setup">Set up Snowflake
                        AWS PrivateLink Integration</h5>
                <p>After AWS PrivateLink is activated, retrieve the AWS PrivateLink configuration for
                    your Region by running the following command in a Snowflake worksheet.
                    Log
                    into your Snowflake console and enter the following under
                        <b>Worksheets</b>: <code class="code">select
                        SYSTEM$GET_PRIVATELINK_CONFIG();</code>
                </p>
                <div class="orderedlist">
                     
                     
                     
                     
                     
                     
                     
                     
                     
                     
                     
                     
                     
                     
                     
                <ol><li>
                        <p>Retrieve the values for the following:
                                <code class="code">privatelink-account-name</code>,
                                <code class="code">privatelink_ocsp-url</code>,
                                <code class="code">privatelink-account-url</code>, and
                                <code class="code">privatelink_ocsp-url</code> from the resulting JSON object.
                            Examples of each value are shown in the following snippet. Store these
                            values for later use.</p>
                        <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="">privatelink-account-name: xxxxxxxx.region.privatelink
privatelink-vpce-id: com.amazonaws.vpce.region.vpce-svc-xxxxxxxxxxxxxxxxx
privatelink-account-url: xxxxxxxx.region.privatelink.snowflakecomputing.com
privatelink_ocsp-url: ocsp.xxxxxxxx.region.privatelink.snowflakecomputing.com       </code></pre>
                    </li><li>
                        <p>Switch to your AWS Console and navigate to the VPC menu.</p>
                    </li><li>
                        <p>From the left side panel, choose the <b>Endpoints</b>
                            link to navigate to the <b>VPC Endpoints</b> setup.</p>
                        <p>Once there, choose <b>Create Endpoint</b>. </p>
                    </li><li>
                        <p>Select the radio button for <b>Find service by name</b>,
                            as shown in the following screenshot. </p>
                        <div class="mediaobject">
                             
                                <img src="../../../images/sagemaker/latest/dg/images/studio/mohave/snowflake-radio.png" class="aws-docs-img-whiteBg aws-docs-img-padding" style="max-width:90%" />
                             
                        </div>
                    </li><li>
                        <p>In the <b>Service Name</b> field, paste in the value for
                                <code class="code">privatelink-vpce-id</code> that you retrieved in the preceding
                            step and choose <b>Verify</b>. </p>
                        <p>If the connection is successful, a green alert saying
                                <b>Service name found</b> appears on your screen and
                            the <b>VPC</b> and <b>Subnet</b> options
                            automatically expand, as shown in the following screenshot. Depending on
                            your targeted Region, your resulting screen may show another AWS
                            Region name. </p>
                        <div class="mediaobject">
                             
                                <img src="../../../images/sagemaker/latest/dg/images/studio/mohave/snowflake-service-name-found.png" class="aws-docs-img-whiteBg aws-docs-img-padding" style="max-width:90%" />
                             
                        </div>
                    </li><li>
                        <p>Select the same VPC ID that you sent to Snowflake from the
                                <b>VPC</b> dropdown list.</p>
                    </li><li>
                        <p>If you have not yet created a subnet, then perform the following set
                            of instructions on creating a subnet. </p>
                    </li><li>
                        <p>Select <b>Subnets</b> from the <b>VPC</b>
                            dropdown list. Then select <b>Create subnet</b> and follow
                            the prompts to create a subset in your VPC. Ensure you select the VPC ID
                            you sent Snowflake. </p>
                    </li><li>
                        <p>Under <b>Security Group Configuration</b>, select
                                <b>Create New Security Group</b> to open the default
                                <b>Security Group</b> screen in a new tab. In this new
                            tab, select t<b>Create Security Group</b>. </p>
                    </li><li>
                        <p>Provide a name for the new security group (such as
                                <code class="code">datawrangler-doc-snowflake-privatelink-connection</code>) and
                            a description. Be sure to select the VPC ID you have used in previous
                            steps. </p>
                    </li><li>
                        <p>Add two rules to allow traffic from within your VPC to this VPC
                            endpoint. </p>
                        <p>Navigate to your VPC under <b>Your VPCs</b> in a
                            separate tab, and retrieve your CIDR block for your VPC. Then choose
                                <b>Add Rule</b> in the <b>Inbound
                                Rules</b> section. Select <code class="code">HTTPS</code> for the type,
                            leave the <b>Source</b> as <b>Custom</b> in
                            the form, and paste in the value retrieved from the preceding
                                <code class="code">describe-vpcs</code> call (such as <code class="code">10.0.0.0/16</code>).
                        </p>
                    </li><li>
                        <p>Choose <b>Create Security Group</b>. Retrieve the
                                <b>Security Group ID</b> from the newly created
                            security group (such as <code class="code">sg-xxxxxxxxxxxxxxxxx</code>).</p>
                    </li><li>
                        <p>In the <b>VPC Endpoint</b> configuration screen, remove
                            the default security group. Paste in the security group ID in the search
                            field and select the checkbox.</p>
                        <div class="mediaobject">
                             
                                <img src="../../../images/sagemaker/latest/dg/images/studio/mohave/snowflake-security-group.png" class="aws-docs-img-whiteBg aws-docs-img-padding" style="max-width:90%" />
                             
                        </div>
                    </li><li>
                        <p>Select <b>Create Endpoint</b>. </p>
                    </li><li>
                        <p>If the endpoint creation is successful, you see a page that has a link
                            to your VPC endpoint configuration, specified by the VPC ID. Select the
                            link to view the configuration in full. </p>
                        <div class="mediaobject">
                             
                                <img src="../../../images/sagemaker/latest/dg/images/studio/mohave/snowflake-success-endpoint.png" class="aws-docs-img-whiteBg aws-docs-img-padding" style="max-width:90%" />
                             
                        </div>
                        <p>Retrieve the topmost record in the DNS names list. This can be
                            differentiated from other DNS names because it only includes the Region
                            name (such as <code class="code">us-west-2</code>), and no Availability Zone letter
                            notation (such as <code class="code">us-west-2a</code>). Store this information for
                            later use.</p>
                    </li></ol></div>
             
             
                <h5 id="data-wrangler-snowflake-vpc-privatelink-dns">Configure DNS for
                        Snowflake Endpoints in your VPC</h5>
                <p>This section explains how to configure DNS for Snowflake endpoints in your
                    VPC. This allows your VPC to resolve requests to the Snowflake AWS PrivateLink
                    endpoint. </p>
                <div class="orderedlist">
                     
                     
                     
                     
                <ol><li>
                        <p>Navigate to the <a href="https://console.aws.amazon.com/route53" rel="noopener noreferrer" target="_blank"><span>Route 53
                                menu</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> within your AWS console.</p>
                    </li><li>
                        <p>Select the <b>Hosted Zones</b> option (if necessary,
                            expand the left-hand menu to find this option).</p>
                    </li><li>
                        <p>Choose <b>Create Hosted Zone</b>.</p>
                        <div class="orderedlist">
                             
                             
                             
                             
                        <ol><li>
                                <p>In the <b>Domain name</b> field, reference the
                                    value that was stored for <code class="code">privatelink-account-url</code>
                                    in the preceding steps. In this field, your Snowflake account ID
                                    is removed from the DNS name and only uses the value starting
                                    with the Region identifier. A <b>Resource Record
                                        Set</b> is also created later for the subdomain, such
                                    as,
                                    <code class="code">region.privatelink.snowflakecomputing.com</code>.</p>
                            </li><li>
                                <p>Select the radio button for <b>Private Hosted
                                        Zone</b> in the <b>Type</b> section.
                                    Your Region code may not be <code class="code">us-west-2</code>. Reference
                                    the DNS name returned to you by Snowflake.</p>
                                <div class="mediaobject">
                                     
                                        <img src="../../../images/sagemaker/latest/dg/images/studio/mohave/snowflake-create-hosted-zone.png" class="aws-docs-img-whiteBg aws-docs-img-padding" style="max-width:90%" />
                                     
                                </div>
                            </li><li>
                                <p>In the <b>VPCs to associate with the hosted
                                        zone</b> section, select the Region in which your VPC
                                    is located and the VPC ID used in previous steps.</p>
                                <div class="mediaobject">
                                     
                                        <img src="../../../images/sagemaker/latest/dg/images/studio/mohave/snowflake-vpc-hosted-zone.png" class="aws-docs-img-whiteBg aws-docs-img-padding" style="max-width:90%" />
                                     
                                </div>
                            </li><li>
                                <p>Choose <b>Create hosted zone</b>.</p>
                            </li></ol></div>
                    </li><li>
                        <p>Next, create two records, one for <code class="code">privatelink-account-url</code>
                            and one for <code class="code">privatelink_ocsp-url</code>.</p>
                        <div class="itemizedlist">
                             
                        <ul class="itemizedlist"><li class="listitem">
                                <p>In the <b>Hosted Zone</b> menu, choose
                                        <b>Create Record Set</b>.</p>
                                <div class="orderedlist">
                                     
                                     
                                     
                                     
                                     
                                <ol><li>
                                        <p>Under <b>Record name</b>, enter your
                                            Snowflake Account ID only (the first 8 characters in
                                                <code class="code">privatelink-account-url</code>).</p>
                                    </li><li>
                                        <p>Under <b>Record type</b>, select
                                                <b>CNAME</b>.</p>
                                    </li><li>
                                        <p>Under <b>Value</b>, enter the DNS name
                                            for the regional VPC endpoint you retrieved in the last
                                            step of the <em>Set up the Snowflake
                                                AWS PrivateLink Integration</em> section. </p>
                                        <div class="mediaobject">
                                             
                                                <img src="../../../images/sagemaker/latest/dg/images/studio/mohave/snowflake-quick-create-record.png" class="aws-docs-img-whiteBg aws-docs-img-padding" style="max-width:90%" />
                                             
                                        </div>
                                    </li><li>
                                        <p>Choose <b>Create records</b>.</p>
                                    </li><li>
                                        <p>Repeat the preceding steps for the OCSP record we
                                            notated as <code class="code">privatelink-ocsp-url</code>, starting
                                            with <code class="code">ocsp</code> through the 8-character Snowflake
                                            ID for the record name (such as
                                                <code class="code">ocsp.xxxxxxxx</code>).</p>
                                        <div class="mediaobject">
                                             
                                                <img src="../../../images/sagemaker/latest/dg/images/studio/mohave/snowflake-quick-create-ocsp.png" class="aws-docs-img-whiteBg aws-docs-img-padding" style="max-width:90%" />
                                             
                                        </div>
                                    </li></ol></div>
                            </li></ul></div>
                    </li></ol></div>
             
             
                <h5 id="data-wrangler-snowflake-vpc-privatelink-route53">Configure
                        Route 53 Resolver Inbound Endpoint for your VPC</h5>
                <p>This section explains how to configure Route 53 resolvers inbound endpoints
                    for your VPC.</p>
                <div class="orderedlist">
                     
                     
                     
                     
                     
                     
                <ol><li>
                        <p>Navigate to the <a href="https://console.aws.amazon.com/route53" rel="noopener noreferrer" target="_blank"><span>Route 53
                                menu</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> within your AWS console.</p>
                        <div class="itemizedlist">
                             
                        <ul class="itemizedlist"><li class="listitem">
                                <p>In the left hand panel in the <b>Security</b>
                                    section, select the <b>Security Groups</b>
                                    option.</p>
                            </li></ul></div>
                    </li><li>
                        <p>Choose <b>Create Security Group</b>. </p>
                        <div class="itemizedlist">
                             
                             
                             
                             
                        <ul class="itemizedlist"><li class="listitem">
                                <p>Provide a name for your security group (such as
                                        <code class="code">datawranger-doc-route53-resolver-sg</code>) and a
                                    description.</p>
                            </li><li class="listitem">
                                <p>Select the VPC ID used in previous steps.</p>
                            </li><li class="listitem">
                                <p>Create rules that allow for DNS over UDP and TCP from within
                                    the VPC CIDR block. </p>
                                <div class="mediaobject">
                                     
                                        <img src="../../../images/sagemaker/latest/dg/images/studio/mohave/snowflake-inbound-rules.png" class="aws-docs-img-whiteBg aws-docs-img-padding" style="max-width:90%" />
                                     
                                </div>
                            </li><li class="listitem">
                                
                                
                                <p>Choose <b>Create Security Group</b>. Note the
                                        <b>Security Group ID</b> because adds a rule
                                    to allow traffic to the VPC endpoint security
                                group.</p>
                            </li></ul></div>
                    </li><li>
                        <p>Navigate to the <a href="https://console.aws.amazon.com/route53" rel="noopener noreferrer" target="_blank"><span>Route 53
                                menu</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> within your AWS console.</p>
                        <div class="itemizedlist">
                             
                        <ul class="itemizedlist"><li class="listitem">
                                <p>In the <b>Resolver</b> section, select the
                                        <b>Inbound Endpoint</b> option.</p>
                            </li></ul></div>
                    </li><li>
                        <p>Choose <b>Create Inbound Endpoint</b>. </p>
                        <div class="itemizedlist">
                             
                             
                             
                             
                             
                        <ul class="itemizedlist"><li class="listitem">
                                <p>Provide an endpoint name.</p>
                            </li><li class="listitem">
                                <p>From the <b>VPC in the Region</b> dropdown list,
                                    select the VPC ID you have used in all previous steps. </p>
                            </li><li class="listitem">
                                <p>In the <b>Security group for this endpoint</b>
                                    dropdown list, select the security group ID from Step 2 in this
                                    section. </p>
                                <div class="mediaobject">
                                     
                                        <img src="../../../images/sagemaker/latest/dg/images/studio/mohave/snowflake-inbound-endpoint.png" class="aws-docs-img-whiteBg aws-docs-img-padding" style="max-width:90%" />
                                     
                                </div>
                            </li><li class="listitem">
                                <p>In the <b>IP Address</b> section, select an
                                    Availability Zones, select a subnet, and leave the radio
                                    selector for <b>Use an IP address that is
                                        selected automatically</b> selected for each IP
                                    address. </p>
                                <div class="mediaobject">
                                     
                                        <img src="../../../images/sagemaker/latest/dg/images/studio/mohave/snowflake-ip-address-1.png" class="aws-docs-img-whiteBg aws-docs-img-padding" style="max-width:90%" />
                                     
                                </div>
                            </li><li class="listitem">
                                <p>Choose <b>Submit</b>.</p>
                            </li></ul></div>
                    </li><li>
                        <p>Select the <b>Inbound endpoint</b> after it has been
                            created.</p>
                    </li><li>
                        <p>Once the inbound endpoint is created, note the two IP addresses for
                            the resolvers.</p>
                        <div class="mediaobject">
                             
                                <img src="../../../images/sagemaker/latest/dg/images/studio/mohave/snowflake-ip-addresses-2.png" class="aws-docs-img-whiteBg aws-docs-img-padding" style="max-width:90%" />
                             
                        </div>
                    </li></ol></div>
             
             
                <h5 id="data-wrangler-snowflake-sagemaker-vpc-endpoints"> SageMaker VPC
                        Endpoints</h5>
                <p> This section explains how to create VPC endpoints for the following:
                    Amazon SageMaker Studio, SageMaker Notebooks, the SageMaker API, SageMaker Runtime, and Amazon SageMaker Feature Store
                    Runtime.</p>
                 
                    <p><b>Create a security group that is applied to all
                            endpoints.</b></p>
                    <div class="orderedlist">
                         
                         
                         
                         
                    <ol><li>
                            <p>Navigate to the <a href="https://console.aws.amazon.com/ec2" rel="noopener noreferrer" target="_blank"><span>EC2 menu</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>
                                in the AWS Console.</p>
                        </li><li>
                            <p>In the <b>Network &amp; Security</b> section, select
                                the <b>Security groups</b> option.</p>
                        </li><li>
                            <p>Choose <b>Create security group</b>.</p>
                        </li><li>
                            <p>Provide a security group name and description (such as
                                    <code class="code">datawrangler-doc-sagemaker-vpce-sg</code>). A rule is
                                added later to allow traffic over HTTPS from SageMaker to this group.
                            </p>
                        </li></ol></div>
                 
                 
                    <p><b>Creating the endpoints</b></p>
                    <div class="orderedlist">
                         
                         
                         
                         
                         
                         
                         
                         
                         
                    <ol><li>
                            <p>Navigate to the <a href="https://console.aws.amazon.com/vpc" rel="noopener noreferrer" target="_blank"><span>VPC menu</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>
                                in the AWS console.</p>
                        </li><li>
                            <p>Select the <b>Endpoints</b> option.</p>
                        </li><li>
                            <p>Choose <b>Create Endpoint</b>.</p>
                        </li><li>
                            <p>Search for the service by entering its name in the
                                    <b>Search</b> field.</p>
                        </li><li>
                            <p>From the <b>VPC</b> dropdown list, select the VPC in
                                which your Snowflake AWS PrivateLink connection exists.</p>
                        </li><li>
                            <p>In the <b>Subnets</b> section, select the subnets
                                which have access to the Snowflake PrivateLink connection.</p>
                        </li><li>
                            <p>Leave the <b>Enable DNS Name</b> checkbox
                                selected.</p>
                        </li><li>
                            <p>In the <b>Security Groups</b> section, select the
                                security group you created in the preceding section.</p>
                        </li><li>
                            <p>Choose <b>Create Endpoint</b>.</p>
                        </li></ol></div>
                 
             
             
                <p><b>Configure Studio and Data Wrangler</b></p>
                <p>This section explains how to configure Studio and Data Wrangler.</p>
                <div class="orderedlist">
                     
                     
                     
                     
                     
                     
                <ol><li>
                        <p>Configure the security group.</p>
                        <div class="orderedlist">
                             
                             
                             
                             
                             
                             
                        <ol><li>
                                <p>Navigate to the Amazon EC2 menu in the AWS Console.</p>
                            </li><li>
                                <p>Select the <b>Security Groups</b> option in the
                                        <b>Network &amp; Security</b> section.</p>
                            </li><li>
                                <p>Choose <b>Create Security Group</b>. </p>
                            </li><li>
                                <p>Provide a name and description for your security group (such
                                    as <code class="code">datawrangler-doc-sagemaker-studio</code>). </p>
                            </li><li>
                                <p>Create the following inbound rules.</p>
                                <div class="itemizedlist">
                                     
                                     
                                     
                                <ul class="itemizedlist"><li class="listitem">
                                        <p>The HTTPS connection to the security group you
                                            provisioned for the Snowflake PrivateLink connection you
                                            created in the <em>Set up the
                                                Snowflake PrivateLink Integration</em>
                                            step.</p>
                                    </li><li class="listitem">
                                        <p>The HTTP connection to the security group you
                                            provisioned for the Snowflake PrivateLink connection you
                                            created in the <em>Set up the
                                                Snowflake PrivateLink Integration
                                            step</em>.</p>
                                    </li><li class="listitem">
                                        <p>The UDP and TCP for DNS (port 53) to Route 53 Resolver
                                            Inbound Endpoint security group you create in step 2 of
                                                <em>Configure Route 53 Resolver
                                                Inbound Endpoint for your VPC</em>.</p>
                                    </li></ul></div>
                            </li><li>
                                <p>Choose <b>Create Security Group</b>
                                    button in the lower right hand corner.</p>
                            </li></ol></div>
                    </li><li>
                        <p>Configure Studio.</p>
                        <div class="itemizedlist">
                             
                             
                             
                             
                             
                             
                             
                             
                        <ul class="itemizedlist"><li class="listitem">
                                
                                
                                <p>Navigate to the SageMaker menu in the AWS console.</p>
                            </li><li class="listitem">
                                <p>From the left hand console, Select the <b>SageMaker
                                        Studio</b> option.</p>
                            </li><li class="listitem">
                                <p>If you do not have any domains configured, the <b>Get
                                        Started</b> menu is present.</p>
                            </li><li class="listitem">
                                <p>Select the <b>Standard Setup</b> option from the
                                        <b>Get Started</b>
                                    menu.</p>
                            </li><li class="listitem">
                                <p>Under <b>Authentication method</b>, select
                                        <b>AWS Identity and Access Management
                                        (IAM)</b>.</p>
                            </li><li class="listitem">
                                <p>From the <b>Permissions</b> menu, you can create
                                    a new role or use a pre-existing role, depending on your use
                                    case.</p>
                                <div class="itemizedlist">
                                     
                                     
                                <ul class="itemizedlist"><li class="listitem">
                                        <p>If you choose <b>Create a new role</b>,
                                            you are presented the option to provide an S3 bucket
                                            name, and a policy is generated for you.</p>
                                    </li><li class="listitem">
                                        <p>If you already have a role created with permissions
                                            for the S3 buckets to which you require access, select
                                            the role from the dropdown list. This role should have
                                            the <code class="code">AmazonSageMakerFullAccess</code> policy
                                            attached to it.</p>
                                    </li></ul></div>
                            </li><li class="listitem">
                                <p>Select the <b>Network and Storage</b> dropdown
                                    list to configure the VPC, security, and subnets SageMaker
                                    uses.</p>
                                <div class="itemizedlist">
                                     
                                     
                                     
                                     
                                <ul class="itemizedlist"><li class="listitem">
                                        <p>Under <b>VPC</b>, select the VPC in
                                            which your Snowflake PrivateLink connection
                                            exists.</p>
                                    </li><li class="listitem">
                                        <p>Under <b>Subnet(s)</b>, select the
                                            subnets which have access to the Snowflake PrivateLink
                                            connection.</p>
                                    </li><li class="listitem">
                                        <p>Under <b>Network Access for
                                                Studio</b>, select <b>VPC
                                                Only</b>.</p>
                                    </li><li class="listitem">
                                        <p>Under <b>Security Group(s)</b>, select
                                            the security group you created in step 1.</p>
                                    </li></ul></div>
                            </li><li class="listitem">
                                <p>Choose <b>Submit</b>.</p>
                            </li></ul></div>
                    </li><li>
                        <p>Edit the SageMaker security group.</p>
                        <div class="itemizedlist">
                             
                        <ul class="itemizedlist"><li class="listitem">
                                <p>Create the following inbound rules:</p>
                                <div class="itemizedlist">
                                     
                                     
                                <ul class="itemizedlist"><li class="listitem">
                                        <p>Port 2049 to the inbound and outbound NFS Security
                                            Groups created automatically by SageMaker in step 2 (the
                                            security group names contain the Studio domain
                                            ID).</p>
                                    </li><li class="listitem">
                                        <p>Access to all TCP ports to itself (required for SageMaker
                                            for VPC Only).</p>
                                    </li></ul></div>
                            </li></ul></div>
                    </li><li>
                        <p>Edit the VPC Endpoint Security Groups:</p>
                        <div class="itemizedlist">
                             
                             
                             
                        <ul class="itemizedlist"><li class="listitem">
                                <p>Navigate to the Amazon EC2 menu in the AWS console.</p>
                            </li><li class="listitem">
                                <p>Locate the security group you created in a preceding
                                    step.</p>
                            </li><li class="listitem">
                                <p>Add an inbound rule allowing for HTTPS traffic from the
                                    security group created in step 1.</p>
                            </li></ul></div>
                    </li><li>
                        <p>Create a user profile.</p>
                        <div class="itemizedlist">
                             
                             
                             
                             
                        <ul class="itemizedlist"><li class="listitem">
                                <p>From the <b>SageMaker Studio Control Panel </b>,
                                    choose <b>Add User</b>.</p>
                            </li><li class="listitem">
                                <p>Provide a user name. </p>
                            </li><li class="listitem">
                                <p>For the <b>Execution Role</b>, choose to create
                                    a new role or to use a pre-existing role.</p>
                                <div class="itemizedlist">
                                     
                                     
                                <ul class="itemizedlist"><li class="listitem">
                                        <p>If you choose <b>Create a new role</b>,
                                            you are presented the option to provide an Amazon S3 bucket
                                            name, and a policy is generated for you.</p>
                                    </li><li class="listitem">
                                        <p>If you already have a role created with permissions to
                                            the Amazon S3 buckets to which you require access, select the
                                            role from the dropdown list. This role should have the
                                                <code class="code">AmazonSageMakerFullAccess</code> policy
                                            attached to it.</p>
                                    </li></ul></div>
                            </li><li class="listitem">
                                <p>Choose <b>Submit</b>. </p>
                            </li></ul></div>
                    </li><li>
                        <p>Create a data flow (follow the data scientist guide outlined in a
                            preceding section). </p>
                        <div class="itemizedlist">
                             
                        <ul class="itemizedlist"><li class="listitem">
                                <p>When adding a Snowflake connection, enter the value of
                                        <code class="code">privatelink-account-name</code> (from the <em>Set up Snowflake PrivateLink
                                        Integration</em> step) into the <b>Snowflake
                                        account name (alphanumeric)</b> field, instead of the
                                    plain Snowflake account name. Everything else is left
                                    unchanged.</p>
                            </li></ul></div>
                    </li></ol></div>
             
         
             
                <h4 id="data-wrangler-snowflake-admin-ds-info">Provide information to the
                        data scientist</h4>
                <p>Provide the data scientist with the information that they need to access
                    Snowflake from Amazon SageMaker Data Wrangler.</p>
                <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>Your users need to run Amazon SageMaker Studio version 1.3.0 or later. For information about checking the version of Studio and updating it, see <a href="data-wrangler.html">Prepare ML Data with Amazon SageMaker Data Wrangler</a>.</p></div></div>

                <div class="procedure"><ol><li>
                        <p>To allow your data scientist to access Snowflake from SageMaker Data Wrangler,
                            provide them with one of the following:</p>
                        <div class="itemizedlist">
                             
                             
                             
                             
                        <ul class="itemizedlist"><li class="listitem">
                                <p>For Basic Authentication, a Snowflake account name, user name, and password.</p>
                            </li><li class="listitem">
                                <p>For OAuth, a user name and password in the identity provider.</p>
                            </li><li class="listitem">
                                <p>For ARN, the Secrets Manager secret Amazon Resource Name (ARN).</p>
                            </li><li class="listitem">
                                <p>A secret created with <a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html">AWS
                                        Secrets Manager</a> and the ARN of the secret. Use the
                                    following procedure below to create the secret for Snowflake if
                                    you choose this option.</p>
                                <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>If your data scientists use the <b>Snowflake
                                            Credentials (User name and Password)</b> option
                                        to connect to Snowflake, you can use <a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html">Secrets
                                            Manager</a> to store the credentials in a secret.
                                        Secrets Manager rotates secrets as part of a best practice
                                        security plan. The secret created in Secrets Manager is only
                                        accessible with the Studio role configured when you set
                                        up a Studio user profile. This requires you to add this
                                        permission, <code class="code">secretsmanager:PutResourcePolicy</code>,
                                        to the policy that is attached to your Studio
                                        role.</p><p>We strongly recommend that you scope the role policy to
                                        use different roles for different groups of Studio
                                        users. You can add additional resource-based permissions for
                                        the Secrets Manager secrets. See <a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/manage_secret-policy.html">Manage Secret Policy</a> for condition keys you can
                                        use.</p><p>For information about creating a secret, see <a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/create_secret.html">Create a secret</a>. You're charged for the secrets
                                        that you create.</p></div></div>

                            </li></ul></div>

                    </li><li>
                        <p>(Optional) Provide the data scientist with the name of the storage integration that
                            you created using the following procedure <a href="                                      https://docs.snowflake.com/en/user-guide/data-load-s3-config-storage-integration.html#step-3-create-a-cloud-storage-integration-in-snowflake" rel="noopener noreferrer" target="_blank"><span>Create a Cloud Storage Integration in Snowflake</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. This is
                            the name of the new integration and is called
                                <code class="code">integration_name</code> in the <code class="code">CREATE INTEGRATION</code>
                            SQL command you ran, which is shown in the following snippet: </p>
                        <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="sql ">
  CREATE STORAGE INTEGRATION integration_name
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = S3
  ENABLED = TRUE
  STORAGE_AWS_ROLE_ARN = 'iam_role'
  [ STORAGE_AWS_OBJECT_ACL = 'bucket-owner-full-control' ]
  STORAGE_ALLOWED_LOCATIONS = ('s3://bucket/path/', 's3://bucket/path/')
  [ STORAGE_BLOCKED_LOCATIONS = ('s3://bucket/path/', 's3://bucket/path/') ]
                             </code></pre>
                    </li></ol></div>
             

         

         
            <h3 id="data-wrangler-snowflake-ds">Data Scientist Guide</h3>
            <p>Use the following to connect Snowflake and access your data in Data Wrangler.</p>
            <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>Your administrator needs to use the information in the preceding sections to set up Snowflake. If you're running into issues, contact them for troubleshooting help.</p></div></div>
          

            
            <p>You can connect to Snowflake in one of the following ways:</p>
            <div class="itemizedlist">
                 
                 
                                          
            <ul class="itemizedlist"><li class="listitem">
                    <p>Specifying your Snowflake credentials (account name, user name, and
                        password) in Data Wrangler. </p>
                </li><li class="listitem">
                    <p>Providing an Amazon Resource Name (ARN) of a secret containing the credentials.</p>
                </li><li class="listitem">
                    <p>Using an open standard for access delegation (OAuth) provider that connects to Snowflake. Your administrator can give you access to one of the following OAuth providers:</p>
                    <div class="itemizedlist">
                         
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p><a href="https://docs.snowflake.com/en/user-guide/oauth-azure.html" rel="noopener noreferrer" target="_blank"><span>Azure AD</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                        </li><li class="listitem">
                            <p><a href="https://docs.snowflake.com/en/user-guide/oauth-okta.html" rel="noopener noreferrer" target="_blank"><span>Okta</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                        </li><li class="listitem">
                            <p><a href="https://docs.snowflake.com/en/user-guide/oauth-pingfed.html" rel="noopener noreferrer" target="_blank"><span>Ping Federate</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                        </li></ul></div>
                </li></ul></div>
            <p>Talk to your administrator about the method that you need to use to connect to Snowflake.</p>

            
            <p>The following sections have information about how you can connect to Snowflake using the preceding methods.</p>
            <awsdocs-tabs><dl style="display: none">
                <dt>Specifying your Snowflake Credentials</dt><dd tab-id="specifying-your-snowflake-credentials">
                        <div class="procedure"><h6>To import a dataset into Data Wrangler from Snowflake using your credentials</h6><ol><li>
                                <p>Sign into <a href="https://console.aws.amazon.com/sagemaker" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker
                                    Console</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                            </li><li>
                                <p>Choose <b>Studio</b>.</p>
                            </li><li>
                                <p>Choose <b>Launch app</b>.</p>
                            </li><li>
                                <p>From the dropdown list, select <b>Studio</b>.</p>
                            </li><li>
                                <p>Choose the Home icon.</p>
                            </li><li>
                                <p>Choose <b>Data</b>.</p>
                            </li><li>
                                <p>Choose <b>Data Wrangler</b>.</p>
                            </li><li>
                                <p>Choose <b>Import data</b>.</p>
                            </li><li>
                                <p>Under <b>Available</b>, choose
                                    <b>Snowflake</b>.</p>
                            </li><li>
                                <p>For <b>Connection name</b>, specify a name that uniquely identifies the connection.</p>
                            </li><li>
                                <p>For <b>Authentication method</b>, choose <b>Basic Username-Password</b>.</p>
                            </li><li>
                                <p>For <b>Snowflake account name (alphanumeric)</b>, specify the full name of the Snowflake account.</p>
                            </li><li>
                                <p>For <b>Username</b>, specify the username that you use to access the Snowflake account.</p>
                            </li><li>
                                <p>For <b>Password</b>, specify the password associated with the username.</p>
                            </li><li>
                                <p>(Optional) For <b>Advanced settings</b>. specify the following:</p>
                                <div class="itemizedlist">
                                     
                                     
                                     
                                <ul class="itemizedlist"><li class="listitem">
                                        <p><b>Role</b> – A role within Snowflake. Some roles have access to different datasets. If you don't specify a role, Data Wrangler uses the default role in your Snowflake account.</p>
                                    </li><li class="listitem">
                                        <p><b>Storage integration</b> – When you specify and run a query, Data Wrangler creates a temporary copy of the query results in memory. To store a permanent copy of the query results, specify the Amazon S3 location for the storage integration. Your administrator provided you with the S3 URI.</p>
                                    </li><li class="listitem">
                                        <p><b>KMS key ID</b> – A KMS key that you've created. You can specify its ARN to encrypt the output of the Snowflake query. Otherwise, Data Wrangler uses the default encryption.</p>
                                    </li></ul></div>
                            </li><li>
                                <p>Choose <b>Connect</b>.</p>
                            </li></ol></div>
                    </dd>
                <dt>Providing an Amazon Resource Name (ARN)</dt><dd tab-id="providing-an-amazon-resource-name-(arn)">
                        <div class="procedure"><h6>To import a dataset into Data Wrangler from Snowflake using an ARN</h6><ol><li>
                                <p>Sign into <a href="https://console.aws.amazon.com/sagemaker" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker
                                    Console</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                            </li><li>
                                <p>Choose <b>Studio</b>.</p>
                            </li><li>
                                <p>Choose <b>Launch app</b>.</p>
                            </li><li>
                                <p>From the dropdown list, select <b>Studio</b>.</p>
                            </li><li>
                                <p>Choose the Home icon.</p>
                            </li><li>
                                <p>Choose <b>Data</b>.</p>
                            </li><li>
                                <p>Choose <b>Data Wrangler</b>.</p>
                            </li><li>
                                <p>Choose <b>Import data</b>.</p>
                            </li><li>
                                <p>Under <b>Available</b>, choose
                                    <b>Snowflake</b>.</p>
                            </li><li>
                                <p>For <b>Connection name</b>, specify a name that uniquely identifies the connection.</p>
                            </li><li>
                                <p>For <b>Authentication method</b>, choose <b>ARN</b>.</p>
                            </li><li>
                                <p><b>Secrets Manager ARN</b> – The ARN of the AWS Secrets Manager secret used to store the credentials used to connect to Snowflake.</p>
                            </li><li>
                                <p>(Optional) For <b>Advanced settings</b>. specify the following:</p>
                                <div class="itemizedlist">
                                     
                                     
                                     
                                <ul class="itemizedlist"><li class="listitem">
                                        <p><b>Role</b> – A role within Snowflake. Some roles have access to different datasets. If you don't specify a role, Data Wrangler uses the default role in your Snowflake account.</p>
                                    </li><li class="listitem">
                                        <p><b>Storage integration</b> – When you specify and run a query, Data Wrangler creates a temporary copy of the query results in memory. To store a permanent copy of the query results, specify the Amazon S3 location for the storage integration. Your administrator provided you with the S3 URI.</p>
                                    </li><li class="listitem">
                                        <p><b>KMS key ID</b> – A KMS key that you've created. You can specify its ARN to encrypt the output of the Snowflake query. Otherwise, Data Wrangler uses the default encryption.</p>
                                    </li></ul></div>
                            </li><li>
                                <p>Choose <b>Connect</b>.</p>
                            </li></ol></div>
                    </dd>
                <dt>Using an OAuth Connection</dt><dd tab-id="using-an-oauth-connection">
                        <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>Your administrator customized your Studio environment to provide the functionality you're using to use an OAuth connection. You might need to restart the Jupyter server application to use the functionality.</p><p>Use the following procedure to update the Jupyter server application.</p><div class="procedure"><ol><li>
                                    <p>Within Studio, choose <b>File</b></p>
                                </li><li>
                                    <p>Choose <b>Shut down</b>.</p>
                                </li><li>
                                    <p>Choose <b>Shut down server</b>.</p>
                                </li><li>
                                    <p>Close the tab or window that you're using to access Studio.</p>
                                </li><li>
                                    <p>From the Amazon SageMaker console, open Studio.</p>
                                </li></ol></div></div></div>
                        <div class="procedure"><h6>To import a dataset into Data Wrangler from Snowflake using your credentials</h6><ol><li>
                                <p>Sign into <a href="https://console.aws.amazon.com/sagemaker" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker
                                    Console</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                            </li><li>
                                <p>Choose <b>Studio</b>.</p>
                            </li><li>
                                <p>Choose <b>Launch app</b>.</p>
                            </li><li>
                                <p>From the dropdown list, select <b>Studio</b>.</p>
                            </li><li>
                                <p>Choose the Home icon.</p>
                            </li><li>
                                <p>Choose <b>Data</b>.</p>
                            </li><li>
                                <p>Choose <b>Data Wrangler</b>.</p>
                            </li><li>
                                <p>Choose <b>Import data</b>.</p>
                            </li><li>
                                <p>Under <b>Available</b>, choose
                                    <b>Snowflake</b>.</p>
                            </li><li>
                                <p>For <b>Connection name</b>, specify a name that uniquely identifies the connection.</p>
                            </li><li>
                                <p>For <b>Authentication method</b>, choose <b>OAuth</b>.</p>
                            </li><li>
                                <p>(Optional) For <b>Advanced settings</b>. specify the following:</p>
                                <div class="itemizedlist">
                                     
                                
                                     
                                    
                                     
                                <ul class="itemizedlist"><li class="listitem">
                                        <p><b>Role</b> – A role within Snowflake. Some roles have access to different datasets. If you don't specify a role, Data Wrangler uses the default role in your Snowflake account.</p>
                                    </li><li class="listitem">
                                        <p><b>Storage integration</b> – When you specify and run a query, Data Wrangler creates a temporary copy of the query results in memory. To store a permanent copy of the query results, specify the Amazon S3 location for the storage integration. Your administrator provided you with the S3 URI.</p>
                                    </li><li class="listitem">
                                        <p><b>KMS key ID</b> – A KMS key that you've created. You can specify its ARN to encrypt the output of the Snowflake query. Otherwise, Data Wrangler uses the default encryption.</p>
                                    </li></ul></div>
                            </li><li>
                                <p>Choose <b>Connect</b>.</p>
                            </li></ol></div>
                    </dd>
            </dl></awsdocs-tabs>
            
            
            
           <p>You can begin the process of importing your data from Snowflake after you've connected to it.</p>
            <p>Within Data Wrangler, you can view your data warehouses,
                databases, and schemas, along with the eye icon with which you can preview
                your table. After you select the <b>Preview Table</b> icon, the
                schema preview of that table is generated. You must select a warehouse
                before you can preview a table.</p> 
          
                
                    <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>If you're importing a dataset with columns of type
                                <code class="code">TIMESTAMP_TZ</code> or <code class="code">TIMESTAMP_LTZ</code>, add
                                <code class="code">::string</code> to the column names of your query. For more
                            information, see <a href="https://community.snowflake.com/s/article/How-To-Unload-Timestamp-data-in-a-Parquet-file" rel="noopener noreferrer" target="_blank"><span>How To: Unload TIMESTAMP_TZ and TIMESTAMP_LTZ data to a Parquet
                                file</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p></div></div>
                                   
                    <p>After you select a data warehouse, database and schema, you can now write
                        queries and run them. The output of your query shows under <b>Query
                            results</b>.</p>
                    
                    <p>After you have settled on the output of your query, you can then import the
                        output of your query into a Data Wrangler flow to perform data transformations. </p>
                    
                    <p>After you've imported your data, navigate to your Data Wrangler flow and start adding transformations to it. For a list of available transforms, see <a href="data-wrangler-transform.html">Transform Data</a>.</p>
                    
         
        
     
        <h2 id="data-wrangler-import-saas">Import Data From Software as a Service (SaaS) Platforms</h2>
        <p>You can use Data Wrangler to import data from more than forty software as a service (SaaS)
            platforms. To import your data from your SaaS platform, you or your administrator must
            use Amazon AppFlow to transfer the data from the platform to Amazon S3 or Amazon Redshift. For more information about Amazon AppFlow, see <a href="https://docs.aws.amazon.com/appflow/latest/userguide/what-is-appflow.html">What is Amazon AppFlow?</a> If you don't need to use Amazon Redshift, we recommend transferring the data to Amazon S3 for a simpler process.</p>
        <p>Data Wrangler supports transferring data from the following SaaS platforms:</p>
        <div class="itemizedlist">
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
                       
        <ul class="itemizedlist"><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/amplitude.html">Amplitude</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-asana.html">Asana</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-braintree.html">Braintree</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-circleci.html">CircleCI</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-docusign-monitor.html">DocuSign Monitor</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-delighted.html">Delighted</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-domo.html">Domo</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/datadog.html">Datadog</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/dynatrace.html">Dynatrace</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-facebook-ads.html">Facebook Ads</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-facebook-page-insights.html">Facebook Page Insights</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-google-ads.html">Google Ads</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-google-analytics-4.html">Google Analytics 4</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-google-calendar.html">Google Calendar</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-google-search-console.html">Google Search Console</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-github.html">GitHub</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-gitlab.html">GitLab</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/infor-nexus.html">Infor Nexus</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-instagram-ads.html">Instagram Ads</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-intercom.html">Intercom</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-jdbc.html">JDBC (Sync)</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-jira-cloud.html">Jira Cloud</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-linkedin-ads.html">LinkedIn Ads</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-mailchimp.html">Mailchimp</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/marketo.html">Marketo</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-microsoft-dynamics-365.html">Microsoft Dynamics 365</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-microsoft-teams.html">Microsoft Teams</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-mixpanel.html">Mixpanel</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-okta.html">Okta</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-oracle-hcm.html">Oracle HCM</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-paypal.html">Paypal Checkout</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-pendo.html">Pendo</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/salesforce.html">Salesforce</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-salesforce-marketing-cloud.html">Salesforce Marketing Cloud</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/pardot.html">Salesforce Pardot</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/sapodata.html">SAP OData</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-sendgrid.html">SendGrid</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/servicenow.html">ServiceNow</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/singular.html">Singular</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/slack.html">Slack</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-smartsheet.html">Smartsheet</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-snapchat-ads.html">Snapchat Ads</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-stripe.html">Stripe</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/trend-micro.html">Trend Micro</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-typeform.html">Typeform</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/veeva.html">Veeva</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-woocommerce.html">WooCommerce</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/slack.html">Zendesk</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-zendesk-chat.html">Zendesk Chat</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-zendesk-sell.html">Zendesk Sell</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-zendesk-sunshine.html">Zendesk Sunshine</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-zoho-crm.html">Zoho CRM</a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/appflow/latest/userguide/connectors-zoom-meetings.html">Zoom Meetings</a></p>
            </li></ul></div>
        <p>The preceding list has links to more information about setting up your data source. You or your administrator can refer to the preceding links after you've read the following information.</p>
        <p>When you navigate to the <b>Import</b> tab of your Data Wrangler flow, you see data sources under the following sections:</p>
        <div class="itemizedlist">
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p><b>Available</b></p>
            </li><li class="listitem">
                <p><b>Set up data sources</b></p>
            </li></ul></div>
        <p>You can connect to data sources under <b>Available</b> without needing additional configuration. You can choose the data source and import your data.</p>
        <p>Data sources under <b>Set up data sources</b>, require you or your administrator to use Amazon AppFlow to transfer the data from the SaaS platform to Amazon S3 or Amazon Redshift. For information about performing a transfer, see <a href="data-wrangler-import.html#data-wrangler-import-saas-transfer">Using Amazon AppFlow to transfer your data</a>.</p>
        <p>After you perform the data transfer, the SaaS platform appears as a data source under
                <b>Available</b>. You can choose it and import the data that you've
            transferred into Data Wrangler. The data that you've transferred appears as tables that you can
            query.</p>
         
            <h3 id="data-wrangler-import-saas-transfer">Using Amazon AppFlow to transfer your data</h3>
            <p>Amazon AppFlow is a platform that you can use to transfer data from your SaaS platform to Amazon S3 or Amazon Redshift without having to write any code. To perform a data transfer, you use the AWS Management Console.</p>
            <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>You must make sure you've set up the permissions to perform a data transfer. For more information, see <a href="data-wrangler-security.html#data-wrangler-appflow-permissions">Amazon AppFlow Permissions</a>.</p></div></div>
            
            <p>After you've added permissions, you can transfer the data. Within Amazon AppFlow, you create a <em>flow</em> to transfer the data. A flow is a series of configurations. You can use it to specify whether you're running the data transfer on a schedule or whether you're partitioning the data into separate files. After you've configured the flow, you run it to transfer the data.</p>
            <p>For information about creating a flow, see <a href="https://docs.aws.amazon.com/appflow/latest/userguide/create-flow.html">Creating flows in Amazon AppFlow</a>. For information about running a flow, see <a href="https://docs.aws.amazon.com/appflow/latest/userguide/run-flow.html">Activate an Amazon AppFlow flow</a>.</p>
               
            <p>After the data has been transferred, use the following procedure to access the data in Data Wrangler.</p>
            <div class="procedure"><div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>Before you try to access your data, make sure your IAM role has the following policy:</p><pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="json ">
<span>{</span>
    "Version": "2012-10-17",
    "Statement": [
        <span>{</span>
            "Effect": "Allow",
            "Action": "glue:SearchTables",
            "Resource": [
                "arn:aws:glue:*:*:table/*/*",
                "arn:aws:glue:*:*:database/*",
                "arn:aws:glue:*:*:catalog"
            ]
        }
    ]
}   
</code></pre><p>By default, the IAM role that you use to access Data Wrangler is the <code class="code">SageMakerExecutionRole</code>. For more information about adding policies, see <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-attach-detach.html#add-policies-console">Adding IAM identity permissions (console)</a>.</p></div></div><p>To connect to a data source, do the following.</p><ol><li>
                    <p>Sign into <a href="https://console.aws.amazon.com/sagemaker" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker
                        Console</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                </li><li>
                    <p>Choose <b>Studio</b>.</p>
                </li><li>
                    <p>Choose <b>Launch app</b>.</p>
                </li><li>
                    <p>From the dropdown list, select <b>Studio</b>.</p>
                </li><li>
                    <p>Choose the Home icon.</p>
                </li><li>
                    <p>Choose <b>Data</b>.</p>
                </li><li>
                    <p>Choose <b>Data Wrangler</b>.</p>
                </li><li>
                    <p>Choose <b>Import data</b>.</p>
                </li><li>
                    <p>Under <b>Available</b>, choose the data source.</p>
                </li><li>
                    <p>For the <b>Name</b> field, specify the name of the
                        connection.</p>
                </li><li>
                    <p>(Optional) Choose <b>Advanced configuration</b>.</p>
                    <ol><li>
                            <p>Choose a <b>Workgroup</b>.</p>
                        </li><li>
                            <p>If your workgroup hasn't enforced the Amazon S3 output location or if you
                                don't use a workgroup, specify a value for <b>Amazon S3 location of
                                    query results</b>.</p>
                        </li><li>
                            <p>(Optional) For <b>Data retention period</b>, select the checkbox 
                                to set a data retention period and specify the number of days to store the data before it's deleted.</p>
                        </li><li>
                            <p>(Optional) By default, Data Wrangler saves the connection. You can choose to deselect the checkbox and not save the connection.</p>
                        </li></ol>
                </li><li>
                    <p>Choose <b>Connect</b>.</p>
                </li><li>
                    <p>Specify a query.</p>
                    <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>To help you specify a query, you can choose a table on the left-hand
                            navigation panel. Data Wrangler shows the table name and a preview of the table.
                            Choose the icon next to the table name to copy the name. You can use the
                            table name in the query.</p></div></div>
                </li><li>
                    <p>Choose <b>Run</b>.</p>
                </li><li>
                    <p>Choose <b>Import query</b>.</p>
                </li><li>
                    <p>For <b>Dataset name</b>, specify the name of the dataset.</p>
                </li><li>
                    <p>Choose <b>Add</b>.</p>
                </li></ol></div>
            
            <p>When you navigate to the <b>Import data</b> screen, you can see the
                connection that you've created. You can use the connection to import more
                data.</p>
            
               

            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
         
        
       
        
        


        

        
     
        <h2 id="data-wrangler-import-storage">Imported Data Storage</h2>
        <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p> We strongly recommend that you follow the best practices around protecting your
                Amazon S3 bucket by following <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html"> Security best practices</a>. </p></div></div>
        <p>When you query data from Amazon Athena or Amazon Redshift, the queried dataset is automatically
            stored in Amazon S3. Data is stored in the default SageMaker S3 bucket for the AWS Region in
            which you are using Studio.</p>
        <p>Default S3 buckets have the following naming convention:
                    <code class="code">sagemaker-<code class="replaceable">region</code>-<code class="replaceable">account
                    number</code></code>. For example, if your account number is
            111122223333 and you are using Studio in <code class="code">us-east-1</code>, your
            imported datasets are stored in <code class="code">sagemaker-us-east-1-</code>111122223333. </p>
        <p> Data Wrangler flows depend on this Amazon S3 dataset location, so you should not modify this
            dataset in Amazon S3 while you are using a dependent flow. If you do modify this S3 location,
            and you want to continue using your data flow, you must remove all objects in
                <code class="code">trained_parameters</code> in your .flow file. To do this, download the .flow
            file from Studio and for each instance of <code class="code">trained_parameters</code>, delete
            all entries. When you are done, <code class="code">trained_parameters</code> should be an empty JSON
            object:</p>
        <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="">"trained_parameters": <span>{</span>}</code></pre>
        <p>When you export and use your data flow to process your data, the .flow file you export
            refers to this dataset in Amazon S3. Use the following sections to learn more. </p>
         
            <h3 id="data-wrangler-import-storage-redshift">Amazon Redshift Import Storage</h3>
            <p>Data Wrangler stores the datasets that result from your query in a Parquet file in your
                default SageMaker S3 bucket. </p>
            <p>This file is stored under the following prefix (directory):
                    redshift/<code class="replaceable">uuid</code>/data/, where
                    <code class="replaceable">uuid</code> is a unique identifier that gets created for
                each query. </p>
            <p>For example, if your default bucket is
                    <code class="code">sagemaker-us-east-1-111122223333</code>, a single dataset
                queried from Amazon Redshift is located in
                    s3://sagemaker-us-east-1-111122223333/redshift/<code class="replaceable">uuid</code>/data/.</p>
         
         
            <h3 id="data-wrangler-import-storage-athena">Amazon Athena Import
                    Storage</h3>
            <p>When you query an Athena database and import a dataset, Data Wrangler stores the dataset, as
                well as a subset of that dataset, or <em>preview
                    files</em>, in Amazon S3. </p>
            <p>The dataset you import by selecting <b>Import dataset</b> is stored
                in Parquet format in Amazon S3. </p>
            <p>Preview files are written in CSV format when you select <b>Run</b>
                on the Athena import screen, and contain up to 100 rows from your queried dataset. </p>
            <p>The dataset you query is located under the prefix (directory):
                    athena/<code class="replaceable">uuid</code>/data/, where
                    <code class="replaceable">uuid</code> is a unique identifier that gets created for
                each query.</p>
            <p>For example, if your default bucket is
                    <code class="code">sagemaker-us-east-1-111122223333</code>, a single dataset
                queried from Athena is located in
                    <code class="code">s3://sagemaker-us-east-1-111122223333</code>/athena/<code class="replaceable">uuid</code>/data/<code class="replaceable">example_dataset.parquet</code>.</p>
            <p>The subset of the dataset that is stored to preview dataframes in Data Wrangler is stored
                under the prefix: athena/.</p>


         
    <awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="https://docs.aws.amazon.com/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./data-wrangler-getting-started.html">Get Started with Data Wrangler</div><div id="next" class="next-link" accesskey="n" href="./data-wrangler-data-flow.html">Create and Use a Data Wrangler Flow</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/data-wrangler-import.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/data-wrangler-import.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>