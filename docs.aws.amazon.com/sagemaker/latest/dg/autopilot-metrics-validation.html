<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Metrics and validation - Amazon SageMaker</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="autopilot-metrics-validation" /><meta name="default_state" content="autopilot-metrics-validation" /><link rel="icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="canonical" href="autopilot-metrics-validation.html" /><meta name="description" content="Learn about which metrics and validation techniques are available to measure machine learning model performance." /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon SageMaker" /><meta name="guide" content="Developer Guide" /><meta name="abstract" content="Use Internet Monitor to build, train, and host machine learning models in AWS." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="autopilot-metrics-validation.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="de" /><link rel="alternative" href="autopilot-metrics-validation.html" hreflang="en-us" /><link rel="alternative" href="autopilot-metrics-validation.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/autopilot-metrics-validation.html" hreflang="zh-tw" /><link rel="alternative" href="autopilot-metrics-validation.html" hreflang="x-default" /><meta name="feedback-item" content="SageMaker" /><meta name="this_doc_product" content="Amazon SageMaker" /><meta name="this_doc_guide" content="Developer Guide" /><script defer="" src="../../../assets/r/vendor4.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor3.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor1.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-common.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-doc-page.js@version=2021.12.02"></script><link href="../../../assets/r/vendor4.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-common.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-doc-page.css@version=2021.12.02.css" rel="stylesheet" /><script async="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'sagemaker'}"></script><meta id="panorama-serviceSubSection" value="Developer Guide" /><meta id="panorama-serviceConsolePage" value="Metrics and validation" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>Metrics and validation - Amazon SageMaker</title><meta name="pdf" content="/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#autopilot-metrics-validation" /><meta name="rss" content="amazon-sagemaker-release-notes.rss" /><meta name="forums" content="http://forums.aws.amazon.com/forum.jspa?forumID=285" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/autopilot-metrics-validation.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/autopilot-metrics-validation.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/autopilot-metrics-validation.html" /><meta name="keywords" content="SageMaker,Amazon SageMaker,machine learning,notebook instance,Autopilot,automatic machine learning,Auto ML,Autopilot experiment for tabular data,Autopilot metrics,AutoML metrics,metric,validation" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "Developer Guide",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Use automated ML, no-code, or low-code",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/use-auto-ml.html"
      },
      {
        "@type" : "ListItem",
        "position" : 5,
        "name" : "SageMaker Autopilot",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.html"
      },
      {
        "@type" : "ListItem",
        "position" : 6,
        "name" : "Create an Amazon SageMaker Autopilot experiment for tabular data",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development-create-experiment.html"
      },
      {
        "@type" : "ListItem",
        "position" : 7,
        "name" : "Metrics and validation",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development-create-experiment.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#autopilot-metrics-validation" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="https://aws.amazon.com">AWS</a><a href="https://docs.aws.amazon.com/index.html">Documentation</a><a href="https://docs.aws.amazon.com/sagemaker/index.html">Amazon SageMaker</a><a href="whatis.html">Developer Guide</a></div><div id="page-toc-src"><a href="autopilot-metrics-validation.html#autopilot-metrics">Autopilot metrics</a><a href="autopilot-metrics-validation.html#autopilot-weighted-metrics">Autopilot weighted metrics</a><a href="autopilot-metrics-validation.html#autopilot-cross-validation">Cross-validation in Autopilot</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="autopilot-metrics-validation">Metrics and validation</h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>This guide shows metrics and validation techniques that you can use to measure machine
    learning model performance. Amazon SageMaker Autopilot produces metrics that measure the predictive quality of
    machine learning model candidates. The metrics calculated for candidates are specified using an
    array of <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_MetricDatum.html">MetricDatum</a> types.</p>
    <h2 id="autopilot-metrics">Autopilot metrics</h2>
    <p>The following list contains the names of the metrics that are currently available to
      measure model performance within Autopilot.</p>

    <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Autopilot supports sample weights. To learn more about sample weights and the available
        objective metrics, see <a href="autopilot-metrics-validation.html#autopilot-weighted-metrics">Autopilot weighted metrics</a>.</p></div></div>

    <p>The following are the available metrics.</p>

    <div class="variablelist">
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
    <dl>
        <dt><b><span class="term"><code class="code">Accuracy</code></span></b></dt>
        <dd>
          <p> The ratio of the number of correctly classified items to the total number of
            (correctly and incorrectly) classified items. It is used for both binary and multiclass
            classification. Accuracy measures how close the predicted class values are to the actual
            values. Values for accuracy metrics vary between zero (0) and one (1). A value of 1
            indicates perfect accuracy, and 0 indicates perfect inaccuracy.</p>
        </dd>
      
        <dt><b><span class="term"><code class="code">AUC</code></span></b></dt>
        <dd>
          <p> The area under the curve (AUC) metric is used to compare and evaluate binary
            classification by algorithms that return probabilities, such as logistic regression. To
            map the probabilities into classifications, these are compared against a threshold
            value. </p>
          <p>The relevant curve is the receiver operating characteristic curve. The
            curve plots the true positive rate (TPR) of predictions (or recall) against the
            false positive rate (FPR) as a function of the threshold value, above which a prediction
            is considered positive. Increasing the threshold results in fewer false positives, but
            more false negatives. </p>
          <p>AUC is the area under this
                            receiver operating characteristic curve. Therefore, AUC provides an aggregated measure
            of the model performance across all possible classification thresholds. AUC scores vary
            between 0 and 1. A score of 1 indicates perfect accuracy, and a score of one half (0.5)
            indicates that the prediction is not better than a random classifier. </p>
        </dd>
      
        <dt><b><span class="term"><code class="code">BalancedAccuracy</code></span></b></dt>
        <dd>
          <p><code class="code">BalancedAccuracy</code> is a metric that measures the ratio of accurate
            predictions to all predictions. This ratio is calculated after normalizing true
            positives (TP) and true negatives (TN) by the total number of positive (P) and negative
            (N) values. It is used in both binary and multiclass classification and is defined as
            follows: 0.5*((TP/P)+(TN/N)), with values ranging from 0 to 1.
              <code class="code">BalancedAccuracy</code> gives a better measure of accuracy when the number of
            positives or negatives differ greatly from each other in an imbalanced dataset, such as
            when only 1% of email is spam. </p>
        </dd>
      
        <dt><b><span class="term"><code class="code">F1</code></span></b></dt>
        <dd>
          <p>The <code class="code">F1</code> score is the harmonic mean of the precision and recall, defined
            as follows: F1 = 2 * (precision * recall) / (precision + recall). It is used for binary
            classification into classes traditionally referred to as positive and negative.
            Predictions are said to be true when they match their actual (correct) class, and false
            when they do not. </p>
          <p>Precision is the ratio of the true positive predictions to all positive predictions,
            and it includes the false positives in a dataset. Precision measures the quality of the
            prediction when it predicts the positive class. </p>
          <p>Recall (or sensitivity) is the ratio of the true positive predictions to all actual
            positive instances. Recall measures how completely a model predicts the actual class
            members in a dataset. </p>
          <p>F1 scores vary between 0 and 1. A score of 1 indicates the best possible
            performance, and 0 indicates the worst.</p>
        </dd>
      
        <dt><b><span class="term"><code class="code">F1macro</code></span></b></dt>
        <dd>
          <p>The <code class="code">F1macro</code> score applies F1 scoring to multiclass classification
            problems. It does this by calculating the precision and recall, and then taking their
            harmonic mean to calculate the F1 score for each class. Lastly, the <code class="code">F1macro</code>
            averages the individual scores to obtain the <code class="code">F1macro</code> score.
              <code class="code">F1macro</code> scores vary between 0 and 1. A score of 1 indicates the best
            possible performance, and 0 indicates the worst.</p>
        </dd>
      
        <dt><b><span class="term"><code class="code">InferenceLatency</code></span></b></dt>
        <dd>
          <p>Inference latency is the approximate amount of time between making a request for a
            model prediction to receiving it from a real time endpoint to which the model is
            deployed. This metric is measured in seconds and only available in ensembling
            mode.</p>
        </dd>
      
        <dt><b><span class="term"><code class="code">LogLoss</code></span></b></dt>
        <dd>
          <p>Log loss, also known as cross-entropy loss, is a metric used to evaluate the quality
            of the probability outputs, rather than the outputs themselves. It is used in both
            binary and multiclass classification and in neural nets. It is also the cost function
            for logistic regression. Log loss is an important metric to indicate when a model makes
            incorrect predictions with high probabilities. Values range from 0 to infinity. A value
            of 0 represents a model that perfectly predicts the data.</p>
        </dd>
      
        <dt><b><span class="term"><code class="code">MAE</code></span></b></dt>
        <dd>
          <p>The mean absolute error (MAE) is a measure of how different the predicted and actual
            values are, when they're averaged over all values. MAE is commonly used in regression
            analysis to understand model prediction error. If there is linear regression, MAE
            represents the average distance from a predicted line to the actual value. MAE is
            defined as the sum of absolute errors divided by the number of observations. Values
            range from 0 to infinity, with smaller numbers indicating a better model fit to the
            data.</p>
        </dd>
      
        <dt><b><span class="term"><code class="code">MSE</code></span></b></dt>
        <dd>
          <p>The mean squared error (MSE) is the average of the squared differences between the
            predicted and actual values. It is used for regression. MSE values are always positive.
            The better a model is at predicting the actual values, the smaller the MSE value
            is.</p>
        </dd>
      
        <dt><b><span class="term"><code class="code">Precision</code></span></b></dt>
        <dd>
          <p>Precision measures how well an algorithm predicts the true positives (TP) out of all
            of the positives that it identifies. It is defined as follows: Precision = TP/(TP+FP),
            with values ranging from zero (0) to one (1), and is used in binary classification.
            Precision is an important metric when the cost of a false positive is high. For example,
            the cost of a false positive is very high if an airplane safety system is falsely deemed
            safe to fly. A false positive (FP) reflects a positive prediction that is actually
            negative in the data.</p>
        </dd>
      
        <dt><b><span class="term"><code class="code">PrecisionMacro</code></span></b></dt>
        <dd>
          <p>The precision macro computes precision for multiclass classification problems. It
            does this by calculating precision for each class and averaging scores to obtain
            precision for several classes. <code class="code">PrecisionMacro</code> scores range from zero (0) to
            one (1). Higher scores reflect the model's ability to predict true positives (TP) out of
            all of the positives that it identifies, averaged across multiple classes.</p>
        </dd>
      
        <dt><b><span class="term"><code class="code">R2</code></span></b></dt>
        <dd>
          <p>R<sup>2</sup>, also known as the coefficient of determination, is
            used in regression to quantify how much a model can explain the variance of a dependent
            variable. Values range from one (1) to negative one (-1). Higher numbers indicate a
            higher fraction of explained variability. <code class="code">R2</code> values close to zero (0)
            indicate that very little of the dependent variable can be explained by the model.
            Negative values indicate a poor fit and that the model is outperformed by a constant
            function. For linear regression, this is a horizontal line.</p>
        </dd>
      
        <dt><b><span class="term"><code class="code">Recall</code></span></b></dt>
        <dd>
          <p>Recall measures how well an algorithm correctly predicts all of the true positives
            (TP) in a dataset. A true positive is a positive prediction that is also an actual
            positive value in the data. Recall is defined as follows: Recall = TP/(TP+FN), with
            values ranging from 0 to 1. Higher scores reflect a better ability of the model to
            predict true positives (TP) in the data. It is used in binary classification. </p>
          <p>Recall is important when testing for cancer because it's used to find all of the
            true positives. A false positive (FP) reflects a positive prediction that is actually
            negative in the data. It is often insufficient to measure only recall, because
            predicting every output as a true positive yields a perfect recall score.</p>
        </dd>
      
        <dt><b><span class="term"><code class="code">RecallMacro</code></span></b></dt>
        <dd>
          <p>The <code class="code">RecallMacro</code> computes recall for multiclass classification problems
            by calculating recall for each class and averaging scores to obtain recall for several
            classes. <code class="code">RecallMacro</code> scores range from 0 to 1. Higher scores reflect the
            model's ability to predict true positives (TP) in a dataset, whereas a true positive
            reflects a positive prediction that is also an actual positive value in the data. It is
            often insufficient to measure only recall, because predicting every output as a true
            positive will yield a perfect recall score.</p>
        </dd>
      
        <dt><b><span class="term"><code class="code">RMSE</code></span></b></dt>
        <dd>
          <p>Root mean squared error (RMSE) measures the square root of the squared difference
            between predicted and actual values, and is averaged over all values. It is used in
            regression analysis to understand model prediction error. It's an important metric to
            indicate the presence of large model errors and outliers. Values range from zero (0) to
            infinity, with smaller numbers indicating a better model fit to the data. RMSE is
            dependent on scale, and should not be used to compare datasets of different
            sizes.</p>
        </dd>
      </dl></div>
    <p>Metrics that are automatically calculated for a model candidate are determined by the type
      of problem being addressed.</p>
    <div class="itemizedlist">
       
       
       
    <ul class="itemizedlist"><li class="listitem">
        <p>Regression: <code class="code">InferenceLatency</code>, <code class="code">MAE</code>, <code class="code">MSE</code>,
            <code class="code">R2</code>, <code class="code">RMSE</code></p>
      </li><li class="listitem">
        <p>Binary classification: <code class="code">Accuracy</code>, <code class="code">AUC</code>,
            <code class="code">BalancedAccuracy</code>, <code class="code">F1</code>, <code class="code">InferenceLatency</code>,
            <code class="code">LogLoss</code>, <code class="code">Precision</code>, <code class="code">Recall</code></p>
      </li><li class="listitem">
        <p>Multiclass classification: <code class="code">Accuracy</code>, <code class="code">BalancedAccuracy</code>,
            <code class="code">F1macro</code>, <code class="code">InferenceLatency</code>, <code class="code">LogLoss</code>,
            <code class="code">PrecisionMacro</code>, <code class="code">RecallMacro</code></p>
      </li></ul></div>
   
    <h2 id="autopilot-weighted-metrics">Autopilot weighted metrics</h2>
    <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Autopilot supports sample weights in ensembling mode only for all <a href="autopilot-metrics-validation.html#autopilot-metrics">available
          metrics</a> with the exception of <code class="code">Balanced Accuracy</code> and
          <code class="code">InferenceLatency</code>. <code class="code">BalanceAccuracy</code> comes with its own weighting
        scheme for imbalanced datasets that does not require sample weights.
          <code class="code">InferenceLatency</code> does not support sample weights. Both objective
          <code class="code">Balanced Accuracy</code> and <code class="code">InferenceLatency</code> metrics ignore any
        existing sample weights when training and evaluating a model.</p></div></div>
    <p>Users can add a sample weights column to their data to ensure that each observation used
      to train a machine learning model is given a weight corresponding to its perceived importance
      to the model. This is especially useful in scenarios in which the observations in the dataset
      have varying degrees of importance, or in which a dataset contains a disproportionate number
      of samples from one class compared to others. Assigning a weight to each observation based on
      its importance or greater importance to a minority class can help a model’s overall
      performance, or ensure that a model is not biased toward the majority class.</p>
    <p>For information about how to pass sample weights when creating an experiment in the
      Studio UI, see <em>Step 7</em> in <a href="autopilot-automate-model-development-create-experiment.html">Create
        an Autopilot experiment using Studio</a>. </p>
    <p>For information about how to pass sample weights programmatically when creating an Autopilot
      experiment using the API, see <em>How to add sample weights to an AutoML
        job</em> in <a href="autopilot-automate-model-development-create-experiment.html#autopilot-automate-model-development-create-experiment-api">Create an Autopilot experiment programmatically</a>.</p>

   
    <h2 id="autopilot-cross-validation">Cross-validation in Autopilot</h2>
    <p>Cross-validation is used in to reduce overfitting and bias in model selection. It is also
      used to assess how well a model can predict the values of an unseen validation dataset, if the
      validation dataset is drawn from the same population. This method is especially important when
      training on datasets that have a limited number of training instances. </p>
    <p>Autopilot uses cross-validation to build models in hyperparameter optimization (HPO) and
      ensemble training mode. The first step in the Autopilot cross-validation process is to split the
      data into k-folds.</p>
     
      <h3 id="autopilot-cross-validation-kfold">K-fold splitting</h3>
      <p>K-fold splitting is a method that separates an input training dataset into multiple
        training and validation datasets. The dataset is split into <code class="code">k</code> equally-sized
        sub-samples called folds. Models are then trained on <code class="code">k-1</code> folds and tested
        against the remaining k<sup>th</sup> fold, which is the validation dataset.
        The process is repeated <code class="code">k</code> times using a different data set for validation. </p>
      <p>The following image depicts k-fold splitting with k = 4 folds. Each fold is represented
        as a row. The dark-toned boxes represent the parts of the data used in training. The
        remaining light-toned boxes indicate the validation datasets. </p>
      <div class="mediaobject">
         
          <img src="../../../images/sagemaker/latest/dg/images/autopilot/autopilot-metrics-kfold-splits.png" class="aws-docs-img-whiteBg aws-docs-img-padding" alt="&#xA;           K-fold splitting with 4-folds depicted as boxes: dark for data used; light for&#xA;            validation datasets.&#xA;        " style="max-width:65%" />
         
         
      </div>

      <p>Autopilot uses k-fold cross-validation for both hyperparameter optimization (HPO) mode and
        ensembling mode.</p>

      <p>You can deploy Autopilot models that are built using cross-validation like you would with
        any other Autopilot or SageMaker model.</p>
     
     
      <h3 id="autopilot-cross-validation-hpo">HPO mode</h3>
      <p>K-fold cross-validation uses the k-fold splitting method for cross-validation. In HPO
        mode, Autopilot automatically implements k-fold cross-validation for small datasets with 50,000
        or fewer training instances. Performing cross-validation is especially important when
        training on small datasets because it protects against overfitting and selection bias. </p>
      <p>HPO mode uses a <em>k</em> value of 5 on each of the candidate
        algorithms that are used to model the dataset. Multiple models are trained on different
        splits, and the models are stored separately. When training is complete, validation metrics
        for each of the models are averaged to produce a single estimation metric. Lastly, Autopilot
        combines the models from the trial with the best validation metric into an ensemble model.
        Autopilot uses this ensemble model to make predictions.</p>
      <p>The validation metric for the models trained by Autopilot is presented as the objective
        metric in the model leaderboard. Autopilot uses the default validation metric for each problem
        type that it handles, unless you specify otherwise. For the list of all metrics that Autopilot
        uses, see <a href="autopilot-metrics-validation.html#autopilot-metrics">Autopilot metrics</a>.</p>
      <p>For example, the <a href="http://lib.stat.cmu.edu/datasets/boston" rel="noopener noreferrer" target="_blank"><span>Boston Housing
          dataset</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> contains only 861 samples. If you build a model to predict house sale
        prices using this dataset without cross-validation, you risk training on a dataset that is
        not representative of the Boston housing stock. If you split the data only once into
        training and validation subsets, the training fold may only contain data mainly from the
        suburbs. As a result, you would train on data that isn't representative of the rest of the
        city. In this example, your model would likely overfit on this biased selection. K-fold
        cross-validation can reduce the risk of this kind of error by making full and randomized use
        of the available data for both training and validation.</p>
      <p>Cross-validation can increase training times by an average of 20%. Training times may
        also increase significantly for complex datasets.</p>
      <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>In HPO mode, you can see the training and validation metrics from each fold in your
            <code class="code">/aws/sagemaker/TrainingJobs</code> CloudWatch Logs. For more information about CloudWatch Logs, see
            <a href="logging-cloudwatch.html">Log Amazon SageMaker Events with Amazon CloudWatch</a>. </p></div></div>
     
     
      <h3 id="autopilot-cross-validation-ensemble">Ensembling mode</h3>
      <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Autopilot supports sample weights in ensembling mode. For the list of available metrics
          supporting sample weights, see <a href="autopilot-metrics-validation.html#autopilot-metrics">Autopilot metrics</a>.</p></div></div>
      <p>In ensembling mode, cross-validation is performed regardless of dataset size. Customers
        can either provide their own validation dataset and custom data split ratio, or let Autopilot
        split the dataset automatically into an 80-20% split ratio. The training data is then split
        into <code class="code">k</code>-folds for cross-validation, where the value of <code class="code">k</code> is
        determined by the AutoGluon engine. An ensemble consists of multiple machine learning
        models, where each model is known as the base model. A single base model is trained on
          (<code class="code">k</code>-1) folds and makes out-of-fold predictions on the remaining fold. This
        process is repeated for all <code class="code">k</code> folds, and the out-of-fold (OOF) predictions are
        concatenated to form a single set of predictions. All base models in the ensemble follow
        this same process of generating OOF predictions.</p>
      <p>The following image depicts k-fold validation with <code class="code">k</code> = 4 folds. Each fold
        is represented as a row. The dark-toned boxes represent the parts of the data used in
        training. The remaining light-toned boxes indicate the validation datasets. </p>
      <p>In the upper part of the image, in each fold, the first base model makes predictions on
        the validation dataset after training on the training datasets. At each subsequent fold, the
        datasets change roles. A dataset that was previously used for training is now used for
        validation, and this also applies in reverse. At the end of <code class="code">k</code> folds, all of the
        predictions are concatenated to form a single set of predictions called an out-of-fold (OOF)
        prediction. This process is repeated for each <code class="code">n</code> base models.</p>
      <div class="mediaobject">
         
          <img src="../../../images/sagemaker/latest/dg/images/autopilot/autopilot-metrics-kfold.PNG" class="aws-docs-img-whiteBg aws-docs-img-padding" alt="&#xA;          k-fold validation: Four rows of boxes depict 4-folds that generate a row of OOF&#xA;            predictions.&#xA;        " style="max-width:65%" />
         
         
      </div>
      <p>The OOF predictions for each base model are then used as features to train a stacking
        model. The stacking model learns the importance weights for each base model. These weights
        are used to combine the OOF predictions to form the final prediction. Performance on the
        validation dataset determines which base or stacking model is the best, and this model is
        returned as the final model.</p>

      <p>In ensemble mode, you can either provide your own validation dataset or let Autopilot split
        the input dataset automatically into 80% train and 20% validation datasets. The training
        data is then split into <code class="code">k</code>-folds for cross-validation and produces an OOF
        prediction and a base model for each fold.</p>
      <p>These OOF predictions are used as features to train a stacking model, which
        simultaneously learns weights for each base model. These weights are used to combine the OOF
        predictions to form the final prediction. The validation datasets for each fold are used for
        hyperparameter tuning of all base models and the stacking model. Performance on the
        validation datasets determines which base or stacking model is the best model, and this
        model is returned as the final model.</p>
     
  <awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="https://docs.aws.amazon.com/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./autopilot-model-support-validation.html">Training Modes and Algorithms</div><div id="next" class="next-link" accesskey="n" href="./autopilot-deploy-models.html">Model Deployment and Prediction</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/autopilot-metrics-validation.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/autopilot-metrics-validation.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>