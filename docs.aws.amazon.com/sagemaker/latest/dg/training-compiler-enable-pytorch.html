<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Run PyTorch Training Jobs with SageMaker Training Compiler - Amazon SageMaker</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="training-compiler-enable-pytorch" /><meta name="default_state" content="training-compiler-enable-pytorch" /><link rel="icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="canonical" href="training-compiler-enable-pytorch.html" /><meta name="description" content="Use SageMaker Python SDK or API to enable SageMaker Training Compiler." /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon SageMaker" /><meta name="guide" content="Developer Guide" /><meta name="abstract" content="Use Internet Monitor to build, train, and host machine learning models in AWS." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="training-compiler-enable-pytorch.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="de" /><link rel="alternative" href="training-compiler-enable-pytorch.html" hreflang="en-us" /><link rel="alternative" href="training-compiler-enable-pytorch.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/training-compiler-enable-pytorch.html" hreflang="zh-tw" /><link rel="alternative" href="training-compiler-enable-pytorch.html" hreflang="x-default" /><meta name="feedback-item" content="SageMaker" /><meta name="this_doc_product" content="Amazon SageMaker" /><meta name="this_doc_guide" content="Developer Guide" /><script defer="" src="../../../assets/r/vendor4.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor3.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor1.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-common.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-doc-page.js@version=2021.12.02"></script><link href="../../../assets/r/vendor4.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-common.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-doc-page.css@version=2021.12.02.css" rel="stylesheet" /><script async="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'sagemaker'}"></script><meta id="panorama-serviceSubSection" value="Developer Guide" /><meta id="panorama-serviceConsolePage" value="Run PyTorch Training Jobs with SageMaker Training Compiler" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>Run PyTorch Training Jobs with SageMaker Training Compiler - Amazon SageMaker</title><meta name="pdf" content="/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#training-compiler-enable-pytorch" /><meta name="rss" content="amazon-sagemaker-release-notes.rss" /><meta name="forums" content="http://forums.aws.amazon.com/forum.jspa?forumID=285" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/training-compiler-enable-pytorch.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/training-compiler-enable-pytorch.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/training-compiler-enable-pytorch.html" /><meta name="keywords" content="SageMaker,Amazon SageMaker,machine learning,notebook instance,amazon sagemaker training compiler, sagemaker training compiler, sm training compiler,compile deep learning model, compile transformer model, compile tensorflow model, compile pytorch model, compile nlp model,amazon sagemaker training compiler, sagemaker training compiler, sm training compiler,enable training compiler, construct sagemaker estimator, sagemaker python sdk,compiler_config, compiler, config,compile deep learning model, compile transformer model, compile tensorflow model, compile pytorch model, compile nlp model,enable training compiler, construct sagemaker estimator, sagemaker python sdk" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "Developer Guide",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Train machine learning models",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/train-model.html"
      },
      {
        "@type" : "ListItem",
        "position" : 5,
        "name" : "Amazon SageMaker Training Compiler",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler.html"
      },
      {
        "@type" : "ListItem",
        "position" : 6,
        "name" : "Enable SageMaker Training Compiler",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-enable.html"
      },
      {
        "@type" : "ListItem",
        "position" : 7,
        "name" : "Run PyTorch Training Jobs with SageMaker Training Compiler",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-enable.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#training-compiler-enable-pytorch" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="https://aws.amazon.com">AWS</a><a href="https://docs.aws.amazon.com/index.html">Documentation</a><a href="https://docs.aws.amazon.com/sagemaker/index.html">Amazon SageMaker</a><a href="whatis.html">Developer Guide</a></div><div id="page-toc-src"><a href="training-compiler-enable-pytorch.html#training-compiler-enable-pytorch-pysdk">Learn how to activate SageMaker Training Compiler using the SageMaker Python SDK.</a><a href="training-compiler-enable-pytorch.html#training-compiler-enable-pytorch-api">Learn how to activate SageMaker Training Compiler using the SageMaker CreateTrainingJob
                API Operation.</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="training-compiler-enable-pytorch">Run PyTorch Training Jobs with SageMaker Training Compiler</h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>You can use any of the SageMaker interfaces to run a training job with SageMaker Training Compiler: Amazon SageMaker Studio,
        Amazon SageMaker notebook instances, AWS SDK for Python (Boto3), and AWS Command Line Interface.</p><div class="highlights" id="inline-topiclist"><h6>Topics</h6><ul><li><a href="training-compiler-enable-pytorch.html#training-compiler-enable-pytorch-pysdk">Using the SageMaker Python SDK</a></li><li><a href="training-compiler-enable-pytorch.html#training-compiler-enable-pytorch-api">Using the SageMaker
                    CreateTrainingJob API Operation</a></li></ul></div>
        <h2 id="training-compiler-enable-pytorch-pysdk">Using the SageMaker Python SDK</h2>
        <p>SageMaker Training Compiler for PyTorch is available through the SageMaker <a href="https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html" rel="noopener noreferrer" target="_blank"><span><code class="code">PyTorch</code></span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> and <a href="https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-estimator" rel="noopener noreferrer" target="_blank"><span><code class="code">HuggingFace</code></span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> framework estimator classes. To turn on
            SageMaker Training Compiler, add the <code class="code">compiler_config</code> parameter to the SageMaker estimators. Import
            the <code class="code">TrainingCompilerConfig</code> class and pass an instance of it to the
                <code class="code">compiler_config</code> parameter. The following code examples show the
            structure of SageMaker estimator classes with SageMaker Training Compiler turned on.</p>
        <div class="awsdocs-note awsdocs-tip"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Tip</h6></div><div class="awsdocs-note-text"><p>To get started with prebuilt models provided by PyTorch or Transformers, try using
                the batch sizes provided in the reference table at <a href="training-compiler-support.html#training-compiler-tested-models">Tested Models</a>.</p></div></div>
        <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>The native PyTorch support is available in the SageMaker Python SDK v2.121.0 and later.
                Make sure that you update the SageMaker Python SDK accordingly.</p></div></div>
        <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Starting PyTorch v1.12.0, SageMaker Training Compiler containers for PyTorch are available. Note that
                the SageMaker Training Compiler containers for PyTorch are not prepackaged with Hugging Face
                Transformers. If you need to install the library in the container, make sure that
                you add the <code class="code">requirements.txt</code> file under the source directory when
                submitting a training job.</p><p>For PyTorch v1.11.0 and before, use the previous versions of the SageMaker Training Compiler
                containers for Hugging Face and PyTorch.</p><p>For a complete list of framework versions and corresponding container information,
                see <a href="training-compiler-support.html#training-compiler-supported-frameworks">Supported
                    Frameworks</a>.</p></div></div>
        <p>For information that fits your use case, see one of the following options.</p>
        <div class="collapsible" data-expand-section="_collapse_all_"><awsui-expandable-section variant="container" header="For single GPU&#xA;                        training" id="training-compiler-estimator-pytorch-single" expanded="false"><awsdocs-tabs><dl style="display: none">
                    <dt>PyTorch v1.12.0 and later</dt><dd tab-id="pytorch-v1.12.0-and-later">
                            <p>To compile and train a PyTorch model, configure a SageMaker PyTorch
                                estimator with SageMaker Training Compiler as shown in the following code
                                example.</p>
                            <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>This native PyTorch support is available in the SageMaker Python
                                    SDK v2.120.0 and later. Make sure that you update the SageMaker
                                    Python SDK.</p></div></div>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="py ">from sagemaker.pytorch import PyTorch, TrainingCompilerConfig

# the original max batch size that can fit into GPU memory without compiler
batch_size_native=<code class="replaceable">12</code>
learning_rate_native=float('<code class="replaceable">5e-5</code>')

# an updated max batch size that can fit into GPU memory with compiler
batch_size=<code class="replaceable">64</code>

# update learning rate
learning_rate=learning_rate_native/batch_size_native*batch_size

hyperparameters=<span>{</span>
    "n_gpus": 1,
    "batch_size": batch_size,
    "learning_rate": learning_rate
}

pytorch_estimator=PyTorch(
    entry_point='<code class="replaceable">train.py</code>',
    source_dir='<code class="replaceable">path-to-requirements-file</code>', # Optional. Add this if need to install additional packages.
    instance_count=1,
    instance_type='<code class="replaceable">ml.p3.2xlarge</code>',
    framework_version='<code class="userinput">1.13.1</code>',
    py_version='py3',
    hyperparameters=hyperparameters,
    compiler_config=TrainingCompilerConfig(),
    disable_profiler=True,
    debugger_hook_config=False
)

pytorch_estimator.fit()</code></pre>
                        </dd>
                    <dt>Hugging Face Transformers with PyTorch v1.11.0 and before</dt><dd tab-id="hugging-face-transformers-with-pytorch-v1.11.0-and-before">
                            <p>To compile and train a transformer model with PyTorch, configure a
                                SageMaker Hugging Face estimator with SageMaker Training Compiler as shown in the following
                                code example.</p>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="py ">from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig

# the original max batch size that can fit into GPU memory without compiler
batch_size_native=<code class="replaceable">12</code>
learning_rate_native=float('<code class="replaceable">5e-5</code>')

# an updated max batch size that can fit into GPU memory with compiler
batch_size=<code class="replaceable">64</code>

# update learning rate
learning_rate=learning_rate_native/batch_size_native*batch_size

hyperparameters=<span>{</span>
    "n_gpus": 1,
    "batch_size": batch_size,
    "learning_rate": learning_rate
}

pytorch_huggingface_estimator=HuggingFace(
    entry_point='<code class="replaceable">train.py</code>',
    instance_count=1,
    instance_type='<code class="replaceable">ml.p3.2xlarge</code>',
    transformers_version='<code class="replaceable">4.21.1</code>',
    pytorch_version='<code class="replaceable">1.11.0</code>',
    hyperparameters=hyperparameters,
    compiler_config=TrainingCompilerConfig(),
    disable_profiler=True,
    debugger_hook_config=False
)

pytorch_huggingface_estimator.fit()</code></pre>
                            <p>To prepare your training script, see the following pages.</p>
                            <div class="itemizedlist">
                                 
                                 
                            <ul class="itemizedlist"><li class="listitem">
                                    <p><a href="training-compiler-pytorch-models.html#training-compiler-pytorch-models-transformers-trainer-single-gpu">For single GPU training</a> of a PyTorch model using Hugging Face Transformers'
                                            <a href="https://huggingface.co/docs/transformers/main_classes/trainer" rel="noopener noreferrer" target="_blank"><span>Trainer API</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                                </li><li class="listitem">
                                    <p><a href="training-compiler-pytorch-models.html#training-compiler-pytorch-models-non-trainer-single-gpu">For
                        single GPU training</a> of a PyTorch model without Hugging Face Transformers'
                                            <a href="https://huggingface.co/transformers/main_classes/trainer.html" rel="noopener noreferrer" target="_blank"><span>Trainer API</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                                </li></ul></div>
                            <p>To find end-to-end examples, see the following notebooks:</p>
                            <div class="itemizedlist">
                                 
                                 
                                 
                            <ul class="itemizedlist"><li class="listitem">
                                    <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-training-compiler/huggingface/pytorch_single_gpu_single_node/albert-base-v2/albert-base-v2.html" rel="noopener noreferrer" target="_blank"><span>Compile and Train a Hugging Face Transformers Trainer
                                            Model for Question and Answering with the SQuAD dataset
                                        </span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>
                                    </p>
                                </li><li class="listitem">
                                    <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-training-compiler/huggingface/pytorch_single_gpu_single_node/bert-base-cased/bert-base-cased-single-node-single-gpu.html" rel="noopener noreferrer" target="_blank"><span>Compile and Train a Hugging Face Transformer
                                                <code class="code">BERT</code> Model with the SST Dataset using
                                            SageMaker Training Compiler</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>
                                    </p>
                                </li><li class="listitem">
                                    <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-training-compiler/huggingface/pytorch_single_gpu_single_node/roberta-base/roberta-base.html" rel="noopener noreferrer" target="_blank"><span>Compile and Train a Binary Classification Trainer Model
                                            with the SST2 Dataset for Single-Node Single-GPU
                                            Training </span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                                </li></ul></div>
                        </dd>
                </dl></awsdocs-tabs></awsui-expandable-section><awsui-expandable-section variant="container" header="For distributed&#xA;                        training" id="training-compiler-estimator-pytorch-distributed" expanded="false"><awsdocs-tabs><dl style="display: none">
                    <dt>PyTorch v1.12</dt><dd tab-id="pytorch-v1.12">
                            <p>For PyTorch v1.12, you can run distributed training with SageMaker Training Compiler
                                by adding the <code class="code">pytorch_xla</code> option specified to the
                                    <code class="code">distribution</code> parameter of the SageMaker PyTorch
                                estimator class.</p>
                            <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>This native PyTorch support is available in the SageMaker Python
                                    SDK v2.121.0 and later. Make sure that you update the SageMaker
                                    Python SDK.</p></div></div>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="py ">from sagemaker.pytorch import PyTorch, TrainingCompilerConfig

# choose an instance type, specify the number of instances you want to use,
# and set the num_gpus variable the number of GPUs per instance.
instance_count=<code class="replaceable">1</code>
instance_type='<code class="replaceable">ml.p3.8xlarge</code>'
num_gpus=<code class="replaceable">4</code>

# the original max batch size that can fit to GPU memory without compiler
batch_size_native=<code class="replaceable">16</code>
learning_rate_native=float('<code class="replaceable">5e-5</code>')

# an updated max batch size that can fit to GPU memory with compiler
batch_size=<code class="replaceable">26</code>

# update learning rate
learning_rate=learning_rate_native/batch_size_native*batch_size*num_gpus*instance_count

hyperparameters=<span>{</span>
    "n_gpus": num_gpus,
    "batch_size": batch_size,
    "learning_rate": learning_rate
}

pytorch_estimator=PyTorch(
    entry_point='<code class="replaceable">your_training_script.py</code>',
    source_dir='<code class="replaceable">path-to-requirements-file</code>', # Optional. Add this if need to install additional packages.
    instance_count=instance_count,
    instance_type=instance_type,
    framework_version='<code class="userinput">1.13.1</code>',
    py_version='py3',
    hyperparameters=hyperparameters,
    compiler_config=TrainingCompilerConfig(),
    distribution =<span>{</span>'pytorchxla' : <span>{</span> 'enabled': True }},
    disable_profiler=True,
    debugger_hook_config=False
)

pytorch_estimator.fit()</code></pre>
                            <div class="awsdocs-note awsdocs-tip"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Tip</h6></div><div class="awsdocs-note-text"><p>To prepare your training script, see <a href="training-compiler-pytorch-models.html">PyTorch</a></p></div></div>
                        </dd>
                    <dt>Transformers v4.21 with PyTorch v1.11</dt><dd tab-id="transformers-v4.21-with-pytorch-v1.11">
                            <p>For PyTorch v1.11 and later, SageMaker Training Compiler is available for distributed
                                training with the <code class="code">pytorch_xla</code> option specified to the
                                    <code class="code">distribution</code> parameter.</p>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="py ">from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig

# choose an instance type, specify the number of instances you want to use,
# and set the num_gpus variable the number of GPUs per instance.
instance_count=<code class="replaceable">1</code>
instance_type='<code class="replaceable">ml.p3.8xlarge</code>'
num_gpus=<code class="replaceable">4</code>

# the original max batch size that can fit to GPU memory without compiler
batch_size_native=<code class="replaceable">16</code>
learning_rate_native=float('<code class="replaceable">5e-5</code>')

# an updated max batch size that can fit to GPU memory with compiler
batch_size=<code class="replaceable">26</code>

# update learning rate
learning_rate=learning_rate_native/batch_size_native*batch_size*num_gpus*instance_count

hyperparameters=<span>{</span>
    "n_gpus": num_gpus,
    "batch_size": batch_size,
    "learning_rate": learning_rate
}

pytorch_huggingface_estimator=HuggingFace(
    entry_point='<code class="replaceable">your_training_script.py</code>',
    instance_count=instance_count,
    instance_type=instance_type,
    transformers_version='<code class="userinput">4.21.1</code>',
    pytorch_version='<code class="userinput">1.11.0</code>',
    hyperparameters=hyperparameters,
    compiler_config=TrainingCompilerConfig(),
    distribution =<span>{</span>'pytorchxla' : <span>{</span> 'enabled': True }},
    disable_profiler=True,
    debugger_hook_config=False
)

pytorch_huggingface_estimator.fit()</code></pre>
                            <div class="awsdocs-note awsdocs-tip"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Tip</h6></div><div class="awsdocs-note-text"><p>To prepare your training script, see the following
                                    pages.</p><div class="itemizedlist">
                                     
                                     
                                <ul class="itemizedlist"><li class="listitem">
                                        <p><a href="training-compiler-pytorch-models.html#training-compiler-pytorch-models-transformers-trainer-distributed">For distributed training</a> of a PyTorch model using Hugging Face Transformers'
                                                <a href="https://huggingface.co/transformers/main_classes/trainer.html" rel="noopener noreferrer" target="_blank"><span>Trainer API</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                                    </li><li class="listitem">
                                        <p><a href="training-compiler-pytorch-models.html#training-compiler-pytorch-models-non-trainer-distributed">For
                        distributed training</a> of a PyTorch model without Hugging Face Transformers'
                                                <a href="https://huggingface.co/transformers/main_classes/trainer.html" rel="noopener noreferrer" target="_blank"><span>Trainer API</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                                    </li></ul></div></div></div>
                        </dd>
                    <dt>Transformers v4.17 with PyTorch v1.10.2 and before</dt><dd tab-id="transformers-v4.17-with-pytorch-v1.10.2-and-before">
                            <p>For the supported version of PyTorch v1.10.2 and before, SageMaker Training Compiler
                                requires an alternate mechanism for launching a distributed training
                                job. To run distributed training, SageMaker Training Compiler requires you to pass a
                                SageMaker distributed training launcher script to the
                                    <code class="code">entry_point</code> argument, and pass your training script
                                to the <code class="code">hyperparameters</code> argument. The following code
                                example shows how to configure a SageMaker Hugging Face estimator
                                applying the required changes.</p>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="py ">from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig

# choose an instance type, specify the number of instances you want to use,
# and set the num_gpus variable the number of GPUs per instance.
instance_count=<code class="replaceable">1</code>
instance_type='<code class="replaceable">ml.p3.8xlarge</code>'
num_gpus=<code class="replaceable">4</code>

# the original max batch size that can fit to GPU memory without compiler
batch_size_native=<code class="replaceable">16</code>
learning_rate_native=float('<code class="replaceable">5e-5</code>')

# an updated max batch size that can fit to GPU memory with compiler
batch_size=<code class="replaceable">26</code>

# update learning rate
learning_rate=learning_rate_native/batch_size_native*batch_size*num_gpus*instance_count

<b>training_script="<code class="replaceable">your_training_script.py</code>"</b>

hyperparameters=<span>{</span>
    "n_gpus": num_gpus,
    "batch_size": batch_size,
    "learning_rate": learning_rate,
    "training_script": training_script     # Specify the file name of your training script.
}

pytorch_huggingface_estimator=HuggingFace(
    entry_point='<code class="replaceable">distributed_training_launcher.py</code>',    # Specify the distributed training launcher script.
    instance_count=instance_count,
    instance_type=instance_type,
    transformers_version='<code class="replaceable">4.17.0</code>',
    pytorch_version='<code class="replaceable">1.10.2</code>',
    hyperparameters=hyperparameters,
    compiler_config=TrainingCompilerConfig(),
    disable_profiler=True,
    debugger_hook_config=False
)

pytorch_huggingface_estimator.fit()</code></pre>
                            <p>The launcher script should look like the following. It wraps your
                                training script and configures the distributed training environment
                                depending on the size of the training instance of your choice. </p>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="py "># distributed_training_launcher.py

#!/bin/python

import subprocess
import sys

if __name__ == "__main__":
    arguments_command = " ".join([arg for arg in sys.argv[1:]])
    """
    The following line takes care of setting up an inter-node communication
    as well as managing intra-node workers for each GPU.
    """
    subprocess.check_call("python -m torch_xla.distributed.sm_dist " + arguments_command, shell=True)</code></pre>
                            <div class="awsdocs-note awsdocs-tip"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Tip</h6></div><div class="awsdocs-note-text"><p>To prepare your training script, see the following
                                    pages.</p><div class="itemizedlist">
                                     
                                     
                                <ul class="itemizedlist"><li class="listitem">
                                        <p><a href="training-compiler-pytorch-models.html#training-compiler-pytorch-models-transformers-trainer-distributed">For distributed training</a> of a PyTorch model using Hugging Face Transformers'
                                                <a href="https://huggingface.co/transformers/main_classes/trainer.html" rel="noopener noreferrer" target="_blank"><span>Trainer API</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                                    </li><li class="listitem">
                                        <p><a href="training-compiler-pytorch-models.html#training-compiler-pytorch-models-non-trainer-distributed">For
                        distributed training</a> of a PyTorch model without Hugging Face Transformers'
                                                <a href="https://huggingface.co/transformers/main_classes/trainer.html" rel="noopener noreferrer" target="_blank"><span>Trainer API</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                                    </li></ul></div></div></div>
                            <div class="awsdocs-note awsdocs-tip"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Tip</h6></div><div class="awsdocs-note-text"><p>To find end-to-end examples, see the following
                                    notebooks:</p><div class="itemizedlist">
                                     
                                     
                                <ul class="itemizedlist"><li class="listitem">
                                        <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-training-compiler/huggingface/pytorch_multiple_gpu_single_node/language-modeling-multi-gpu-single-node.html" rel="noopener noreferrer" target="_blank"><span>Compile and Train the GPT2 Model using the
                                                Transformers Trainer API with the SST2 Dataset for
                                                Single-Node Multi-GPU Training</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                                    </li><li class="listitem">
                                        <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-training-compiler/huggingface/pytorch_multiple_gpu_multiple_node/language-modeling-multi-gpu-multi-node.html" rel="noopener noreferrer" target="_blank"><span>Compile and Train the GPT2 Model using the
                                                Transformers Trainer API with the SST2 Dataset for
                                                Multi-Node Multi-GPU Training</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                                    </li></ul></div></div></div>
                        </dd>
                </dl></awsdocs-tabs></awsui-expandable-section></div>
        <p>The following list is the minimal set of parameters required to run a SageMaker training
            job with the compiler.</p>
        <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>When using the SageMaker Hugging Face estimator, you must specify the
                    <code class="code">transformers_version</code>, <code class="code">pytorch_version</code>,
                    <code class="code">hyperparameters</code>, and <code class="code">compiler_config</code> parameters to
                enable SageMaker Training Compiler. You cannot use <code class="code">image_uri</code> to manually specify the
                Training Compiler integrated Deep Learning Containers that are listed at <a href="training-compiler-support.html#training-compiler-supported-frameworks">Supported
                    Frameworks</a>.</p></div></div>
        <div class="itemizedlist">
             
             
             
             
             
             
             
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p><code class="code">entry_point</code> (str) – Required. Specify the file name of
                    your training script.</p>
                <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>To run a distributed training with SageMaker Training Compiler and PyTorch v1.10.2 and before,
                        specify the file name of a launcher script to this parameter. The launcher
                        script should be prepared to wrap your training script and configure the
                        distributed training environment. For more information, see the following
                        example notebooks:</p><div class="itemizedlist">
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-training-compiler/huggingface/pytorch_multiple_gpu_single_node/language-modeling-multi-gpu-single-node.html" rel="noopener noreferrer" target="_blank"><span>Compile and Train the GPT2 Model using the Transformers Trainer
                                    API with the SST2 Dataset for Single-Node Multi-GPU
                                    Training</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                        </li><li class="listitem">
                            <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-training-compiler/huggingface/pytorch_multiple_gpu_multiple_node/language-modeling-multi-gpu-multi-node.html" rel="noopener noreferrer" target="_blank"><span>Compile and Train the GPT2 Model using the Transformers Trainer
                                    API with the SST2 Dataset for Multi-Node Multi-GPU
                                    Training</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                        </li></ul></div></div></div>
            </li><li class="listitem">
                <p><code class="code">source_dir</code> (str)  – Optional. Add this if need to install
                    additional packages. To install packages, you need to prapare a
                        <code class="code">requirements.txt</code> file under this directory.</p>
            </li><li class="listitem">
                <p><code class="code">instance_count</code> (int) – Required. Specify the number of
                    instances.</p>
            </li><li class="listitem">
                <p><code class="code">instance_type</code> (str) – Required. Specify the instance
                    type.</p>
            </li><li class="listitem">
                <p><code class="code">transformers_version</code> (str) – Required only when using the
                    SageMaker Hugging Face estimator. Specify the Hugging Face Transformers library
                    version supported by SageMaker Training Compiler. To find available versions, see <a href="training-compiler-support.html#training-compiler-supported-frameworks">Supported
                    Frameworks</a>.</p>
            </li><li class="listitem">
                <p><code class="code">framework_version</code> or <code class="code">pytorch_version</code> (str) –
                    Required. Specify the PyTorch version supported by SageMaker Training Compiler. To find available
                    versions, see <a href="training-compiler-support.html#training-compiler-supported-frameworks">Supported
                    Frameworks</a>.</p>
                <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>When using the SageMaker Hugging Face estimator, you must specify both
                            <code class="code">transformers_version</code> and
                        <code class="code">pytorch_version</code>.</p></div></div>
            </li><li class="listitem">
                <p><code class="code">hyperparameters</code> (dict) – Optional. Specify hyperparameters
                    for the training job, such as <code class="code">n_gpus</code>, <code class="code">batch_size</code>, and
                        <code class="code">learning_rate</code>. When you enable SageMaker Training Compiler, try larger batch sizes
                    and adjust the learning rate accordingly. To find case studies of using the
                    compiler and adjusted batch sizes to improve training speed, see <a href="training-compiler-support.html#training-compiler-tested-models">Tested Models</a> and <a href="training-compiler-examples-and-blogs.html">SageMaker Training Compiler Example Notebooks and
                Blogs</a>.</p>
                <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>To run a distributed training with SageMaker Training Compiler and PyTorch v1.10.2 and before,
                        you need to add an additional parameter, <code class="code">"training_script"</code>, to
                        specify your training script, as shown in the preceding code example.</p></div></div>
            </li><li class="listitem">
                <p><code class="code">compiler_config</code> (TrainingCompilerConfig object) – Required
                    to activate SageMaker Training Compiler. Include this parameter to turn on SageMaker Training Compiler. The following
                    are parameters for the <code class="code">TrainingCompilerConfig</code> class.</p>
                <div class="itemizedlist">
                     
                     
                <ul class="itemizedlist"><li class="listitem">
                        <p><code class="code">enabled</code> (bool) – Optional. Specify
                                <code class="code">True</code> or <code class="code">False</code> to turn on or turn off
                            SageMaker Training Compiler. The default value is <code class="code">True</code>.</p>
                    </li><li class="listitem">
                        <p><code class="code">debug</code> (bool) – Optional. To receive more detailed
                            training logs from your compiler-accelerated training jobs, change it to
                                <code class="code">True</code>. However, the additional logging might add
                            overhead and slow down the compiled training job. The default value is
                                <code class="code">False</code>.</p>
                    </li></ul></div>
            </li><li class="listitem">
                <p><code class="code">distribution</code> (dict) – Optional. To run a distributed
                    training job with SageMaker Training Compiler, add <code class="code">distribution = <span>{</span> 'pytorchxla' : <span>{</span>
                        'enabled': True }}</code>.</p>
            </li></ul></div>
        <div class="awsdocs-note awsdocs-warning"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Warning</h6></div><div class="awsdocs-note-text"><p>If you turn on SageMaker Debugger, it might impact the performance of SageMaker Training Compiler. We
                recommend that you turn off Debugger when running SageMaker Training Compiler to make sure there's no
                impact on performance. For more information, see <a href="training-compiler-tips-pitfalls.html#training-compiler-tips-pitfalls-considerations">Considerations</a>. To turn the Debugger
                functionalities off, add the following two arguments to the estimator:</p><pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="py ">disable_profiler=True,
debugger_hook_config=False</code></pre></div></div>
        <p>If the training job with the compiler is launched successfully, you receive the
            following logs during the job initialization phase: </p>
        <div class="itemizedlist">
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p>With <code class="code">TrainingCompilerConfig(debug=False)</code></p>
                <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="txt ">Found configuration for Training Compiler
Configuring SM Training Compiler...</code></pre>
            </li><li class="listitem">
                <p>With <code class="code">TrainingCompilerConfig(debug=True)</code></p>
                <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="txt ">Found configuration for Training Compiler
Configuring SM Training Compiler...
Training Compiler set to debug mode</code></pre>
            </li></ul></div>
     
        <h2 id="training-compiler-enable-pytorch-api">Using the SageMaker
                    <code class="code">CreateTrainingJob</code> API Operation</h2>
        <p>SageMaker Training Compiler configuration options must be specified through the
                <code class="code">AlgorithmSpecification</code> and <code class="code">HyperParameters</code> field in the
            request syntax for the <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html"><code class="code">CreateTrainingJob</code> API operation</a>.</p>
        <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="json ">"AlgorithmSpecification": <span>{</span>
    "TrainingImage": "<code class="replaceable">&lt;sagemaker-training-compiler-enabled-dlc-image&gt;</code>"
},

"HyperParameters": <span>{</span>
    "sagemaker_training_compiler_enabled": "true",
    "sagemaker_training_compiler_debug_mode": "false",
    "sagemaker_pytorch_xla_multi_worker_enabled": "false"    // set to "true" for distributed training
}</code></pre>
        <p>To find a complete list of deep learning container image URIs that have SageMaker Training Compiler
            implemented, see <a href="training-compiler-support.html#training-compiler-supported-frameworks">Supported
                    Frameworks</a>.</p>
    <awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="https://docs.aws.amazon.com/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./training-compiler-enable.html">Enable Training Compiler</div><div id="next" class="next-link" accesskey="n" href="./training-compiler-enable-tensorflow.html">Run TensorFlow Training Jobs with Training Compiler</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/training-compiler-enable-pytorch.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/training-compiler-enable-pytorch.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>