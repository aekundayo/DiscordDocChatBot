<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>How Hyperparameter Tuning Works - Amazon SageMaker</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="automatic-model-tuning-how-it-works" /><meta name="default_state" content="automatic-model-tuning-how-it-works" /><link rel="icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="canonical" href="automatic-model-tuning-how-it-works.html" /><meta name="description" content="Amazon SageMaker hyperparameter tuning uses either a Bayesian or a random search strategy to find the best values for hyperparameters." /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon SageMaker" /><meta name="guide" content="Developer Guide" /><meta name="abstract" content="Use Internet Monitor to build, train, and host machine learning models in AWS." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="automatic-model-tuning-how-it-works.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="de" /><link rel="alternative" href="automatic-model-tuning-how-it-works.html" hreflang="en-us" /><link rel="alternative" href="automatic-model-tuning-how-it-works.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" hreflang="zh-tw" /><link rel="alternative" href="automatic-model-tuning-how-it-works.html" hreflang="x-default" /><meta name="feedback-item" content="SageMaker" /><meta name="this_doc_product" content="Amazon SageMaker" /><meta name="this_doc_guide" content="Developer Guide" /><script defer="" src="../../../assets/r/vendor4.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor3.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor1.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-common.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-doc-page.js@version=2021.12.02"></script><link href="../../../assets/r/vendor4.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-common.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-doc-page.css@version=2021.12.02.css" rel="stylesheet" /><script async="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'sagemaker'}"></script><meta id="panorama-serviceSubSection" value="Developer Guide" /><meta id="panorama-serviceConsolePage" value="How Hyperparameter Tuning Works" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>How Hyperparameter Tuning Works - Amazon SageMaker</title><meta name="pdf" content="/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#automatic-model-tuning-how-it-works" /><meta name="rss" content="amazon-sagemaker-release-notes.rss" /><meta name="forums" content="http://forums.aws.amazon.com/forum.jspa?forumID=285" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html" /><meta name="keywords" content="SageMaker,Amazon SageMaker,machine learning,notebook instance,automatic model tuning,hyperparameter optimization,how hyperparameter tuning works,introduction hyperparameter optimization" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "Developer Guide",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Train machine learning models",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/train-model.html"
      },
      {
        "@type" : "ListItem",
        "position" : 5,
        "name" : "Perform Automatic Model Tuning with SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html"
      },
      {
        "@type" : "ListItem",
        "position" : 6,
        "name" : "How Hyperparameter Tuning Works",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#automatic-model-tuning-how-it-works" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="https://aws.amazon.com">AWS</a><a href="https://docs.aws.amazon.com/index.html">Documentation</a><a href="https://docs.aws.amazon.com/sagemaker/index.html">Amazon SageMaker</a><a href="whatis.html">Developer Guide</a></div><div id="page-toc-src"><a href="automatic-model-tuning-how-it-works.html#automatic-tuning-grid-search">Grid Search</a><a href="automatic-model-tuning-how-it-works.html#automatic-tuning-random-search">Random Search</a><a href="automatic-model-tuning-how-it-works.html#automatic-tuning-bayesian-optimization.title">Bayesian Optimization</a><a href="automatic-model-tuning-how-it-works.html#automatic-tuning-hyperband">Hyperband</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="automatic-model-tuning-how-it-works">How Hyperparameter Tuning Works</h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>When you build complex machine learning systems like deep learning neural networks,
      exploring all of the possible combinations is impractical. Hyperparameter tuning can
      accelerate your productivity by trying many variations of a model. It looks for the best model
      automatically by focusing on the most promising combinations of hyperparameter values within
      the ranges that you specify. To get good results, you must choose the right ranges to explore. </p><p>Use the <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/Welcome.html?icmpid=docs_sagemaker_lp">API reference
        guide</a> to understand how to interact with hyperparameter tuning. The examples on this
      page can be found in the <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_HyperParameterTuningJobConfig.html">HyperParameterTuningJobConfig</a> and <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_HyperbandStrategyConfig.html">HyperbandStrategyConfig</a> APIs.</p><div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Because the algorithm itself is stochastic, itâ€™s possible that the hyperparameter tuning
        model will fail to converge on the best answer. This can occur even if the best possible
        combination of values is within the ranges that you choose.</p></div></div>
      <h2 id="automatic-tuning-grid-search">Grid Search</h2>
      <p> When using grid search, hyperparameter tuning chooses combinations of values from the
        range of categorical values that you specify when you create the job. Only categorical
        parameters are supported when using the grid search strategy. You do not need to specify the
          <code class="code">MaxNumberOfTrainingJobs</code>. The number of training jobs created by the tuning
        job will be automatically calculated to be the total number of distinct categorical
        combinations possible. If specified, the value of <code class="code">MaxNumberOfTrainingJobs</code>
        should equal the total number of distinct categorical combinations possible.</p>
     
      <h2 id="automatic-tuning-random-search">Random Search</h2>
      <p>When using random search, hyperparameter tuning chooses a random combination of values
        from within the ranges that you specify for hyperparameters for each training job it
        launches. Because the choice of hyperparameter values doesn't depend on the results of
        previous training jobs, you can run the maximum number of concurrent training jobs without
        affecting the performance of the tuning.</p>
      <p>For an example notebook that uses random search, see the <a href="https://github.com/aws/amazon-sagemaker-examples-community/blob/215215eb25b40eadaf126d055dbb718a245d7603/training/sagemaker-automatic-model-tuning/hpo_xgboost_random_log.ipynb" rel="noopener noreferrer" target="_blank"><span> Random search and hyperparameter scaling with SageMaker XGBoost and Automatic Model
          Tuning</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> notebook.</p>
     
      <h2 id="automatic-tuning-bayesian-optimization.title">Bayesian Optimization</h2>
      <p>Bayesian optimization treats hyperparameter tuning like a <em><a href="https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html#[regression]">regression</a></em> problem. Given a
        set of input features (the hyperparameters), hyperparameter tuning optimizes a model for the
        metric that you choose. To solve a regression problem, hyperparameter tuning makes guesses
        about which hyperparameter combinations are likely to get the best results, and runs
        training jobs to test these values. After testing a set of hyperparameter values,
        hyperparameter tuning uses regression to choose the next set of hyperparameter values to
        test.</p>
      <p>Hyperparameter tuning uses an Amazon SageMaker implementation of Bayesian optimization.</p>
      <p>When choosing the best hyperparameters for the next training job, hyperparameter tuning
        considers everything that it knows about this problem so far. Sometimes it chooses a
        combination of hyperparameter values close to the combination that resulted in the best
        previous training job to incrementally improve performance. This allows hyperparameter
        tuning to exploit the best known results. Other times, it chooses a set of hyperparameter
        values far removed from those it has tried. This allows it to explore the range of
        hyperparameter values to try to find new areas that are not yet well understood. The
        explore/exploit trade-off is common in many machine learning problems.</p>
      <p>For more information about Bayesian optimization, see the following:</p>
      <div class="itemizedlist">
        <h6>Basic Topics on Bayesian Optimization</h6>
         
         
         
      <ul class="itemizedlist"><li class="listitem">
          <p><a href="https://arxiv.org/abs/1012.2599" rel="noopener noreferrer" target="_blank"><span>A Tutorial on Bayesian Optimization of
              Expensive Cost Functions, with Application to Active User Modeling and Hierarchical
              Reinforcement Learning</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
        </li><li class="listitem">
          <p><a href="https://arxiv.org/abs/1206.2944" rel="noopener noreferrer" target="_blank"><span>Practical Bayesian Optimization of
              Machine Learning Algorithms</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
        </li><li class="listitem">
          <p><a href="http://ieeexplore.ieee.org/document/7352306/?reload=true" rel="noopener noreferrer" target="_blank"><span>Taking the
              Human Out of the Loop: A Review of Bayesian Optimization</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
        </li></ul></div>
      <div class="itemizedlist">
        <h6>Speeding up Bayesian Optimization</h6>
         
         
         
      <ul class="itemizedlist"><li class="listitem">
          <p><a href="https://dl.acm.org/citation.cfm?id=3098043" rel="noopener noreferrer" target="_blank"><span>Google Vizier: A Service for
              Black-Box Optimization</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
        </li><li class="listitem">
          <p><a href="https://openreview.net/forum?id=S11KBYclx" rel="noopener noreferrer" target="_blank"><span>Learning Curve Prediction
              with Bayesian Neural Networks</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
        </li><li class="listitem">
          <p><a href="https://dl.acm.org/citation.cfm?id=2832731" rel="noopener noreferrer" target="_blank"><span>Speeding up automatic
              hyperparameter optimization of deep neural networks by extrapolation of learning
              curves</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
        </li></ul></div>
      <div class="itemizedlist">
        <h6>Advanced Modeling and Transfer Learning</h6>
         
         
         
         
         
      <ul class="itemizedlist"><li class="listitem">
          <p><a href="https://papers.nips.cc/paper/7917-scalable-hyperparameter-transfer-learning" rel="noopener noreferrer" target="_blank"><span>Scalable Hyperparameter Transfer Learning</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
        </li><li class="listitem">
          <p><a href="http://proceedings.mlr.press/v70/jenatton17a.html" rel="noopener noreferrer" target="_blank"><span>Bayesian Optimization
              with Tree-structured Dependencies</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
        </li><li class="listitem">
          <p><a href="https://papers.nips.cc/paper/6116-bayesian-optimization-with-robust-bayesian-neural-networks" rel="noopener noreferrer" target="_blank"><span>Bayesian Optimization with Robust Bayesian Neural Networks</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
        </li><li class="listitem">
          <p><a href="http://proceedings.mlr.press/v37/snoek15.pdf" rel="noopener noreferrer" target="_blank"><span>Scalable Bayesian
              Optimization Using Deep Neural Networks</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
        </li><li class="listitem">
          <p><a href=" https://arxiv.org/abs/1402.0929" rel="noopener noreferrer" target="_blank"><span>Input Warping for Bayesian
              Optimization of Non-stationary Functions</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
        </li></ul></div>
     
      <h2 id="automatic-tuning-hyperband">Hyperband</h2>
      <p>Hyperband is a multi-fidelity based tuning strategy that dynamically reallocates
        resources. Hyperband uses both intermediate and final results of training jobs to
        re-allocate epochs to well-utilized hyperparameter configurations and automatically stops
        those that underperform. It also seamlessly scales to using many parallel training jobs.
        These features can significantly speed up hyperparameter tuning over random search and
        Bayesian optimization strategies.</p>
      <p>Hyperband should only be used to tune iterative algorithms that publish results at
        different resource levels. For example, Hyperband can be used to tune a neural network for
        image classification which publishes accuracy metrics after every epoch.</p>
      <p>For more information about Hyperband, see the following links:</p>
      <div class="itemizedlist">
         
         
         
         
      <ul class="itemizedlist"><li class="listitem">
          <p><a href="https://arxiv.org/pdf/1603.06560.pdf" rel="noopener noreferrer" target="_blank"><span>Hyperband: A Novel Bandit-Based
              Approach to Hyperparameter Optimization</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
        </li><li class="listitem">
          <p><a href="https://liamcli.com/assets/pdf/asha_arxiv.pdf" rel="noopener noreferrer" target="_blank"><span>Massively Parallel
              Hyperparameter Tuning</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
        </li><li class="listitem">
          <p><a href="http://proceedings.mlr.press/v80/falkner18a/falkner18a.pdf" rel="noopener noreferrer" target="_blank"><span>BOHB: Robust
              and Efficient Hyperparameter Optimization at Scale</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
        </li><li class="listitem">
          <p><a href="https://openreview.net/pdf?id=a2rFihIU7i" rel="noopener noreferrer" target="_blank"><span>Model-based Asynchronous
              Hyperparameter and Neural Architecture Search</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
        </li></ul></div>
       
        <h3 id="automatic-tuning-hyperband-early-stopping">Hyperband with early
            stopping</h3>
        <p>Training jobs can be stopped early when they are unlikely to improve the objective
          metric of the hyperparameter tuning job. This can help reduce compute time and avoid
          overfitting your model. Hyperband uses an advanced internal mechanism to apply early
          stopping. Thus, the parameter <code class="code">TrainingJobEarlyStoppingType</code> in the
            <code class="code">HyperParameterTuningJobConfig</code> API must be set to <code class="code">OFF</code> when
          using the Hyperband internal early stopping feature.</p>
       
    <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Hyperparameter tuning might not improve your model. It is an advanced tool for building
        machine solutions. As such, it should be considered part of the scientific development
        process. </p></div></div><awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="https://docs.aws.amazon.com/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./automatic-model-tuning.html">Perform Automatic Model Tuning</div><div id="next" class="next-link" accesskey="n" href="./automatic-model-tuning-define-metrics-variables.html">Define metrics and
        environment variables</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>