<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Use Amazon SageMaker Elastic Inference (EI) - Amazon SageMaker</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="ei" /><meta name="default_state" content="ei" /><link rel="icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="canonical" href="ei.html" /><meta name="description" content="Use Amazon Elastic Inference (EI) to speed up the throughput and decrease latency for inference in Amazon SageMaker" /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon SageMaker" /><meta name="guide" content="Developer Guide" /><meta name="abstract" content="Use Internet Monitor to build, train, and host machine learning models in AWS." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="ei.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/ei.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/ei.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/ei.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/ei.html" hreflang="de" /><link rel="alternative" href="ei.html" hreflang="en-us" /><link rel="alternative" href="ei.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/ei.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/ei.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/ei.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/ei.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/ei.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/ei.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/ei.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/ei.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/ei.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/ei.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/ei.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/ei.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/ei.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/ei.html" hreflang="zh-tw" /><link rel="alternative" href="ei.html" hreflang="x-default" /><meta name="feedback-item" content="SageMaker" /><meta name="this_doc_product" content="Amazon SageMaker" /><meta name="this_doc_guide" content="Developer Guide" /><script defer="" src="../../../assets/r/vendor4.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor3.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor1.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-common.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-doc-page.js@version=2021.12.02"></script><link href="../../../assets/r/vendor4.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-common.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-doc-page.css@version=2021.12.02.css" rel="stylesheet" /><script async="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'sagemaker'}"></script><meta id="panorama-serviceSubSection" value="Developer Guide" /><meta id="panorama-serviceConsolePage" value="Use Amazon SageMaker Elastic Inference (EI)" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>Use Amazon SageMaker Elastic Inference (EI) - Amazon SageMaker</title><meta name="pdf" content="/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#ei" /><meta name="rss" content="amazon-sagemaker-release-notes.rss" /><meta name="forums" content="http://forums.aws.amazon.com/forum.jspa?forumID=285" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/ei.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/ei.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/ei.html" /><meta name="keywords" content="SageMaker,Amazon SageMaker,machine learning,notebook instance,inference,deploy model,endpoint,prediction,ML application,serverless machine learning,multi model deployment,inference pipelines,ML model" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "Developer Guide",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Deploy models for inference",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html"
      },
      {
        "@type" : "ListItem",
        "position" : 5,
        "name" : "Use Amazon SageMaker Elastic Inference (EI)",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#ei" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="https://aws.amazon.com">AWS</a><a href="https://docs.aws.amazon.com/index.html">Documentation</a><a href="https://docs.aws.amazon.com/sagemaker/index.html">Amazon SageMaker</a><a href="whatis.html">Developer Guide</a></div><div id="page-toc-src"><a href="ei.html#ei-migration">Migrate from Amazon Elastic Inference to other instances</a><a href="ei.html#ei-choose-type">Choose an EI Accelerator Type</a><a href="ei.html#ei-intro-notebook">Use EI in a SageMaker Notebook Instance</a><a href="ei.html#ei-intro-endpoint">Use EI on a Hosted Endpoint</a><a href="ei.html#ei-supported-frameworks">Frameworks that Support EI</a><a href="ei.html#ei-built-in">Use EI with SageMaker Built-in Algorithms</a><a href="ei.html#ei-intro-sample-nb">EI Sample Notebooks</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="ei">Use Amazon SageMaker Elastic Inference (EI) </h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>Starting April 15, 2023, AWS will not onboard new customers to Amazon Elastic Inference (EI), and will help current customers
        migrate their workloads to options that offer better price and performance. After April 15, 2023, new customers will not
        be able to launch instances with Amazon EI accelerators in Amazon SageMaker, Amazon ECS, or Amazon EC2. However, customers who have used
        Amazon EI at least once during the past 30-day period are considered current customers and will be able to continue using the service.</p><p>Machine learning (ML) on AWS helps you innovate faster with the most comprehensive set of ML services and infrastructure made available in a low-cost,
        pay as-you-go usage model. AWS continuously delivers better performing and lower cost infrastructure for ML inference workloads.
        AWS launched Amazon Elastic Inference (EI) in 2018 to enable customers to attach low-cost GPU-powered acceleration to Amazon EC2, Amazon SageMaker instances, or Amazon Elastic Container Service (ECS) tasks
        to reduce the cost of running deep learning inference by up to 75% compared to standalone GPU based instances such as Amazon EC2 P4d and Amazon EC2 G5.
        In 2019, AWS launched AWS Inferentia, Amazon's first custom silicon designed to accelerate deep learning workloads by providing high performance
        inference in the cloud. Amazon EC2 Inf1 instances based on AWS Inferentia chips deliver up 2.3x higher throughput and up to 70% lower cost per inference
        than comparable current generation GPU-based Amazon EC2 instances. With the availability of new accelerated compute options such as AWS Inferentia
        and Amazon EC2 G5 instances, the benefit of attaching a fractional GPU to a CPU host instance using Amazon EI has diminished. For example, customers
        hosting models on Amazon EI who move to <code class="code">ml.inf1.xlarge</code> instances can get up to 56% in cost savings and 2x performance improvement.</p><p>Customers can use Amazon SageMaker Inference Recommender to help them choose the best alternative instances to Amazon EI for deploying their ML models.</p><p><b>Frequently asked questions</b></p><div class="orderedlist">
         
         
         
         
         
         
         
         
    <ol><li>
            <p><b>Why is Amazon encouraging customers to move workloads from Amazon Elastic Inference (EI) to newer hardware acceleration
                options such as AWS Inferentia?</b></p>
            <p>Customers get better performance at a much better price than Amazon EI with new hardware accelerator options such as
                <a href="http://aws.amazon.com/machine-learning/inferentia/" rel="noopener noreferrer" target="_blank"><span>AWS Inferentia</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>
                for their inference workloads. AWS Inferentia is designed to provide high performance inference in the cloud, to drive down the total cost
                of inference, and to make it easy for developers to integrate machine learning into their business applications. To enable customers to benefit
                from such newer generation hardware accelerators, we will not onboard new customers to Amazon EI after April 15, 2023.</p>
        </li><li>
            <p><b>Which AWS services are impacted by the move to stop onboarding new customers to Amazon Elastic Inference (EI)?</b></p>
            <p>This announcement will affect Amazon EI accelerators attached to any Amazon EC2, Amazon SageMaker instances, or Amazon Elastic Container Service (ECS) tasks.
                In Amazon SageMaker, this applies to both endpoints and notebook kernels using Amazon EI accelerators.</p>
        </li><li>
            <p><b>Will I be able to create a new Amazon Elastic Inference (EI) accelerator after April 15, 2023?</b></p>
            <p>No, if you are a new customer and have not used Amazon EI in the past 30 days, then you will not be able create a new Amazon EI instance
                in your AWS account after April 15, 2023. However, if you have used an Amazon EI accelerator at least once in the past 30 days, you can attach
                a new Amazon EI accelerator to your instance.</p>
        </li><li>
            <p><b>We currently use Amazon Elastic Inference (EI) accelerators. Will we be able to continue using them after April 15, 2023?</b></p>
            <p>Yes, you will be able use Amazon EI accelerators. We recommend that you migrate your current ML Inference workloads running on Amazon EI
                to other hardware accelerator options at your earliest convenience. </p>
        </li><li>
            <p><b>How do I evaluate alternative instance options for my current Amazon SageMaker Inference Endpoints?</b></p>
            <p><a href="inference-recommender.html">Amazon SageMaker Inference Recommender</a> can help you identify cost-effective
                deployments to migrate existing workloads from Amazon Elastic Inference (EI) to an appropriate ML instance supported by SageMaker.</p>
        </li><li>
            <p><b>How do I change the instance type for my existing endpoint in Amazon SageMaker?</b></p>
            <p>You can change the instance type for your existing endpoint by doing the following:</p>
            <div class="procedure"><ol><li><p>First, <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateEndpointConfig.html">create a new EndpointConfig</a>
                    that uses the new instance type. If you have an autoscaling policy,
                    <a href="endpoint-auto-scaling-delete.html">delete the existing autoscaling policy</a>.</p></li><li><p>Call <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_UpdateEndpoint.html">UpdateEndpoint</a>
                    while specifying your newly created EndpointConfig.</p></li><li><p>Wait for your endpoint to change status to <code class="code">InService</code>. This will take approximately 10-15 minutes.</p></li><li><p>Finally, if you need autoscaling for your new endpoint, create a new autoscaling policy for this new endpoint and ProductionVariant.</p></li></ol></div>
        </li><li>
            <p><b>How do I change the instance type for my existing
                <a href="nbi.html">Amazon SageMaker Notebook Instance</a> using Amazon Elastic Inference (EI)?</b></p>
            <p>Choose <b>Notebook instances</b> in the SageMaker console, and then choose the Notebook Instance you want to update.
                Make sure the Notebook Instance has a <code class="code">Stopped</code> status. Finally, you can choose <b>Edit</b> and change your instance type.
                Make sure that, when your Notebook Instance starts up, you select the right kernel for your new instance.</p>
        </li><li>
            <p><b>Is there a specific instance type which is a good alternative to Amazon Elastic Inference (EI)?</b></p>
            <p>Every machine learning workload is unique. We recommend using <a href="inference-recommender.html">Amazon SageMaker Inference Recommender</a>
                to help you identify the right instance type for your ML workload, performance requirements, and budget. <a href="http://aws.amazon.com/machine-learning/inferentia/" rel="noopener noreferrer" target="_blank"><span>AWS Inferentia</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>,
                specifically <code class="code">inf1.xlarge</code>, is the best high performance and low-cost alternative for Amazon EI customers.</p>
        </li></ol></div>
        <h2 id="ei-migration">Migrate from Amazon Elastic Inference to other instances</h2>
        <p>The following information can help you migrate your SageMaker-hosted endpoints from instances that use
        Amazon Elastic Inference accelerators to other instances. The advice varies depending on your framework.</p>
        
         
            <h3 id="ei-migration-pytorch">PyTorch</h3>
            <p>If you're migrating from PyTorch, use the following guidelines.</p>
            <p><b>1. Choose the right instance type</b></p>
            <p>Every machine learning workload is unique. We recommend using Amazon SageMaker Inference Recommender to help you identify the right
                instance type for your ML workload, performance requirements, and budget. AWS Inferentia, specifically <code class="code">inf1.xlarge</code>,
                is the best high performance and low-cost alternative for Amazon Elastic Inference customers.</p>
            <p>In our load testing with Inference Recommender, <code class="code">g4dn.xlarge</code> instances performed
                better than <code class="code">m5.large</code> instances with <code class="code">eia.2large</code> attached.
                With Amazon Elastic Inference, you have to pay the additional cost of the ML
                instance to which the accelerator is attached. Amazon Elastic Inference also only
                supports PyTorch 1.5 and TensorFlow 2.3. If you migrate to <code class="code">ml.g4dn</code>
                instances, you can use the latest versions of PyTorch 1.11 and TensorFlow 2.9.
                Additionally, <code class="code">ml.g4dn</code> and AWS Inferentia are available in all AWS
                Regions, whereas Amazon Elastic Inference is only available in 6 Regions. Both AWS
                Inferentia and <code class="code">ml.g4dn</code> offer better performance at lower price for most
                ML inference workloads.</p>
            <p><b>2. Modify <code class="code">inference.py</code></b></p>
            <p>Modify your <code class="code">inference.py</code> file to remove any Elastic
                Inference-specific required changes and use default handlers. Based on different
                user cases, you might have different input and output handlers, but the main changes
                you must make are in the model loading handler functions <code class="code">model_fn</code> and
                    <code class="code">predict_fn</code>. Remove the Elastic Inference-specific predict handler
                    <code class="code">predict_fn</code> and restore the model loading handler
                    <code class="code">model_fn</code> to the default format. The following example shows how to
                do this, with the parts you should remove from <code class="code">inference.py</code> commented
                out:</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">from __future__ import print_function

import os

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

def model_fn(model_dir, context):
    model = <span>{</span>customer_model}
    # if torch.__version__ in VERSIONS_USE_NEW_API:
        # import torcheia
        # loaded_model = loaded_model.eval()
        # loaded_model = torcheia.jit.attach_eia(loaded_model, 0)
    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:
        model.load_state_dict(torch.load(f))
    return model
    

# def predict_fn(input_data, model):
#     logger.info(
#         "Performing EIA inference with Torch JIT context with input of size <span>{</span>}".format(
#             input_data.shape
#         )
#     )
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     input_data = input_data.to(device)
#     with torch.no_grad():
#        if torch.__version__ in VERSIONS_USE_NEW_API:
#             import torcheia
# 
#             torch._C._jit_set_profiling_executor(False)
#             with torch.jit.optimized_execution(True):
#                 return model.forward(input_data)
#         else:
#             with torch.jit.optimized_execution(True, <span>{</span>"target_device": "eia:0"}):
#                 return model(input_data)

def predict_fn(input_data, model):
    return model(input_data)</code></pre>
            <p><b>3. Create a model</b></p>
            <p>Create a new model that points to your modified <code class="code">inference.py</code> file.
                You can keep the <code class="code">inference.py</code> file locally and point to it by
                specifying <code class="code">source_dir</code> and <code class="code">entry_point</code> or tar the
                    <code class="code">inference.py</code> file into the model tarball. The following example
                shows the former case:</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">from sagemaker.pytorch import PyTorchModel

pytorch = PyTorchModel(
    model_data=<span>{</span>model_data_url},
    role=role,
    entry_point="inference.py",
    source_dir="code",
    framework_version="1.5.1",
    py_version="py3",
    sagemaker_session=sagemaker_session,
)</code></pre>
            <p><b>4. Deploy the model to the endpoint and invoke it</b></p>
            <p>You can use one of the following options for deploying your model after making the preceding changes.</p>
            <p><em>Option 1: Deploy from scratch</em></p>
            <p>You can deploy the model to a new endpoint with a recommended instance from the <b>Accelerated Computing</b> category, such as G4.</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">predictor = pytorch.deploy(
        ...
        # instance_type = "ml.c5.xlarge",
        instance_type="ml.g4dn.2xlarge",
        ...
response = predictor.predict(payload)</code></pre>
            <p><em>Option 2: Update the existing endpoint</em></p>
            <p>Complete the following steps to update your existing endpoint:</p>
            <div class="procedure"><ol><li>
                    <p>Call <code class="code">CreateEndpointConfig</code> to create a new <code class="code">EndpointConfig</code> that uses the new instance type. If you have
                    an autoscaling policy, delete the existing autoscaling policy.</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">endpoint_config_response = sagemaker_client.create_endpoint_config(
    EndpointConfigName=endpoint_config_name,
    ProductionVariants=[
        <span>{</span>
            "VariantName": "variant1", # The name of the production variant.
            "ModelName": model_name, # The name of new created model 
            "InstanceType": instance_type, # Specify the right-sized instance type.
            "InitialInstanceCount": 1 # Number of instances to launch initially.
        }
    ]
)</code></pre>
                </li><li>
                    <p>Call <code class="code">UpdateEndpoint</code> and specify your newly created <code class="code">EndpointConfig</code>.</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">endpoint_config_response = sagemaker_client.update_endpoint(
    EndpointConfigName=endpoint_config_name, # The name of the new endpoint config just created
    EndpointName=endpoint_name # The name of the existing endpoint you want to update
 )</code></pre>
                </li><li><p>Wait for your endpoint to change status to <code class="code">InService</code>. This takes approximately
                        10–15 minutes.</p></li><li><p>Finally, if you need autoscaling for your new endpoint, create a new autoscaling policy for your new endpoint and <code class="code">ProductionVariant</code>.</p></li></ol></div>
         
        
         
            <h3 id="ei-migration-tensorflow">TensorFlow</h3>
            <p>If you're migrating from TensorFlow, use the following guidelines.</p>
            <p><b>1. Choose the right instance type</b></p>
            <p>Refer to the <b>1. Choose the right instance type</b> guidance in the <a href="ei.html#ei-migration-pytorch">PyTorch section</a>.</p>
            <p><b>2. Deploy the model to the endpoint and invoke it</b></p>
            <p>You can use one of the following options for deploying your model.</p>
            <p><em>Option 1: Deploy from scratch</em></p>
            <p>You can migrate from Elastic Inference by re-deploying the model to a new endpoint
                by removing the <code class="code">accelerator_type</code> field and specifying a right-sized
                instance type from the <b>Accelerated Computing</b>
                category, such as G4. In the following example, the commented out line causes you to
                deploy without using an Elastic Inference accelerator.</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">predictor = tensorflow_model.deploy(
    ...
    instance_type="ml.g4dn.2xlarge"
    # instance_type="ml.c5.xlarge",
    # accelerator_type="ml.eia1.medium"
    ...
)</code></pre>
            <p><em>Option 2: Update the existing endpoint</em></p>
            <p>Refer to the <em>Option 2. Update the existing
                    endpoint</em> guidance in Step 4 of the <a href="ei.html#ei-migration-pytorch">PyTorch section</a>.</p>
         
        
         
            <h3 id="ei-migration-mxnet">MXNet</h3>
            <p>If you're migrating from MXNet, use the following guidelines.</p>
            
            <p><b>1. Choose the right instance type</b></p>
            <p>Refer to the <b>1. Choose the right instance type</b> guidance in the <a href="ei.html#ei-migration-pytorch">PyTorch section</a>.</p>
            
            <p><b>2. Deploy the model to the endpoint and invoke it</b></p>
            <p>You can use one of the following options for deploying your model.</p>
            <p><em>Option 1: Deploy from scratch</em></p>
            <p>You can migrate from Elastic Inference by re-deploying the model to a new endpoint
                by removing the <code class="code">accelerator_type</code> field and specifying a right-sized
                instance type from the <b>Accelerated Computing</b>
                category, such as G4. In the following example, the commented out line causes you to
                deploy without using an Elastic Inference accelerator.</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">predictor = mxnet_model.deploy(
        ...
        # instance_type="ml.c5.xlarge",
        instance_type="ml.g4dn.2xlarge"
        ...
)</code></pre>
            <p><em>Option 2: Update the existing endpoint</em></p>
            <p>Refer to the <em>Option 2: Update the existing endpoint</em> guidance in Step 4 of the <a href="ei.html#ei-migration-pytorch">PyTorch section</a>.</p>
         
    <div class="highlights"><h6>Topics</h6><ul><li><a href="ei-how-it-works.html">How EI Works</a></li><li><a href="ei.html#ei-choose-type">Choose an EI Accelerator Type</a></li><li><a href="ei.html#ei-intro-notebook">Use EI in a SageMaker Notebook Instance</a></li><li><a href="ei.html#ei-intro-endpoint">Use EI on a Hosted Endpoint</a></li><li><a href="ei.html#ei-supported-frameworks">Frameworks that Support EI</a></li><li><a href="ei.html#ei-built-in">Use EI with SageMaker Built-in Algorithms</a></li><li><a href="ei.html#ei-intro-sample-nb">EI Sample Notebooks</a></li><li><a href="ei-setup.html">Set Up to Use EI</a></li><li><a href="ei-notebook-instance.html">Attach EI to a Notebook Instance</a></li><li><a href="ei-endpoints.html">Use EI on Amazon SageMaker Hosted Endpoints</a></li></ul></div>
        <h2 id="ei-choose-type">Choose an EI Accelerator Type</h2>
        <p>Consider the following factors when choosing an accelerator type for a hosted
            model:</p>
        <div class="itemizedlist">
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p>Models, input tensors and batch sizes influence the amount of accelerator
                    memory you need. Start with an accelerator type that provides at least as much
                    memory as the file size of your trained model. Factor in that a model might use
                    significantly more memory than the file size at runtime.</p>
            </li><li class="listitem">
                <p>Demands on CPU compute resources, main system memory, and GPU-based
                    acceleration and accelerator memory vary significantly between different kinds
                    of deep learning models. The latency and throughput requirements of the
                    application also determine the amount of compute and acceleration you need.
                    Thoroughly test different configurations of instance types and EI accelerator
                    sizes to make sure you choose the configuration that best fits the performance
                    needs of your application.</p>
            </li></ul></div>
        <p>For more information on selecting an EI accelerator, see:</p>
        <div class="itemizedlist">
             
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p><a href="https://aws.amazon.com/machine-learning/elastic-inference/" rel="noopener noreferrer" target="_blank"><span>Amazon Elastic Inference Overview </span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
            </li><li class="listitem">
                <p><a href="https://docs.aws.amazon.com/elastic-inference/latest/developerguide/before.html#getting-started-choosing">Choosing an Instance and Accelerator Type for Your Model</a></p>
            </li><li class="listitem">
                <p><a href="https://aws.amazon.com/blogs/machine-learning/optimizing-costs-in-amazon-elastic-inference-with-amazon-tensorflow/" rel="noopener noreferrer" target="_blank"><span>Optimizing costs in Amazon Elastic Inference with TensorFlow</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
            </li></ul></div>
     
        <h2 id="ei-intro-notebook">Use EI in a SageMaker Notebook Instance</h2>
        <p>Typically, you build and test machine learning models in a SageMaker notebook before you
            deploy them for production. You can attach EI to your notebook instance when you create
            the notebook instance. You can set up an endpoint that is hosted locally on the notebook
            instance by using the local mode supported by TensorFlow, MXNet, and PyTorch estimators
            and models in the <a href="https://sagemaker.readthedocs.io" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker Python SDK</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> to test inference performance. Elastic Inference
            enabled PyTorch is not currently supported on notebook instances. For instructions on
            how to attach EI to a notebook instance and set up a local endpoint for inference, see
                <a href="ei-notebook-instance.html">Attach EI to a Notebook Instance</a>. There
            are also Elastic Inference-enabled SageMaker Notebook Jupyter kernels for Elastic
            Inference-enabled versions of TensorFlow and Apache MXNet. For information about using
            SageMaker notebook instances, see <a href="nbi.html">Use Amazon SageMaker
                Notebook Instances</a></p>
     
        <h2 id="ei-intro-endpoint">Use EI on a Hosted Endpoint</h2>
        <p>When you are ready to deploy your model for production to provide inferences, you
            create a SageMaker hosted endpoint. You can attach EI to the instance where your endpoint is
            hosted to increase its performance at providing inferences. For instructions on how to
            attach EI to a hosted endpoint instance, see <a href="ei-endpoints.html">Use EI on Amazon SageMaker Hosted Endpoints</a>.</p>
     
        <h2 id="ei-supported-frameworks">Frameworks that Support EI</h2>
        <p>Amazon Elastic Inference is designed to be used with AWS enhanced versions of TensorFlow, Apache MXNet, or
            PyTorch machine learning frameworks. These enhanced versions of the frameworks are
            automatically built into containers when you use the Amazon SageMaker Python SDK, or you
            can download them as binary files and import them in your own Docker containers. </p>

        <p>You can download the EI-enabled TensorFlow binary files from the public
            <a href="https://console.aws.amazon.com/s3/buckets/amazonei-tensorflow" rel="noopener noreferrer" target="_blank"><span>amazonei-tensorflow</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> Amazon S3 bucket to the TensorFlow serving containers.
            For more information about building a container that uses the EI-enabled version of TensorFlow,
            see <a href="https://github.com/aws/sagemaker-tensorflow-serving-container#sagemaker-tensorflow-serving-container" rel="noopener noreferrer" target="_blank"><span>
                Amazon Elastic Inference with TensorFlow in SageMaker</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>

        <p>You can download the EI-enabled MXNet binary files from the public
            <a href="https://console.aws.amazon.com/s3/buckets/amazonei-apachemxnet" rel="noopener noreferrer" target="_blank"><span>amazonei-apachemxnet</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> Amazon S3 bucket to the MXNet serving containers.
            For more information about building a container that uses the EI-enabled version of MXNet, see
                <a href="https://github.com/aws/sagemaker-mxnet-serving-container#amazon-elastic-inference-with-mxnet-in-sagemaker" rel="noopener noreferrer" target="_blank"><span>
                    Amazon Elastic Inference with MXNet in SageMaker</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>

        <p>You can download the <a href="https://amazonei-pytorcheia.s3.amazonaws.com/releases/v1.0.0/torcheia-1.0.0-cp36-cp36m-manylinux1_x86_64.whl" rel="noopener noreferrer" target="_blank"><span>Elastic Inference enabled binary for PyTorch</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.
            For more information about building a container that uses the EI-enabled version of PyTorch, see
                <a href="https://github.com/aws/sagemaker-pytorch-serving-container/#amazon-elastic-inference-with-pytorch-in-sagemaker" rel="noopener noreferrer" target="_blank"><span>
                    Amazon Elastic Inference with PyTorch in SageMaker</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>

        <p>To use Elastic Inference in a hosted endpoint, you can choose any of the following frameworks depending on your
            needs.</p>
        <div class="itemizedlist">
             
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p><a href="https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#deploy-tensorflow-serving-models" rel="noopener noreferrer" target="_blank"><span>
                    SageMaker Python SDK - Deploy TensorFlow models
                </span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p></li><li class="listitem">
                <p><a href="https://sagemaker.readthedocs.io/en/stable/frameworks/mxnet/using_mxnet.html#deploy-mxnet-models" rel="noopener noreferrer" target="_blank"><span>
                    SageMaker Python SDK - Deploy MXNet models
                </span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p></li><li class="listitem">
                <p><a href="https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#deploy-pytorch-models" rel="noopener noreferrer" target="_blank"><span>
                    SageMaker Python SDK - Deploy PyTorch models
                </span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p></li></ul></div>
        <p>If you need to create a custom container for deploying your model that is
            complex and requires extensions to a framework that the SageMaker pre-built containers do not
            support, use <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/guide/ec2-example-managing-instances.html" rel="noopener noreferrer" target="_blank"><span>
                the low-level AWS SDK for Python (Boto 3)
            </span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
     
        <h2 id="ei-built-in">Use EI with SageMaker Built-in Algorithms</h2>
        <p>Currently, the <a href="image-classification.html">Image Classification - MXNet</a> and <a href="object-detection.html">Object Detection - MXNet</a> built-in algorithms support EI. For an example
            that uses the Image Classification algorithm with EI, see <a href="https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-fulltraining.html" rel="noopener noreferrer" target="_blank"><span>End-to-End Multiclass Image Classification Example</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
     
        <h2 id="ei-intro-sample-nb">EI Sample Notebooks</h2>
        <p>The following Sample notebooks provide examples of using EI in SageMaker:</p>
        <div class="itemizedlist">
            
             

             
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-python-sdk/mxnet_mnist/mxnet_mnist_elastic_inference.html" rel="noopener noreferrer" target="_blank"><span>Using Amazon Elastic Inference with MXNet on Amazon SageMaker
                    </span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
            </li><li class="listitem">
                <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-python-sdk/mxnet_mnist/mxnet_mnist_elastic_inference_local.html" rel="noopener noreferrer" target="_blank"><span>Using Amazon Elastic Inference with MXNet on an Amazon SageMaker Notebook Instance
                    </span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>
                </p>
            </li><li class="listitem">
                <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-python-sdk/tensorflow_serving_using_elastic_inference_with_your_own_model/tensorflow_neo_compiled_model_elastic_inference.html" rel="noopener noreferrer" target="_blank"><span>Using Amazon Elastic Inference with Neo-compiled TensorFlow model on SageMaker</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
            </li><li class="listitem">
                <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-python-sdk/tensorflow_serving_using_elastic_inference_with_your_own_model/tensorflow_serving_pretrained_model_elastic_inference.html" rel="noopener noreferrer" target="_blank"><span>Using Amazon Elastic Inference with a pre-trained TensorFlow Serving model on SageMaker</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
            </li></ul></div>

    <awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="https://docs.aws.amazon.com/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./neo-troubleshooting-target-devices-ambarella.html">Troubleshoot
                Ambarella Errors</div><div id="next" class="next-link" accesskey="n" href="./ei-how-it-works.html">How EI Works</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/ei.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/ei.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>