<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Adapting Your Own Inference Container - Amazon SageMaker</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="adapt-inference-container" /><meta name="default_state" content="adapt-inference-container" /><link rel="icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="canonical" href="adapt-inference-container.html" /><meta name="description" content="Learn how to adapt your own Docker container to work with SageMaker hosting." /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon SageMaker" /><meta name="guide" content="Developer Guide" /><meta name="abstract" content="Use Internet Monitor to build, train, and host machine learning models in AWS." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="adapt-inference-container.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/adapt-inference-container.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/adapt-inference-container.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/adapt-inference-container.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/adapt-inference-container.html" hreflang="de" /><link rel="alternative" href="adapt-inference-container.html" hreflang="en-us" /><link rel="alternative" href="adapt-inference-container.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/adapt-inference-container.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/adapt-inference-container.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/adapt-inference-container.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/adapt-inference-container.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/adapt-inference-container.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/adapt-inference-container.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/adapt-inference-container.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/adapt-inference-container.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/adapt-inference-container.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/adapt-inference-container.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/adapt-inference-container.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/adapt-inference-container.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/adapt-inference-container.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/adapt-inference-container.html" hreflang="zh-tw" /><link rel="alternative" href="adapt-inference-container.html" hreflang="x-default" /><meta name="feedback-item" content="SageMaker" /><meta name="this_doc_product" content="Amazon SageMaker" /><meta name="this_doc_guide" content="Developer Guide" /><script defer="" src="../../../assets/r/vendor4.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor3.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor1.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-common.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-doc-page.js@version=2021.12.02"></script><link href="../../../assets/r/vendor4.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-common.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-doc-page.css@version=2021.12.02.css" rel="stylesheet" /><script async="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'sagemaker'}"></script><meta id="panorama-serviceSubSection" value="Developer Guide" /><meta id="panorama-serviceConsolePage" value="Adapting Your Own Inference Container" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>Adapting Your Own Inference Container - Amazon SageMaker</title><meta name="pdf" content="/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#adapt-inference-container" /><meta name="rss" content="amazon-sagemaker-release-notes.rss" /><meta name="forums" content="http://forums.aws.amazon.com/forum.jspa?forumID=285" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/adapt-inference-container.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/adapt-inference-container.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/adapt-inference-container.html" /><meta name="keywords" content="SageMaker,Amazon SageMaker,machine learning,notebook instance,Docker SageMaker hosting" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "Developer Guide",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Use Docker containers to build models",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers.html"
      },
      {
        "@type" : "ListItem",
        "position" : 5,
        "name" : "Adapting your own Docker container to work with SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers-adapt-your-own.html"
      },
      {
        "@type" : "ListItem",
        "position" : 6,
        "name" : "Adapting Your Own Inference Container",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers-adapt-your-own.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#adapt-inference-container" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="https://aws.amazon.com">AWS</a><a href="https://docs.aws.amazon.com/index.html">Documentation</a><a href="https://docs.aws.amazon.com/sagemaker/index.html">Amazon SageMaker</a><a href="whatis.html">Developer Guide</a></div><div id="page-toc-src"><a href="adapt-inference-container.html#byoc-inference">Using the SageMaker inference toolkit</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="adapt-inference-container">Adapting Your Own Inference
                    Container</h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>If none of the Amazon SageMaker pre-built inference containers suffice for your situation,
                you can build your own Docker container and use it inside SageMaker for training and
                inference. For more information and an example of how to build your own Docker
                container for training and inference with SageMaker, see <a href="https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb" rel="noopener noreferrer" target="_blank"><span>Building your own algorithm container</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. </p><p>You can also use the <a href="https://github.com/aws/sagemaker-inference-toolkit" rel="noopener noreferrer" target="_blank"><span>SageMaker Inference
                    Toolkit</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> to adapt your container to work with SageMaker hosting. The following
                guide shows you how to use the SageMaker Inference Toolkit to adapt your inference
                container.</p>
                <h2 id="byoc-inference">Using the SageMaker inference toolkit</h2>
                <p>To adapt your container to work with SageMaker hosting, create the inference code
                    in one or more Python script files and use a Dockerfile that imports the
                    inference toolkit.</p>
                <p>The inference code for the inference toolkit should include an inference
                    handler, a handler service, and an entrypoint. In this example, they are stored
                    as three separate Python files. All three of these Python files must be in the
                    same directory as your Dockerfile.</p>
                <p>For an example Jupyter notebook that shows a complete example of extending a
                    container by using the SageMaker inference toolkit, see <a href="https://sagemaker-examples.readthedocs.io/en/latest/advanced_functionality/multi_model_bring_your_own/multi_model_endpoint_bring_your_own.html" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker Multi-Model Endpoints using your own algorithm
                    container</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>

                 
                    <h3 id="byoc-inference-handler">Step 1: Create an Inference
                            Handler</h3>
                    <p>The SageMaker inference toolkit is built on the multi-model server (MMS). MMS
                        expects a Python script that implements the following functions:</p>
                    <div class="itemizedlist">
                         
                         
                         
                         
                    <ul class="itemizedlist"><li class="listitem">
                            <p>Load the model.</p>
                        </li><li class="listitem">
                            <p>Pre-process input data.</p>
                        </li><li class="listitem">
                            <p>Get predictions from the model.</p>
                        </li><li class="listitem">
                            <p>Process the output data in a model handler.</p>
                        </li></ul></div>
                    <p>Optionally, you can use the <code class="code">context</code> argument in the functions
                        you implement to access additional serving information, such as a GPU ID or
                        the batch size. If you are using multiple GPUs, you must specify the
                            <code class="code">context</code> argument in the <code class="code">predict_fn</code> function to
                        select a GPU for prediction. For more information about the
                            <code class="code">context</code> argument and how to use it, see the examples in
                            <a href="https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#id3" rel="noopener noreferrer" target="_blank"><span>The SageMaker PyTorch Model Server</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> in the <em>SageMaker Python SDK</em>.</p>
                     
                        <h4 id="byoc-inference-handler-modelfn">The model_fn
                                Function</h4>
                        <p>The <code class="code">model_fn</code> function loads your model. There are default
                            implementations for the <code class="code">model_fn</code> function, named
                                <code class="code">default_model_fn</code>, on the SageMaker PyTorch and MXNet
                            Inference toolkits. The default implementation loads models saved using
                            torchscript, of the form <code>.pt</code> or
                                <code>.pth</code>. If your model requires custom methods to
                            load, or you want to perform extra steps when loading your model, you
                            must implement the <code class="code">model_fn</code> function. The following simple
                            example shows an implementation of a <code class="code">model_fn</code> function that
                            loads a PyTorch model:</p>
                        <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">def model_fn(self, model_dir):
    import torch
    
    logger.info('model_fn')
    device = "cuda" if torch.cuda.is_available() else "cpu"
    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:
        model = torch.jit.load(f)
    return model.to(device)</code></pre>
                     
                     
                        <h4 id="byoc-inference-handler-inputfn">The input
                                Function</h4>
                        <p>The <code class="code">input_fn</code> pre-processes input data. Specifically, the
                                <code class="code">input_fn</code> function is responsible for deserializing your
                            input data so that it can be passed to your model. It takes input data
                            and content type as parameters, and returns deserialized data. The SageMaker
                            inference toolkit provides a default implementation that deserializes
                            the following content types:</p>
                        <div class="itemizedlist">
                             
                             
                             
                             
                        <ul class="itemizedlist"><li class="listitem">
                                <p>JSON</p>
                            </li><li class="listitem">
                                <p>CSV</p>
                            </li><li class="listitem">
                                <p>Numpy array</p>
                            </li><li class="listitem">
                                <p>NPZ</p>
                            </li></ul></div>
                        <p>If your model requires a different content type, or you want to
                            preprocess your input data before sending it to the model, you must
                            implement the <code class="code">input_fn</code> function. The following example
                            shows a simple implementation of the <code class="code">input_fn</code>
                            function.</p>
                        <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">from sagemaker_inference import content_types, decoder
def input_fn(self, input_data, content_type):
        """A default input_fn that can handle JSON, CSV and NPZ formats.
         
        Args:
            input_data: the request payload serialized in the content_type format
            content_type: the request content_type

        Returns: input_data deserialized into torch.FloatTensor or torch.cuda.FloatTensor depending if cuda is available.
        """
        return decoder.decode(input_data, content_type)</code></pre>
                     
                     
                        <h4 id="byoc-inference-handler-predictfn">The predict_fn
                                Function</h4>
                        <p>The <code class="code">predict_fn</code> function is responsible for getting
                            predictions from the model. It takes the model and the data returned
                            from <code class="code">input_fn</code> as parameters, and returns the prediction.
                            There is no default implementation for the <code class="code">predict_fn</code>. You
                            must implement it yourself. The following is a simple implementation of
                            the <code class="code">predict_fn</code> function for a PyTorch model.</p>
                        <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">def predict_fn(self, data, model):
        """A default predict_fn for PyTorch. Calls a model on data deserialized in input_fn.
        Runs prediction on GPU if cuda is available.

        Args:
            data: input data (torch.Tensor) for prediction deserialized by input_fn
            model: PyTorch model loaded in memory by model_fn

        Returns: a prediction
        """
        return model(data)</code></pre>
                     
                     
                        <h4 id="byoc-inference-handler-outputfn">The output_fn
                                Function</h4>
                        <p>The <code class="code">output_fn</code> function processes the output data in a
                            model handler. A model handler contains functions that are defined at
                            the model level. The <code class="code">output_fn</code> function is responsible for
                            serializing the data that the <code class="code">predict_fn</code> function returns
                            as a prediction. The SageMaker inference toolkit implements a default
                                <code class="code">output_fn</code> function that serializes Numpy arrays, JSON,
                            and CSV. If your model outputs any other content type, or you want to
                            perform other post-processing of your data before sending it to the
                            user, you must implement your own <code class="code">output_fn</code> function. The
                            following shows a simple <code class="code">output_fn</code> function for a PyTorch
                            model.</p>
                        <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">from sagemaker_inference import encoder
def output_fn(self, prediction, accept):
        """A default output_fn for PyTorch. Serializes predictions from predict_fn to JSON, CSV or NPY format.

        Args:
            prediction: a prediction result from predict_fn
            accept: type which the output data needs to be serialized

        Returns: output data serialized
        """
        return encoder.encode(prediction, accept)</code></pre>
                     
                 
                 
                    <h3 id="byoc-inference-handler-service">Step 2: Implement a Handler
                            Service</h3>
                    <p>The handler service implements <code class="code">initialize</code> and
                            <code class="code">handle</code> methods, and is run by the model server. The
                            <code class="code">initialize</code> method is invoked when the model server starts,
                        and the <code class="code">handle</code> method is invoked for all incoming inference
                        requests to the model server. For more information, see <a href="https://github.com/awslabs/multi-model-server/blob/master/docs/custom_service.md" rel="noopener noreferrer" target="_blank"><span>Custom Service</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> in the Multi-model server documentation. The
                        following is an example of a handler service for a PyTorch model
                        server.</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">from sagemaker_inference.default_handler_service import DefaultHandlerService
from sagemaker_inference.transformer import Transformer
from sagemaker_pytorch_serving_container.default_inference_handler import DefaultPytorchInferenceHandler


class HandlerService(DefaultHandlerService):
    """Handler service that is executed by the model server.
    Determines specific default inference handlers to use based on model being used.
    This class extends ``DefaultHandlerService``, which define the following:
        - The ``handle`` method is invoked for all incoming inference requests to the model server.
        - The ``initialize`` method is invoked at model server start up.
    Based on: https://github.com/awslabs/mxnet-model-server/blob/master/docs/custom_service.md
    """
    def __init__(self):
        transformer = Transformer(default_inference_handler=DefaultPytorchInferenceHandler())
        super(HandlerService, self).__init__(transformer=transformer)</code></pre>
                 
                 
                    <h3 id="byoc-inference-entrypoint">Step 3: Implement an
                            Entrypoint</h3>
                    <p>An entrypoint starts the model server by invoking the handler service.
                        Specify the location of the entrypoint in your Dockerfile. The following is
                        an example of an entrypoint.</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">from sagemaker_inference import model_server

model_server.start_model_server(handler_service=HANDLER_SERVICE)</code></pre>
                 
                 
                    <h3 id="byoc-inference-dockerfile">Step 4: Write a Dockerfile</h3>
                    <p>Write a Dockerfile. For a full example of a Dockerfile for an inference
                        container, see <a href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/multi_model_bring_your_own/container/Dockerfile" rel="noopener noreferrer" target="_blank"><span>Dockerfile</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. In your <code class="code">Dockerfile</code>, copy the model
                        handler from Step 2 and specify the Python file from the previous step as
                        the entrypoint in your <code class="code">Dockerfile</code>. The following is an example
                        of the lines you can add to your Dockerfile to copy the model handler and
                        specify the entrypoint.</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python "># Copy the default custom service file to handle incoming data and inference requests
COPY model_handler.py /home/model-server/model_handler.py

# Define an entrypoint script for the docker image
ENTRYPOINT ["python", "/usr/local/bin/entrypoint.py"]</code></pre>
                 
                 
                    <h3 id="byoc-inference-build-register">Step 5: Build and Register
                            Your Container</h3>
                    <p>Last, build your container and register it. The following shell script
                        from the sample notebook builds the container and uploads it to an Amazon ECR
                        repository in your AWS account.</p>
                    <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>SageMaker hosting supports using inference containers that are stored in
                            repositories other than Amazon ECR. For information, see <a href="your-algorithms-containers-inference-private.html">Use a Private Docker Registry
            for Real-Time Inference Containers</a>.</p></div></div>
                    <p>The following shell script shows how to build and register a
                        container.</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="sh ">%%sh

# The name of our algorithm
algorithm_name=demo-sagemaker-multimodel

cd container

account=$(aws sts get-caller-identity --query Account --output text)

# Get the region defined in the current configuration (default to us-west-2 if none defined)
region=$(aws configure get region)
region=$<span>{</span>region:-us-west-2}

fullname="$<span>{</span>account}.dkr.ecr.$<span>{</span>region}.amazonaws.com/$<span>{</span>algorithm_name}:latest"

# If the repository doesn't exist in ECR, create it.
aws ecr describe-repositories --repository-names "$<span>{</span>algorithm_name}" &gt; /dev/null 2&gt;&amp;1

if [ $? -ne 0 ]
then
    aws ecr create-repository --repository-name "$<span>{</span>algorithm_name}" &gt; /dev/null

# Get the login command from ECR and execute it directly
aws ecr get-login-password --region $<span>{</span>region}|docker login --username AWS --password-stdin $<span>{</span>fullname}

# Build the Docker image locally with the image name and then push it to ECR
# with the full name.

docker build -q -t $<span>{</span>algorithm_name} .
docker tag $<span>{</span>algorithm_name} $<span>{</span>fullname}

docker push $<span>{</span>fullname}</code></pre>
                    <p>You can now use this container to deploy endpoints in SageMaker.</p>
                 
                 
                    <h3 id="byoc-inference-build-info">Additional Information</h3>
                    <p>For an example of how to deploy an endpoint in SageMaker, see <a href="https://sagemaker-examples.readthedocs.io/en/latest/advanced_functionality/multi_model_bring_your_own/multi_model_endpoint_bring_your_own.html" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker Multi-Model Endpoints using your own algorithm
                            container</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                    <p>For an introductory tutorial about deploying a model to a real-time
                        inference endpoint, see <a href="http://aws.amazon.com/getting-started/hands-on/machine-learning-tutorial-deploy-model-to-real-time-inference-endpoint/" rel="noopener noreferrer" target="_blank"><span>Deploy a Machine Learning Model to a Real-Time Inference
                            Endpoint</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. </p>
                    <p>For information about how to deploy a custom Docker image in SageMaker in an
                        API gateway, see <a href="http://aws.amazon.com/blogs/machine-learning/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker/" rel="noopener noreferrer" target="_blank"><span>Creating a machine learning-powered REST API with Amazon API Gateway
                            mapping templates and Amazon SageMaker</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                 
            <awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="https://docs.aws.amazon.com/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./docker-containers-adapt-your-own-private-registry-authentication.html">Use a Docker registry that requires authentication for training</div><div id="next" class="next-link" accesskey="n" href="./docker-containers-create.html">Create a container with your own algorithms
                and models</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/adapt-inference-container.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/adapt-inference-container.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>