<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Serverless Inference - Amazon SageMaker</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="serverless-endpoints" /><meta name="default_state" content="serverless-endpoints" /><link rel="icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="canonical" href="serverless-endpoints.html" /><meta name="description" content="Amazon SageMaker Serverless Inference enables you to deploy your ML model without selecting instance types. Serverless Inference manages the heavy lifting of managing servers so you can focus on leveraging ML." /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon SageMaker" /><meta name="guide" content="Developer Guide" /><meta name="abstract" content="Use Internet Monitor to build, train, and host machine learning models in AWS." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="serverless-endpoints.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/serverless-endpoints.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/serverless-endpoints.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/serverless-endpoints.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/serverless-endpoints.html" hreflang="de" /><link rel="alternative" href="serverless-endpoints.html" hreflang="en-us" /><link rel="alternative" href="serverless-endpoints.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/serverless-endpoints.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/serverless-endpoints.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/serverless-endpoints.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/serverless-endpoints.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/serverless-endpoints.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/serverless-endpoints.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/serverless-endpoints.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/serverless-endpoints.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/serverless-endpoints.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/serverless-endpoints.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/serverless-endpoints.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/serverless-endpoints.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/serverless-endpoints.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/serverless-endpoints.html" hreflang="zh-tw" /><link rel="alternative" href="serverless-endpoints.html" hreflang="x-default" /><meta name="feedback-item" content="SageMaker" /><meta name="this_doc_product" content="Amazon SageMaker" /><meta name="this_doc_guide" content="Developer Guide" /><script defer="" src="../../../assets/r/vendor4.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor3.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor1.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-common.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-doc-page.js@version=2021.12.02"></script><link href="../../../assets/r/vendor4.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-common.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-doc-page.css@version=2021.12.02.css" rel="stylesheet" /><script async="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'sagemaker'}"></script><meta id="panorama-serviceSubSection" value="Developer Guide" /><meta id="panorama-serviceConsolePage" value="Serverless Inference" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>Serverless Inference - Amazon SageMaker</title><meta name="pdf" content="/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#serverless-endpoints" /><meta name="rss" content="amazon-sagemaker-release-notes.rss" /><meta name="forums" content="http://forums.aws.amazon.com/forum.jspa?forumID=285" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/serverless-endpoints.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/serverless-endpoints.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/serverless-endpoints.html" /><meta name="keywords" content="SageMaker,Amazon SageMaker,machine learning,notebook instance,inference,deploy model,endpoint,prediction,ML application,serverless machine learning,multi model deployment,inference pipelines,ML model,serverless" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "Developer Guide",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Deploy models for inference",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html"
      },
      {
        "@type" : "ListItem",
        "position" : 5,
        "name" : "Serverless Inference",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#serverless-endpoints" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="https://aws.amazon.com">AWS</a><a href="https://docs.aws.amazon.com/index.html">Documentation</a><a href="https://docs.aws.amazon.com/sagemaker/index.html">Amazon SageMaker</a><a href="whatis.html">Developer Guide</a></div><div id="page-toc-src"><a href="serverless-endpoints.html#serverless-endpoints-how-it-works">How it works</a><a href="serverless-endpoints.html#serverless-endpoints-get-started">Getting started</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="serverless-endpoints">Serverless Inference</h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>Amazon SageMaker Serverless Inference is a purpose-built inference option that enables you to deploy and scale ML models
  without configuring or managing any of the underlying infrastructure. On-demand Serverless Inference is ideal for
  workloads which have idle periods between traffic spurts and can tolerate cold starts. Serverless
  endpoints automatically launch compute resources and scale them in and out depending on traffic,
  eliminating the need to choose instance types or manage scaling policies. This takes away the
  undifferentiated heavy lifting of selecting and managing servers. Serverless Inference integrates with
  AWS Lambda to offer you high availability, built-in fault tolerance and automatic
  scaling. With a pay-per-use model, Serverless Inference is a cost-effective option if you have an infrequent or
  unpredictable traffic pattern. During times when there are no requests, Serverless Inference scales your endpoint
  down to 0, helping you to minimize your costs. For more information about pricing for on-demand Serverless Inference, see
   <a href="http://aws.amazon.com/sagemaker/pricing/" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker Pricing</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p><p>Optionally, you can also use Provisioned Concurrency with Serverless Inference. Serverless Inference with provisioned
  concurrency is a cost-effective option when you have predictable bursts in your traffic.
  Provisioned Concurrency allows you to deploy models on serverless endpoints with predictable
  performance, and high scalability by keeping your endpoints warm. SageMaker ensures that for the number
  of Provisioned Concurrency that you allocate, the compute resources are initialized and ready to
  respond within milliseconds. For Serverless Inference with Provisioned Concurrency, you pay for the compute
  capacity used to process inference requests, billed by the millisecond, and the amount of data
  processed. You also pay for Provisioned Concurrency usage, based on the memory configured,
  duration provisioned, and the amount of concurrency enabled. For more information about pricing
  for Serverless Inference with Provisioned Concurrency, see <a href="http://aws.amazon.com/sagemaker/pricing/" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker Pricing</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p><p>You can integrate Serverless Inference with your MLOps Pipelines to streamline your ML workflow, and you can
  use a serverless endpoint to host a model registered with <a href="model-registry.html">Model
   Registry</a>.</p><p>Serverless Inference is generally available in 21 AWS Regions: US East (N. Virginia), US East (Ohio), 
  US West (N. California), US West (Oregon), Africa (Cape Town), Asia Pacific (Hong Kong), Asia Pacific (Mumbai),
  Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Osaka), Asia Pacific (Singapore),
  Asia Pacific (Sydney), Canada (Central), Europe (Frankfurt), Europe (Ireland), 
  Europe (London), Europe (Paris), Europe (Stockholm), Europe (Milan), Middle East (Bahrain), 
  South America (São Paulo). For more information about Amazon SageMaker regional availability, see the 
  <a href="http://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/" rel="noopener noreferrer" target="_blank"><span>AWS 
   Regional Services List</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
  <h2 id="serverless-endpoints-how-it-works">How it works</h2>
  <p>The following diagram shows the workflow of on-demand Serverless Inference and the benefits of using a
   serverless endpoint.</p>
  <div class="mediaobject">
    
    <img src="../../../images/sagemaker/latest/dg/images/serverless-endpoints-how-it-works.png" class="aws-docs-img-whiteBg aws-docs-img-padding" alt="&#xA;    Diagram of the Serverless Inference workflow: the client sends a request to Serverless Inference and model predictions are sent back in response.&#xA;   " style="max-width:100%" />
    
    
  </div>
  <p>When you create an on-demand serverless endpoint, SageMaker provisions and manages the compute
   resources for you. Then, you can make inference requests to the endpoint and receive model
   predictions in response. SageMaker scales the compute resources up and down as needed to handle
   your request traffic, and you only pay for what you use.</p>
  <p>For Provisioned Concurrency, Serverless Inference also integrates with Application Auto Scaling, so that you can manage
   Provisioned Concurrency based on a target metric or on a schedule. For more information, see 
   <a href="serverless-endpoints-autoscale.html">Automatically scale Provisioned Concurrency for a serverless endpoint</a>.</p>
  <p>The following sections provide additional details about Serverless Inference and how it
   works.</p>
  
  <div class="highlights" id="inline-topiclist"><h6>Topics</h6><ul><li><a href="serverless-endpoints.html#serverless-endpoints-how-it-works-containers">Container support</a></li><li><a href="serverless-endpoints.html#serverless-endpoints-how-it-works-memory">Memory size</a></li><li><a href="serverless-endpoints.html#serverless-endpoints-how-it-works-concurrency">Concurrent invocations</a></li><li><a href="serverless-endpoints.html#serverless-endpoints-how-it-works-cold-starts">Minimizing cold starts</a></li><li><a href="serverless-endpoints.html#serverless-endpoints-how-it-works-exclusions">Feature exclusions</a></li></ul></div>
  
  <h3 id="serverless-endpoints-how-it-works-containers">Container support</h3>
   <p>For your endpoint container, you can choose either a SageMaker-provided container or bring your
    own. SageMaker provides containers for its built-in algorithms and prebuilt Docker images for some of
    the most common machine learning frameworks, such as Apache MXNet, TensorFlow, PyTorch, and Chainer.
    For a list of available SageMaker images, see <a href="https://github.com/aws/deep-learning-containers/blob/master/available_images.md" rel="noopener noreferrer" target="_blank"><span>Available
     Deep Learning Containers Images</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. If you are bringing your own container, you must
    modify it to work with SageMaker. For more information about bringing your own container, see <a href="adapt-inference-container.html">Adapting Your Own Inference
                    Container</a>.</p>
   <p>The maximum size of the container image you can use is 10 GB. For serverless endpoints, we
    recommend creating only one worker in the container and only loading one copy of the model. Note
    that this is unlike real-time endpoints, where some SageMaker containers may create a worker for each
    vCPU to process inference requests and load the model in each worker.</p>
   <p>If you already have a container for a real-time endpoint, you can use the same container
    for your serverless endpoint, though some capabilities are excluded. To learn more about the
    container capabilities that are not supported in Serverless Inference, see <a href="serverless-endpoints.html#serverless-endpoints-how-it-works-exclusions">Feature exclusions</a>. If you choose to use the same
    container, SageMaker escrows (retains) a copy of your container image until you delete all endpoints
    that use the image. SageMaker encrypts the copied image at rest with a SageMaker-owned AWS KMS key.</p>
   
   
   <h3 id="serverless-endpoints-how-it-works-memory">Memory size</h3>
   <p>Your serverless endpoint has a minimum RAM size of 1024 MB (1 GB), and the maximum RAM size
    you can choose is 6144 MB (6 GB). The memory sizes you can choose are 1024 MB, 2048 MB, 3072 MB,
    4096 MB, 5120 MB, or 6144 MB. Serverless Inference auto-assigns compute resources proportional to the memory
    you select. If you choose a larger memory size, your container has access to more vCPUs. Choose
    your endpoint’s memory size according to your model size. Generally, the memory size should be
    at least as large as your model size. You may need to benchmark in order to choose the right
    memory selection for your model based on your latency SLAs. For a step by step guide to benchmark,
    see <a href="http://aws.amazon.com/blogs/machine-learning/introducing-the-amazon-sagemaker-serverless-inference-benchmarking-toolkit/" rel="noopener noreferrer" target="_blank"><span>
     Introducing the Amazon SageMaker Serverless Inference Benchmarking Toolkit</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. The memory size increments have
    different pricing; see the <a href="https://aws.amazon.com/sagemaker/pricing/" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker
     pricing page</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> for more information.</p>
   <p>Regardless of the memory size you choose, your serverless endpoint has 5 GB of ephemeral
    disk storage available. For help with container permissions issues when working with storage,
    see <a href="serverless-endpoints-troubleshooting.html">Troubleshooting</a>.</p>
   
   
   <h3 id="serverless-endpoints-how-it-works-concurrency">Concurrent invocations</h3>
   <p>On-demand Serverless Inference manages predefined scaling policies and quotas for the capacity of your endpoint.
    Serverless endpoints have a quota for how many concurrent invocations can be processed at the
    same time. If the endpoint is invoked before it finishes processing the first request, then it
    handles the second request concurrently.</p>
   <p>The total concurrency that you can share between all serverless endpoints in your account depends
    on your region:</p> 
   <div class="itemizedlist">
     
     
   <ul class="itemizedlist"><li class="listitem">
     <p>For the US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Singapore),
      Asia Pacific (Sydney), Asia Pacific (Tokyo), Europe (Frankfurt), and Europe (Ireland)
      Regions, the total concurrency you can share between all serverless endpoints per
      Region in your account is 1000.</p>
    </li><li class="listitem">
     <p>For the US West (N. California), Africa (Cape Town),
     Asia Pacific (Hong Kong), Asia Pacific (Mumbai), Asia Pacific (Osaka), Asia Pacific (Seoul),
     Canada (Central), Europe (London), Europe (Milan), Europe (Paris),
     Europe (Stockholm), Middle East (Bahrain), and South America (São Paulo) Regions, the total concurrency
     per Region in your account is 500.</p>
    </li></ul></div>
   <p>You can set the maximum concurrency for a single endpoint up to 200, and the total
    number of serverless endpoints you can host in a Region is 50. The maximum concurrency for an
    individual endpoint prevents that endpoint from taking up all of the invocations allowed for your
    account, and any endpoint invocations beyond the maximum are throttled.</p>
   <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Provisioned Concurrency that you assign to a serverless endpoint should always be less than
     or equal to the maximum concurrency that you assigned to that endpoint.</p></div></div>
   <p>To learn how to set the maximum concurrency for your endpoint, see <a href="serverless-endpoints-create.html#serverless-endpoints-create-config">Create an endpoint configuration</a>. For more information about quotas and limits, see <a href="https://docs.aws.amazon.com/general/latest/gr/sagemaker.html"> Amazon SageMaker endpoints and quotas</a> in the <em>AWS General Reference</em>. To request a service limit increase, contact <a href="https://console.aws.amazon.com/support" rel="noopener noreferrer" target="_blank"><span>AWS Support</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. For instructions on how to request a
    service limit increase, see <a href="regions-quotas.html">Supported Regions and Quotas</a>.</p>
   
   
   <h3 id="serverless-endpoints-how-it-works-cold-starts">Minimizing cold starts</h3>
   <p>If your on-demand Serverless Inference endpoint does not receive traffic for a while and then your endpoint suddenly
    receives new requests, it can take some time for your endpoint to spin up the compute resources
    to process the requests. This is called a <em>cold start</em>. Since
    serverless endpoints provision compute resources on demand, your endpoint may experience cold
    starts. A cold start can also occur if your concurrent requests exceed the current concurrent
    request usage. The cold start time depends on your model size, how long it takes to download
    your model, and the start-up time of your container.</p>
   <p>To monitor how long your cold start time is, you can use the Amazon CloudWatch metric
     <code class="code">OverheadLatency</code> to monitor your serverless endpoint. This metric tracks the time it
    takes to launch new compute resources for your endpoint. To learn more about using CloudWatch metrics
    with serverless endpoints, see <a href="serverless-endpoints-monitoring.html">Monitor a serverless endpoint</a>.</p>
   <p>You can minimize cold starts by using Provisioned Concurrency. SageMaker keeps the endpoint warm
    and ready to respond in milliseconds, for the number of Provisioned Concurrency that you allocated.</p>
   
   
   <h3 id="serverless-endpoints-how-it-works-exclusions">Feature exclusions</h3>
   <p>Some of the features currently available for SageMaker Real-time Inference are not supported for
    Serverless Inference, including GPUs, AWS marketplace model packages, private Docker registries, Multi-Model Endpoints, VPC
    configuration, network isolation, data capture, multiple production variants, Model Monitor, and
    inference pipelines.</p>
   <p>You cannot convert your instance-based, real-time endpoint to a serverless
    endpoint. If you try to update your real-time endpoint to serverless, you receive a
     <code class="code">ValidationError</code> message. You can convert a serverless endpoint to real-time, but
    once you make the update, you cannot roll it back to serverless.</p>
   
  
  <h2 id="serverless-endpoints-get-started">Getting started</h2>
  <p>You can create, update, describe, and delete a serverless endpoint using the SageMaker console,
   the AWS SDKs, the <a href="https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-serverless-inference" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker Python
    SDK</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>, and the AWS CLI. You can invoke your endpoint using the AWS SDKs, the <a href="https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-serverless-inference" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker Python SDK</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>, and the AWS CLI.
   For serverless endpoints with Provisioned Concurrency, you can use Application Auto Scaling to auto scale Provisioned Concurrency based on a target
   metric or a schedule. For more information about how to set up and use a serverless endpoint, read the guide <a href="serverless-endpoints-create-invoke-update-delete.html">Create, invoke, update, and delete a serverless endpoint</a>. For more information on auto scaling serverless endpoints
  with Provisioned Concurrency, see <a href="serverless-endpoints-autoscale.html">Automatically scale Provisioned Concurrency for a serverless endpoint</a>.</p>
  <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>
    Application Auto Scaling for Serverless Inference with Provisioned Concurrency is currently not supported on AWS CloudFormation.
   </p></div></div>
   
   <h3 id="serverless-endpoints-get-started-nbs">Example notebooks and blogs</h3>
   <p>For Jupyter notebook examples that show end-to-end serverless endpoint workflows, see
    the <a href="https://github.com/aws/amazon-sagemaker-examples/tree/master/serverless-inference" rel="noopener noreferrer" target="_blank"><span>Serverless Inference example notebooks</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
   
   
 <awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="https://docs.aws.amazon.com/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./realtime-endpoints-test-endpoints.html">Invoke real-time endpoints</div><div id="next" class="next-link" accesskey="n" href="./serverless-endpoints-create-invoke-update-delete.html">Create, invoke, update, and delete a serverless endpoint</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/serverless-endpoints.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/serverless-endpoints.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>