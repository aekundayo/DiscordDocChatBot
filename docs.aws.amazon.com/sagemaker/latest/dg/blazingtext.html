<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>BlazingText algorithm - Amazon SageMaker</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="blazingtext" /><meta name="default_state" content="blazingtext" /><link rel="icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="canonical" href="blazingtext.html" /><meta name="description" content="The Amazon SageMaker BlazingText algorithm provides implementations of the Word2vec and text classification algorithms." /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon SageMaker" /><meta name="guide" content="Developer Guide" /><meta name="abstract" content="Use Internet Monitor to build, train, and host machine learning models in AWS." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="blazingtext.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/blazingtext.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/blazingtext.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/blazingtext.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/blazingtext.html" hreflang="de" /><link rel="alternative" href="blazingtext.html" hreflang="en-us" /><link rel="alternative" href="blazingtext.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/blazingtext.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/blazingtext.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/blazingtext.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/blazingtext.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/blazingtext.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/blazingtext.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/blazingtext.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/blazingtext.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/blazingtext.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/blazingtext.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/blazingtext.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/blazingtext.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/blazingtext.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/blazingtext.html" hreflang="zh-tw" /><link rel="alternative" href="blazingtext.html" hreflang="x-default" /><meta name="feedback-item" content="SageMaker" /><meta name="this_doc_product" content="Amazon SageMaker" /><meta name="this_doc_guide" content="Developer Guide" /><script defer="" src="../../../assets/r/vendor4.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor3.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor1.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-common.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-doc-page.js@version=2021.12.02"></script><link href="../../../assets/r/vendor4.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-common.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-doc-page.css@version=2021.12.02.css" rel="stylesheet" /><script async="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'sagemaker'}"></script><meta id="panorama-serviceSubSection" value="Developer Guide" /><meta id="panorama-serviceConsolePage" value="BlazingText algorithm" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>BlazingText algorithm - Amazon SageMaker</title><meta name="pdf" content="/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#blazingtext" /><meta name="rss" content="amazon-sagemaker-release-notes.rss" /><meta name="forums" content="http://forums.aws.amazon.com/forum.jspa?forumID=285" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/blazingtext.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/blazingtext.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/blazingtext.html" /><meta name="keywords" content="SageMaker,Amazon SageMaker,machine learning,notebook instance" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "Developer Guide",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Train machine learning models",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/train-model.html"
      },
      {
        "@type" : "ListItem",
        "position" : 5,
        "name" : "Choose an Algorithm",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-choose.html"
      },
      {
        "@type" : "ListItem",
        "position" : 6,
        "name" : "Use Amazon SageMaker Built-in Algorithms or Pre-trained Models",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"
      },
      {
        "@type" : "ListItem",
        "position" : 7,
        "name" : "Built-in SageMaker Algorithms for Text Data",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-text.html"
      },
      {
        "@type" : "ListItem",
        "position" : 8,
        "name" : "BlazingText algorithm",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-text.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#blazingtext" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="https://aws.amazon.com">AWS</a><a href="https://docs.aws.amazon.com/index.html">Documentation</a><a href="https://docs.aws.amazon.com/sagemaker/index.html">Amazon SageMaker</a><a href="whatis.html">Developer Guide</a></div><div id="page-toc-src"><a href="blazingtext.html#bt-inputoutput">Input/Output Interface for the BlazingText
                Algorithm</a><a href="blazingtext.html#blazingtext-instances">EC2 Instance Recommendation for the BlazingText
                Algorithm</a><a href="blazingtext.html#blazingtext-sample-notebooks">Sample Notebooks</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="blazingtext">BlazingText algorithm</h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>The Amazon SageMaker BlazingText algorithm provides highly optimized implementations of the
        Word2vec and text classification algorithms. The Word2vec algorithm is useful for many
        downstream natural language processing (NLP) tasks, such as sentiment analysis, named entity
        recognition, machine translation, etc. Text classification is an important task for
        applications that perform web searches, information retrieval, ranking, and document
        classification.</p><p>The Word2vec algorithm maps words to high-quality distributed vectors. The resulting
        vector representation of a word is called a <em>word embedding</em>. Words that
        are semantically similar correspond to vectors that are close together. That way, word
        embeddings capture the semantic relationships between words. </p><p>Many natural language processing (NLP) applications learn word embeddings by training on
        large collections of documents. These pretrained vector representations provide information
        about semantics and word distributions that typically improves the generalizability of other
        models that are later trained on a more limited amount of data. Most implementations of the
        Word2vec algorithm are not optimized for multi-core CPU architectures. This makes it
        difficult to scale to large datasets. </p><p>With the BlazingText algorithm, you can scale to large datasets easily. Similar to
        Word2vec, it provides the Skip-gram and continuous bag-of-words (CBOW) training
        architectures. BlazingText's implementation of the supervised multi-class, multi-label text
        classification algorithm extends the fastText text classifier to use GPU acceleration with
        custom <a href="https://docs.nvidia.com/cuda/index.html" rel="noopener noreferrer" target="_blank"><span>CUDA </span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> kernels. You can
        train a model on more than a billion words in a couple of minutes using a multi-core CPU or
        a GPU. And, you achieve performance on par with the state-of-the-art deep learning text
        classification algorithms.</p><p>The BlazingText algorithm is not parallelizable. For more information on 
        parameters related to training, see <a href="https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html">
            Docker Registry Paths for SageMaker Built-in Algorithms</a>.</p><p> The SageMaker BlazingText algorithms provides the following features:</p><div class="itemizedlist">
         
         
         
    <ul class="itemizedlist"><li class="listitem">
            <p>Accelerated training of the fastText text classifier on multi-core CPUs or a GPU
                and Word2Vec on GPUs using highly optimized CUDA kernels. For more information, see
                    <a href="https://dl.acm.org/citation.cfm?doid=3146347.3146354" rel="noopener noreferrer" target="_blank"><span>BlazingText:
                    Scaling and Accelerating Word2Vec using Multiple GPUs</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
        </li><li class="listitem">
            <p><a href="https://arxiv.org/abs/1607.04606" rel="noopener noreferrer" target="_blank"><span>Enriched Word Vectors with Subword
                    Information</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> by learning vector representations for character n-grams.
                This approach enables BlazingText to generate meaningful vectors for
                out-of-vocabulary (OOV) words by representing their vectors as the sum of the
                character n-gram (subword) vectors.</p>
        </li><li class="listitem">
            <p>A <code class="code">batch_skipgram</code>
                <code class="code">mode</code> for the Word2Vec algorithm that allows faster training and
                distributed computation across multiple CPU nodes. The <code class="code">batch_skipgram</code>
                <code class="code">mode</code> does mini-batching using the Negative Sample Sharing strategy to
                convert level-1 BLAS operations into level-3 BLAS operations. This efficiently
                leverages the multiply-add instructions of modern architectures. For more
                information, see <a href="https://arxiv.org/pdf/1604.04661.pdf" rel="noopener noreferrer" target="_blank"><span>Parallelizing
                    Word2Vec in Shared and Distributed Memory</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
        </li></ul></div><p>To summarize, the following modes are supported by BlazingText on different types
        instances:</p><div class="table-container"><div class="table-contents"><table id="w714aac23c11c21c33b9c19"><thead>
                <tr>
                    <th>Modes</th>
                    <th>
                        <p>Word2Vec</p>
                        <p>(Unsupervised Learning)</p>
                    </th>
                    <th>
                        <p>Text Classification</p>
                        <p>(Supervised Learning)</p>
                    </th>
                </tr>
            </thead>
                <tr>
                    <td tabindex="-1">
                        <p>Single CPU instance</p>
                    </td>
                    <td tabindex="-1">
                        <p><code class="code">cbow</code></p>
                        <p><code class="code">Skip-gram</code></p>
                        <p><code class="code">Batch Skip-gram</code></p>
                    </td>
                    <td tabindex="-1">
                        <p><code class="code">supervised</code></p>
                    </td>
                </tr>
                <tr>
                    <td tabindex="-1">
                        <p>Single GPU instance (with 1 or more GPUs)</p>
                    </td>
                    <td tabindex="-1">
                        <p><code class="code">cbow</code></p>
                        <p><code class="code">Skip-gram</code></p>
                    </td>
                    <td tabindex="-1">
                        <p><code class="code">supervised</code> with one GPU</p>
                    </td>
                </tr>
                <tr>
                    <td tabindex="-1">
                        <p>Multiple CPU instances</p>
                    </td>
                    <td tabindex="-1"><code class="code">Batch Skip-gram </code></td>
                    <td tabindex="-1">None</td>
                </tr>
            </table></div></div><p>For more information about the mathematics behind BlazingText, see <a href="https://dl.acm.org/citation.cfm?doid=3146347.3146354" rel="noopener noreferrer" target="_blank"><span>BlazingText: Scaling and
            Accelerating Word2Vec using Multiple GPUs</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p><div class="highlights"><h6>Topics</h6><ul><li><a href="blazingtext.html#bt-inputoutput">Input/Output Interface for the BlazingText
                Algorithm</a></li><li><a href="blazingtext.html#blazingtext-instances">EC2 Instance Recommendation for the BlazingText
                Algorithm</a></li><li><a href="blazingtext.html#blazingtext-sample-notebooks">BlazingText Sample Notebooks</a></li><li><a href="blazingtext_hyperparameters.html">BlazingText Hyperparameters</a></li><li><a href="blazingtext-tuning.html">Tune a BlazingText Model</a></li></ul></div>
        <h2 id="bt-inputoutput">Input/Output Interface for the BlazingText
                Algorithm</h2>
        <p>The BlazingText algorithm expects a single preprocessed text file with space-separated
            tokens. Each line in the file should contain a single sentence. If you need to train on
            multiple text files, concatenate them into one file and upload the file in the
            respective channel.</p>
         
            <h3 id="blazingtext-data-formats">Training and Validation Data
                    Format</h3>
             
                <h4 id="blazingtext-data-formats-word2vec">Training and Validation Data
                        Format for the Word2Vec Algorithm </h4>
                <p>For Word2Vec training, upload the file under the <em>train</em>
                    channel. No other channels are supported. The file should contain a training
                    sentence per line.</p>
             
             
                <h4 id="blazingtext-data-formats-text-class">Training and Validation
                        Data Format for the Text Classification Algorithm </h4>
                <p>For supervised mode, you can train with file mode or with the augmented
                    manifest text format.</p>
                 
                    <h5 id="blazingtext-data-formats-text-class-file-mode">Train with
                            File Mode</h5>
                    <p>For <code class="code">supervised</code> mode, the training/validation file should
                        contain a training sentence per line along with the labels. Labels are words
                        that are prefixed by the string <em>__label__</em>. Here is an
                        example of a training/validation file:</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="">
__label__4  linux ready for prime time , intel says , despite all the linux hype , the open-source movement has yet to make a huge splash in the desktop market . that may be about to change , thanks to chipmaking giant intel corp .

__label__2  bowled by the slower one again , kolkata , november 14 the past caught up with sourav ganguly as the indian skippers return to international cricket was short lived .
</code></pre>
                    <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>The order of labels within the sentence doesn't matter. </p></div></div>
                    <p>Upload the training file under the train channel, and optionally upload
                        the validation file under the validation channel.</p>
                 
                 
                    <h5 id="blazingtext-data-formats-text-class-augmented-manifest">Train with Augmented Manifest Text Format</h5>
                    <p>Supervised mode for CPU instances also supports the augmented manifest
                        format, which enables you to do training in pipe mode without needing to
                        create RecordIO files. While using the format, an S3 manifest file needs to
                        be generated that contains the list of sentences and their corresponding
                        labels. The manifest file format should be in <a href="http://jsonlines.org/" rel="noopener noreferrer" target="_blank"><span>JSON Lines</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> format in which each line
                        represents one sample. The sentences are specified using the
                            <code class="code">source</code> tag and the label can be specified using the
                            <code class="code">label</code> tag. Both <code class="code">source</code> and <code class="code">label</code>
                        tags should be provided under the <code class="code">AttributeNames</code> parameter
                        value as specified in the request.</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class=""><span>{</span>"source":"linux ready for prime time , intel says , despite all the linux hype", "label":1}
<span>{</span>"source":"bowled by the slower one again , kolkata , november 14 the past caught up with sourav ganguly", "label":2}</code></pre>
                    <p>Multi-label training is also supported by specifying a JSON array of
                        labels.</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class=""><span>{</span>"source":"linux ready for prime time , intel says , despite all the linux hype", "label": [1, 3]}
<span>{</span>"source":"bowled by the slower one again , kolkata , november 14 the past caught up with sourav ganguly", "label": [2, 4, 5]}</code></pre>
                    <p>For more information on augmented manifest files, see <a href="augmented-manifest.html">Provide Dataset Metadata to Training Jobs with an
            Augmented Manifest File</a>.</p>
                 
             
         
         
            <h3 id="blazingtext-artifacts-inference">Model Artifacts and
                    Inference</h3>
             
                <h4 id="blazingtext--artifacts-inference-word2vec">Model Artifacts for
                        the Word2Vec Algorithm </h4>
                <p>For Word2Vec training, the model artifacts consist of
                        <em>vectors.txt</em>, which contains words-to-vectors mapping,
                    and <em>vectors.bin</em>, a binary used by BlazingText for hosting,
                    inference, or both. <em>vectors.txt</em> stores the vectors in a
                    format that is compatible with other tools like Gensim and Spacy. For example, a
                    Gensim user can run the following commands to load the vectors.txt file:</p>
                <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="">from gensim.models import KeyedVectors
word_vectors = KeyedVectors.load_word2vec_format('vectors.txt', binary=False)
word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])
word_vectors.doesnt_match("breakfast cereal dinner lunch".split())</code></pre>
                <p>If the evaluation parameter is set to <code class="code">True</code>, an additional file,
                        <em>eval.json</em>, is created. This file contains the
                    similarity evaluation results (using Spearman’s rank correlation coefficients)
                    on WS-353 dataset. The number of words from the WS-353 dataset that aren't there
                    in the training corpus are reported.</p>
                <p>For inference requests, the model accepts a JSON file containing a list of
                    strings and returns a list of vectors. If the word is not found in vocabulary,
                    inference returns a vector of zeros. If subwords is set to <code class="code">True</code>
                    during training, the model is able to generate vectors for out-of-vocabulary
                    (OOV) words.</p>
                 
                    <h5 id="word2vec-json-request">Sample JSON Request </h5>
                    <p>Mime-type:<code class="code"> application/json</code></p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class=""><span>{</span>
"instances": ["word1", "word2", "word3"]
}</code></pre>
                 
             
             
                <h4 id="blazingtext-artifacts-inference-text-class">Model Artifacts for
                        the Text Classification Algorithm </h4>
                <p>Training with supervised outputs creates a <em>model.bin</em> file
                    that can be consumed by BlazingText hosting. For inference, the BlazingText
                    model accepts a JSON file containing a list of sentences and returns a list of
                    corresponding predicted labels and probability scores. Each sentence is expected
                    to be a string with space-separated tokens, words, or both.</p>
                 
                    <h5 id="text-class-json-request">Sample JSON Request </h5>
                    <p>Mime-type:<code class="code"> application/json</code></p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class=""><span>{</span>
 "instances": ["the movie was excellent", "i did not like the plot ."]
}</code></pre>
                    <p>By default, the server returns only one prediction, the one with the
                        highest probability. For retrieving the top <em>k</em> predictions, you can set <em>k</em> in the configuration, as follows:</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class=""><span>{</span>
 "instances": ["the movie was excellent", "i did not like the plot ."],
 "configuration": <span>{</span>"k": 2}
}</code></pre>
                    
                    <p>For BlazingText, the<code class="code"> content-type</code> and <code class="code">accept</code>
                        parameters must be equal. For batch transform, they both need to be
                            <code class="code">application/jsonlines</code>. If they differ, the
                            <code class="code">Accept</code> field is ignored. The format for input
                        follows:</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="json ">content-type: application/jsonlines

<span>{</span>"source": "source_0"}
<span>{</span>"source": "source_1"}

if you need to pass the value of k for top-k, then you can do it in the following way:

<span>{</span>"source": "source_0", "k": 2}
<span>{</span>"source": "source_1", "k": 3}</code></pre>
                    <p>The format for output follows:</p>
                    <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="json ">accept: application/jsonlines


<span>{</span>"prob": [prob_1], "label": ["__label__1"]}
<span>{</span>"prob": [prob_1], "label": ["__label__1"]}

If you have passed the value of k to be more than 1, then response will be in this format:

<span>{</span>"prob": [prob_1, prob_2], "label": ["__label__1", "__label__2"]}
<span>{</span>"prob": [prob_1, prob_2], "label": ["__label__1", "__label__2"]}</code></pre>
                 
             
            <p>For both supervised (text classification) and unsupervised (Word2Vec) modes, the
                binaries (<em>*.bin</em>) produced by BlazingText can be cross-consumed
                by fastText and vice versa. You can use binaries produced by BlazingText by
                fastText. Likewise, you can host the model binaries created with fastText using
                BlazingText.</p>
            <p>Here is an example of how to use a model generated with BlazingText with
                fastText:</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="">
#Download the model artifact from S3
aws s3 cp s3://&lt;YOUR_S3_BUCKET&gt;/&lt;PREFIX&gt;/model.tar.gz model.tar.gz

#Unzip the model archive
tar -xzf model.tar.gz

#Use the model archive with fastText
fasttext predict ./model.bin test.txt</code></pre>
            
            <p>However, the binaries are only supported when training on CPU and single GPU;
                training on multi-GPU will not produce binaries.</p>
         
     
        <h2 id="blazingtext-instances">EC2 Instance Recommendation for the BlazingText
                Algorithm</h2>
            <p>For <code class="code">cbow</code> and <code class="code">skipgram</code> modes, BlazingText supports single CPU
            and single GPU instances. Both of these modes support learning of <code class="code">subwords</code>
            embeddings. To achieve the highest speed without compromising accuracy, we recommend
            that you use an ml.p3.2xlarge instance. </p>
        <p>For <code class="code">batch_skipgram</code> mode, BlazingText supports single or multiple CPU
            instances. When training on multiple instances, set the value of the
                <code class="code">S3DataDistributionType</code> field of the <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_S3DataSource.html"><code class="code">S3DataSource</code></a> object that you pass to <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html"><code class="code">CreateTrainingJob</code></a> to <code class="code">FullyReplicated</code>. BlazingText
            takes care of distributing data across machines.</p>
        <p>For the supervised text classification mode, a C5 instance is recommended if the
            training dataset is less than 2 GB. For larger datasets, use an instance with a single
            GPU. BlazingText supports P2, P3, G4dn, and G5 instances for training and
            inference.</p>
     
        <h2 id="blazingtext-sample-notebooks">BlazingText Sample Notebooks</h2>
        <p>For a sample notebook that trains and deploys the SageMaker BlazingText algorithm to
            generate word vectors, see <a href="https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/blazingtext_word2vec_text8/blazingtext_word2vec_text8.html" rel="noopener noreferrer" target="_blank"><span>Learning Word2Vec Word Representations using BlazingText</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. For instructions
            for creating and accessing Jupyter notebook instances that you can use to run the
            example in SageMaker, see <a href="nbi.html">Amazon SageMaker Notebook Instances</a>. After creating and
            opening a notebook instance, choose the <b>SageMaker Examples</b>
            tab to see a list of all the SageMaker examples. The topic modeling example notebooks that
            use the Blazing Text are located in the <b>Introduction to Amazon
                algorithms</b> section. To open a notebook, choose its <b>Use</b> tab, then choose <b>Create
            copy</b>.</p>
    <awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="https://docs.aws.amazon.com/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./algorithms-text.html">Text</div><div id="next" class="next-link" accesskey="n" href="./blazingtext_hyperparameters.html">Hyperparameters</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/blazingtext.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/blazingtext.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>