<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>XGBoost Algorithm - Amazon SageMaker</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="xgboost" /><meta name="default_state" content="xgboost" /><link rel="icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="canonical" href="xgboost.html" /><meta name="description" content="XGBoost is a supervised learning algorithm that is an open-source implementation of the gradient boosted trees algorithm." /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon SageMaker" /><meta name="guide" content="Developer Guide" /><meta name="abstract" content="Use Internet Monitor to build, train, and host machine learning models in AWS." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="xgboost.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/xgboost.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/xgboost.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/xgboost.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/xgboost.html" hreflang="de" /><link rel="alternative" href="xgboost.html" hreflang="en-us" /><link rel="alternative" href="xgboost.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/xgboost.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/xgboost.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/xgboost.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/xgboost.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/xgboost.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/xgboost.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/xgboost.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/xgboost.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/xgboost.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/xgboost.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/xgboost.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/xgboost.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/xgboost.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/xgboost.html" hreflang="zh-tw" /><link rel="alternative" href="xgboost.html" hreflang="x-default" /><meta name="feedback-item" content="SageMaker" /><meta name="this_doc_product" content="Amazon SageMaker" /><meta name="this_doc_guide" content="Developer Guide" /><script defer="" src="../../../assets/r/vendor4.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor3.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor1.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-common.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-doc-page.js@version=2021.12.02"></script><link href="../../../assets/r/vendor4.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-common.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-doc-page.css@version=2021.12.02.css" rel="stylesheet" /><script async="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'sagemaker'}"></script><meta id="panorama-serviceSubSection" value="Developer Guide" /><meta id="panorama-serviceConsolePage" value="XGBoost Algorithm" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>XGBoost Algorithm - Amazon SageMaker</title><meta name="pdf" content="/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#xgboost" /><meta name="rss" content="amazon-sagemaker-release-notes.rss" /><meta name="forums" content="http://forums.aws.amazon.com/forum.jspa?forumID=285" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/xgboost.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/xgboost.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/xgboost.html" /><meta name="keywords" content="SageMaker,Amazon SageMaker,machine learning,notebook instance" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "Developer Guide",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Train machine learning models",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/train-model.html"
      },
      {
        "@type" : "ListItem",
        "position" : 5,
        "name" : "Choose an Algorithm",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-choose.html"
      },
      {
        "@type" : "ListItem",
        "position" : 6,
        "name" : "Use Amazon SageMaker Built-in Algorithms or Pre-trained Models",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"
      },
      {
        "@type" : "ListItem",
        "position" : 7,
        "name" : "Built-in SageMaker Algorithms for Tabular Data",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-tabular.html"
      },
      {
        "@type" : "ListItem",
        "position" : 8,
        "name" : "XGBoost Algorithm",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/algorithms-tabular.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#xgboost" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="https://aws.amazon.com">AWS</a><a href="https://docs.aws.amazon.com/index.html">Documentation</a><a href="https://docs.aws.amazon.com/sagemaker/index.html">Amazon SageMaker</a><a href="whatis.html">Developer Guide</a></div><div id="page-toc-src"><a href="xgboost.html#xgboost-supported-versions">Supported versions</a><a href="xgboost.html#xgboost-modes">How to Use XGBoost</a><a href="xgboost.html#InputOutput-XGBoost">Input/Output Interface for the XGBoost
                Algorithm</a><a href="xgboost.html#Instance-XGBoost">EC2 Instance Recommendation for the XGBoost
                Algorithm</a><a href="xgboost.html#xgboost-sample-notebooks">Sample Notebooks</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="xgboost">XGBoost Algorithm</h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>The <a href="https://github.com/dmlc/xgboost" rel="noopener noreferrer" target="_blank"><span>XGBoost</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> (eXtreme Gradient
        Boosting) is a popular and efficient open-source implementation of the gradient boosted
        trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to
        accurately predict a target variable by combining an ensemble of estimates from a set of
        simpler and weaker models. The XGBoost algorithm performs well in machine learning
        competitions because of its robust handling of a variety of data types, relationships,
        distributions, and the variety of hyperparameters that you can fine-tune. You can use
        XGBoost for regression, classification (binary and multiclass), and ranking problems. </p><p>You can use the new release of the XGBoost algorithm either as a Amazon SageMaker built-in
        algorithm or as a framework to run training scripts in your local environments. This
        implementation has a smaller memory footprint, better logging, improved hyperparameter
        validation, and an expanded set of metrics than the original versions. It provides an
        XGBoost <code class="code">estimator</code> that executes a training script in a managed XGBoost
        environment. The current release of SageMaker XGBoost is based on the original XGBoost versions
        1.0, 1.2, 1.3, 1.5, and 1.7.</p>
        <h2 id="xgboost-supported-versions">Supported versions</h2>
        <div class="itemizedlist">
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p>Framework (open source) mode: 1.0-1, 1.2-1, 1.2-2, 1.3-1, 1.5-1, 1.7-1</p>
            </li><li class="listitem">
                <p>Algorithm mode: 1.0-1, 1.2-1, 1.2-2, 1.3-1, 1.5-1, 1.7-1</p>
            </li></ul></div>
        <div class="awsdocs-note awsdocs-warning"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Warning</h6></div><div class="awsdocs-note-text"><p>Due to required compute capacity, version 1.7-1 of SageMaker XGBoost is not compatible with
                GPU instances from the P2 instance family for training or inference.</p></div></div>
        <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>When you retrieve the SageMaker XGBoost image URI, do not use <code class="code">:latest</code>
                or <code class="code">:1</code> for the image URI tag. You must specify one of the Supported versions to choose the SageMaker-managed XGBoost container with the native XGBoost package
                version that you want to use. To find the package version migrated into the
                SageMaker XGBoost containers, see <a href="sagemaker-algo-docker-registry-paths.html">Docker Registry Paths and Example Code</a>, choose your AWS Region, and
                navigate to the <b>XGBoost (algorithm)</b> section.</p></div></div>
        <div class="awsdocs-note awsdocs-warning"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Warning</h6></div><div class="awsdocs-note-text"><p>The XGBoost 0.90 versions are deprecated. Supports for security updates or bug fixes for
                XGBoost 0.90 is discontinued. It is highly recommended to upgrade the XGBoost
                version to one of the newer versions.</p></div></div>
        <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>XGBoost v1.1 is not supported on SageMaker because XGBoost 1.1 has a broken capability to run
                prediction when the test input has fewer features than the training data in LIBSVM
                inputs. This capability has been restored in XGBoost v1.2. Consider using SageMaker
                XGBoost 1.2-2 or later.</p></div></div>
     
        <h2 id="xgboost-modes">How to Use SageMaker XGBoost</h2>
        <p>With SageMaker, you can use XGBoost as a built-in algorithm or framework. By using XGBoost
            as a framework, you have more flexibility and access to more advanced scenarios, such as
            k-fold cross-validation, because you can customize your own training scripts. The
            following sections describe how to use XGBoost with the SageMaker Python SDK. For information
            on how to use XGBoost from the Amazon SageMaker Studio UI, see <a href="studio-jumpstart.html">SageMaker JumpStart</a>.</p>
        <div class="itemizedlist">
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p><b>Use XGBoost as a framework</b></p>
                <p>Use XGBoost as a framework to run your customized training scripts that can
                    incorporate additional data processing into your training jobs. In the following
                    code example, you can find how SageMaker Python SDK provides the XGBoost API as a
                    framework in the same way it provides other framework APIs, such as TensorFlow,
                    MXNet, and PyTorch.</p>
                <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="py ">import boto3
import sagemaker
from sagemaker.xgboost.estimator import XGBoost
from sagemaker.session import Session
from sagemaker.inputs import TrainingInput

# initialize hyperparameters
hyperparameters = <span>{</span>
        "max_depth":"5",
        "eta":"0.2",
        "gamma":"4",
        "min_child_weight":"6",
        "subsample":"0.7",
        "verbosity":"1",
        "objective":"reg:squarederror",
        "num_round":"50"}

# set an output path where the trained model will be saved
bucket = sagemaker.Session().default_bucket()
prefix = 'DEMO-xgboost-as-a-framework'
output_path = 's3://<span>{</span>}/<span>{</span>}/<span>{</span>}/output'.format(bucket, prefix, 'abalone-xgb-framework')

# construct a SageMaker XGBoost estimator
# specify the entry_point to your xgboost training script
estimator = XGBoost(entry_point = "<code class="replaceable">your_xgboost_abalone_script.py</code>", 
                    framework_version='<code class="replaceable">1.7-1</code>',
                    hyperparameters=hyperparameters,
                    role=sagemaker.get_execution_role(),
                    instance_count=1,
                    instance_type='ml.m5.2xlarge',
                    output_path=output_path)

# define the data type and paths to the training and validation datasets
content_type = "libsvm"
train_input = TrainingInput("s3://<span>{</span>}/<span>{</span>}/<span>{</span>}/".format(bucket, prefix, 'train'), content_type=content_type)
validation_input = TrainingInput("s3://<span>{</span>}/<span>{</span>}/<span>{</span>}/".format(bucket, prefix, 'validation'), content_type=content_type)

# execute the XGBoost training job
estimator.fit(<span>{</span>'train': train_input, 'validation': validation_input})</code></pre>
                <p>For an end-to-end example of using SageMaker XGBoost as a framework, see <a href="https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone_dist_script_mode.html" rel="noopener noreferrer" target="_blank"><span>Regression with Amazon SageMaker XGBoost</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
            </li><li class="listitem">
                <p><b>Use XGBoost as a built-in algorithm</b></p>
                <p>Use the XGBoost built-in algorithm to build an XGBoost training container as
                    shown in the following code example. You can automatically spot the XGBoost
                    built-in algorithm image URI using the SageMaker <code class="code">image_uris.retrieve</code> API
                    (or the <code class="code">get_image_uri</code> API if using <a href="https://sagemaker.readthedocs.io" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker Python SDK</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> version 1).
                    If you want to ensure if the <code class="code">image_uris.retrieve</code> API finds the
                    correct URI, see <a href="sagemaker-algo-docker-registry-paths.html"> Common
                        parameters for built-in algorithms </a> and look up <code class="code">xgboost</code>
                    from the full list of built-in algorithm image URIs and available
                    regions.</p>
                <p>After specifying the XGBoost image URI, you can use the XGBoost container to
                    construct an estimator using the SageMaker Estimator API and initiate a training job.
                    This XGBoost built-in algorithm mode does not incorporate your own XGBoost
                    training script and runs directly on the input datasets. </p>
                <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>When you retrieve the SageMaker XGBoost image URI, do not use
                            <code class="code">:latest</code> or <code class="code">:1</code> for the image URI tag. You must
                        specify one of the <a href="xgboost.html#xgboost-supported-versions">Supported versions</a> to choose the
                        SageMaker-managed XGBoost container with the native XGBoost package version
                        that you want to use. To find the package version migrated into the
                        SageMaker XGBoost containers, see <a href="sagemaker-algo-docker-registry-paths.html">Docker Registry Paths and Example Code</a>, choose your AWS Region, and
                        navigate to the <b>XGBoost (algorithm)</b>
                        section.</p></div></div>
                <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="py ">import sagemaker
import boto3
from sagemaker import image_uris
from sagemaker.session import Session
from sagemaker.inputs import TrainingInput

# initialize hyperparameters
hyperparameters = <span>{</span>
        "max_depth":"5",
        "eta":"0.2",
        "gamma":"4",
        "min_child_weight":"6",
        "subsample":"0.7",
        "objective":"reg:squarederror",
        "num_round":"50"}

# set an output path where the trained model will be saved
bucket = sagemaker.Session().default_bucket()
prefix = 'DEMO-xgboost-as-a-built-in-algo'
output_path = 's3://<span>{</span>}/<span>{</span>}/<span>{</span>}/output'.format(bucket, prefix, 'abalone-xgb-built-in-algo')

# this line automatically looks for the XGBoost image URI and builds an XGBoost container.
# specify the repo_version depending on your preference.
xgboost_container = sagemaker.image_uris.retrieve("xgboost", region, "<code class="replaceable">1.7-1</code>")

# construct a SageMaker estimator that calls the xgboost-container
estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, 
                                          hyperparameters=hyperparameters,
                                          role=sagemaker.get_execution_role(),
                                          instance_count=1, 
                                          instance_type='ml.m5.2xlarge', 
                                          volume_size=5, # 5 GB 
                                          output_path=output_path)

# define the data type and paths to the training and validation datasets
content_type = "libsvm"
train_input = TrainingInput("s3://<span>{</span>}/<span>{</span>}/<span>{</span>}/".format(bucket, prefix, 'train'), content_type=content_type)
validation_input = TrainingInput("s3://<span>{</span>}/<span>{</span>}/<span>{</span>}/".format(bucket, prefix, 'validation'), content_type=content_type)

# execute the XGBoost training job
estimator.fit(<span>{</span>'train': train_input, 'validation': validation_input})</code></pre>
                <p>For more information about how to set up the XGBoost as a built-in algorithm,
                    see the following notebook examples.</p>
                <div class="itemizedlist">
                     
                     
                <ul class="itemizedlist"><li class="listitem">
                        <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_managed_spot_training.html" rel="noopener noreferrer" target="_blank"><span> Managed Spot Training for XGBoost </span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                    </li><li class="listitem">
                        <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_parquet_input_training.html" rel="noopener noreferrer" target="_blank"><span> Regression with Amazon SageMaker XGBoost (Parquet input) </span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                    </li></ul></div>
            </li></ul></div>
     
        <h2 id="InputOutput-XGBoost">Input/Output Interface for the XGBoost
                Algorithm</h2>
        <p>Gradient boosting operates on tabular data, with the rows representing observations,
            one column representing the target variable or label, and the remaining columns
            representing features. </p>
        <p>The SageMaker implementation of XGBoost supports the following data formats for training
            and inference:</p>
        <div class="itemizedlist">
             
             
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p>
                    <em>text/libsvm</em> (default) </p>
            </li><li class="listitem">
                <p>
                    <em>text/csv</em></p>
            </li><li class="listitem">
                <p>
                    <em>application/x-parquet</em></p>
            </li><li class="listitem">
                <p>
                    <em>application/x-recordio-protobuf</em></p>
            </li></ul></div>
        <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>There are a few considerations to be aware of regarding training and inference
                input:</p><div class="itemizedlist">
                     
                     
                     
                     
                <ul class="itemizedlist"><li class="listitem">
                    <p>For training with columnar input, the algorithm assumes that the target
                        variable (label) is the first column. For inference, the algorithm assumes
                        that the input has no label column.</p></li><li class="listitem"><p>For CSV data, the input should not have a header record.</p></li><li class="listitem"><p>For LIBSVM training, the algorithm assumes that subsequent columns after the label column
                        contain the zero-based index value pairs for features. So each row has the
                        format: : &lt;label&gt; &lt;index0&gt;:&lt;value0&gt;
                        &lt;index1&gt;:&lt;value1&gt;.</p></li><li class="listitem">
                        <p>For information on instance types and distributed training, see <a href="xgboost.html#Instance-XGBoost">EC2 Instance Recommendation for the XGBoost
                Algorithm</a>.</p></li></ul></div></div></div>
        <p>For CSV training input mode, the total memory available to the algorithm (Instance
            Count * the memory available in the <code class="code">InstanceType</code>) must be able to hold the
            training dataset. For libsvm training input mode, it's not required, but we recommend
            it.</p>
        <p>For v1.3-1 and later, SageMaker XGBoost saves the model in the XGBoost internal binary
            format, using <code class="code">Booster.save_model</code>. Previous versions use the Python pickle
            module to serialize/deserialize the model.</p>
        <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Be mindful of versions when using an SageMaker XGBoost model in open source XGBoost. Versions 1.3-1 and later use the XGBoost internal binary format while previous versions use the Python pickle module.</p></div></div>
        <div class="procedure"><h6>To use a model trained with SageMaker XGBoost v1.3-1 or later in open source
                XGBoost</h6><ul><li>
                <p>Use the following Python code:</p>
                <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">import xgboost as xgb

xgb_model = xgb.Booster()
xgb_model.load_model(<code class="replaceable">model_file_path</code>)
xgb_model.predict(<code class="replaceable">dtest</code>)</code></pre>
            </li></ul></div>
        <div class="procedure"><h6>To use a model trained with previous versions of SageMaker XGBoost in open source
                XGBoost</h6><ul><li>
                <p>Use the following Python code:</p>
                <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">import pickle as pkl 
import tarfile

t = tarfile.open('model.tar.gz', 'r:gz')
t.extractall()

model = pkl.load(open(<code class="replaceable">model_file_path</code>, 'rb'))

# prediction with test data
pred = model.predict(<code class="replaceable">dtest</code>)</code></pre>
            </li></ul></div>
        <div class="procedure"><h6>To differentiate the importance of labelled data points use Instance Weight
                Supports</h6><ul><li>
                <p>SageMaker XGBoost allows customers to differentiate the importance of labelled data
                    points by assigning each instance a weight value. For
                        <em>text/libsvm</em> input, customers can assign weight values
                    to data instances by attaching them after the labels. For example,
                        <code class="code">label:weight idx_0:val_0 idx_1:val_1...</code>. For
                        <em>text/csv</em> input, customers need to turn on the
                        <code class="code">csv_weights</code> flag in the parameters and attach weight values in
                    the column after labels. For example:
                    <code class="code">label,weight,val_0,val_1,...</code>).</p>
            </li></ul></div>
     
        <h2 id="Instance-XGBoost">EC2 Instance Recommendation for the XGBoost
                Algorithm</h2>
        <p>SageMaker XGBoost supports CPU and GPU training and inference. Instance recommendations
            depend on training and inference needs, as well as the version of the XGBoost algorithm.
            Choose one of the following options for more information:</p>
        <div class="itemizedlist">
             
             
             
             
             
        <ul class="itemizedlist"><li class="listitem"><p><a href="xgboost.html#Instance-XGBoost-training-cpu">CPU training</a></p></li><li class="listitem"><p><a href="xgboost.html#Instance-XGBoost-training-gpu">GPU training</a></p></li><li class="listitem"><p><a href="xgboost.html#Instance-XGBoost-distributed-training-cpu">Distributed CPU training</a></p></li><li class="listitem"><p><a href="xgboost.html#Instance-XGBoost-distributed-training-gpu">Distributed GPU training</a></p></li><li class="listitem"><p><a href="xgboost.html#Instance-XGBoost-inference">Inference</a></p></li></ul></div>
         
            <h3 id="Instance-XGBoost-training">Training</h3>
            <p>The SageMaker XGBoost algorithm supports CPU and GPU training.</p>
             
            <h4 id="Instance-XGBoost-training-cpu">CPU training</h4>
                <p>SageMaker XGBoost 1.0-1 or earlier only trains using CPUs. It is a memory-bound (as opposed
                    to compute-bound) algorithm. So, a general-purpose compute instance (for example, M5) is
                    a better choice than a compute-optimized instance (for example, C4). Further, we
                    recommend that you have enough total memory in selected instances to hold the training
                    data. Although it supports the use of disk space to handle data that does not fit into
                    main memory (the out-of-core feature available with the libsvm input mode), writing
                    cache files onto disk slows the algorithm processing time.</p>
         
             
                <h4 id="Instance-XGBoost-training-gpu">GPU training</h4>
            <p>SageMaker XGBoost version 1.2-2 or later supports GPU training. Despite higher
                per-instance costs, GPUs train more quickly, making them more cost effective. </p>
        <p>SageMaker XGBoost version 1.2-2 or later supports P2, P3, G4dn, and G5 GPU instance families.</p>
        <p>SageMaker XGBoost version 1.7-1 or later supports P3, G4dn, and G5 GPU instance families. Note that due to compute capacity requirements, version 1.7-1 or later does not support the P2 instance family.</p>
        <p>To take advantage of GPU training, specify the instance type as one of the GPU
                instances (for example, P3) and set the <code class="code">tree_method</code> hyperparameter to
                    <code class="code">gpu_hist</code> in your existing XGBoost script. </p>
             
         
         
            <h3 id="Instance-XGBoost-distributed-training">Distributed training</h3>
            <p>SageMaker XGBoost supports CPU and GPU instances for distributed training.</p>
             
                <h4 id="Instance-XGBoost-distributed-training-cpu">Distributed CPU training</h4>
                <p>To run CPU training on multiple instances, set the <code class="code">instance_count</code>
                    parameter for the estimator to a value greater than one. The input data must be
                    divided between the total number of instances. </p>
                 
                    <h5 id="Instance-XGBoost-distributed-training-divide-data">Divide input data across instances</h5>
                    <p>Divide the input data using the following steps:</p>
                <div class="procedure"><ol><li>
                        <p>Break the input data down into smaller files. The number of files
                            should be at least equal to the number of instances used for distributed
                            training. Using multiple smaller files as opposed to one large file also
                            decreases the data download time for the training job.</p></li><li>
                        <p>When creating your <a href="https://sagemaker.readthedocs.io/en/stable/api/utility/inputs.html" rel="noopener noreferrer" target="_blank"><span>TrainingInput</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>, set the distribution parameter to
                                <code class="code">ShardedByS3Key</code>. This parameter ensures that each
                            instance gets approximately <em>1/n</em> of the number of
                            files in S3 if there are <em>n</em> instances specified in
                            the training job.</p></li></ol></div>
             
             
                <h4 id="Instance-XGBoost-distributed-training-gpu">Distributed GPU training</h4>
                <p>You can use distributed training with either single-GPU or multi-GPU instances.</p>
                <p><b>Distributed training with single-GPU instances
                    </b></p>
                <p>SageMaker XGBoost versions 1.2-2 through 1.3-1 only support single-GPU
                    instance training. This means that even if you select a multi-GPU instance, only
                    one GPU is used per instance.</p>
                <p>If you use XGBoost versions 1.2-2 through 1.3-1, or if you do not need to use
                    multi-GPU instances, then you must divide your input data between the total
                    number of instances. For more information, see <a href="xgboost.html#Instance-XGBoost-distributed-training-divide-data">Divide input data across instances</a>.</p>
                <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>Versions 1.2-2 through 1.3-1 of SageMaker XGBoost only use one GPU per instance
                    even if you choose a multi-GPU instance.</p></div></div>
                <p><b>Distributed training with multi-GPU
                    instances</b></p>
                <p>Starting with version 1.5-1, SageMaker XGBoost offers distributed GPU training
                    with <a href="https://www.dask.org/" rel="noopener noreferrer" target="_blank"><span>Dask</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. With Dask you can utilize
                    all GPUs when using one or more multi-GPU instances. Dask also works when using
                    single-GPU instances. </p>
                <p>Train with Dask using the following steps:</p>
                <div class="procedure"><ol><li><p>Either omit the <code class="code">distribution</code> parameter in your <a href="https://sagemaker.readthedocs.io/en/stable/api/utility/inputs.html" rel="noopener noreferrer" target="_blank"><span>TrainingInput</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> or set it to
                            <code class="code">FullyReplicated</code>.</p></li><li><p>When defining your hyperparameters, set <code class="code">use_dask_gpu_training</code> to
                    <code class="code">"true"</code>.</p></li></ol></div>
                <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>Distributed training with Dask only supports CSV and Parquet input formats. If you use other
                        data formats such as LIBSVM or PROTOBUF, the training job fails. </p><p>For Parquet data, ensure that the column names are saved as strings.
                        Columns that have names of other data types will fail to load.</p></div></div>
                <div class="awsdocs-note awsdocs-important"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Important</h6></div><div class="awsdocs-note-text"><p>Distributed training with Dask does not support pipe mode. If pipe mode is
                    specified, the training job fails.</p></div></div>
                <p>There are a few considerations to be aware of when training SageMaker XGBoost with
                    Dask. Be sure to split your data into smaller files. Dask reads each Parquet file as a
                    partition. There is a Dask worker for every GPU, so the number of files should
                    be greater than the total number of GPUs (instance count * number of GPUs per
                    instance). Having a very large number of files can also degrade performance. For
                    more information, see <a href="https://docs.dask.org/en/stable/best-practices.html" rel="noopener noreferrer" target="_blank"><span>Dask Best
                        Practices</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
             
             
                <h4 id="Instance-XGBoost-distributed-training-output">Variations in output</h4>
                <p>The specified <code class="code">tree_method</code> hyperparameter determines the algorithm
                    that is used for XGBoost training. The tree methods <code class="code">approx</code>,
                        <code class="code">hist</code> and <code class="code">gpu_hist</code> are all approximate methods and
                    use sketching for quantile calculation. For more information, see <a href="https://xgboost.readthedocs.io/en/stable/treemethod.html" rel="noopener noreferrer" target="_blank"><span>Tree
                        Methods</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> in the XGBoost documentation. Sketching is an approximate
                    algorithm. Therefore, you can expect variations in the model depending on
                    factors such as the number of workers chosen for distributed training. The
                    significance of the variation is data-dependent.</p>
             
         
         
            <h3 id="Instance-XGBoost-inference">Inference</h3>
            <p>SageMaker XGBoost supports CPU and GPU instances for inference. For information about the
            instance types for inference, see <a href="http://aws.amazon.com/sagemaker/pricing/instance-types/" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker ML Instance
                Types</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
     
        <h2 id="xgboost-sample-notebooks">XGBoost Sample Notebooks</h2>

        <p>
            The following table outlines a variety of sample notebooks that address different use cases of Amazon SageMaker XGBoost algorithm.</p>
            <div class="table-container"><div class="table-contents"><table id="w714aac23c11c21c31c24c15b5"><thead>
                        <tr>
                            <th><b>Notebook Title</b></th>
                            <th><b>Description</b></th>
                        </tr>
                    </thead>
                        <tr>
                            <td tabindex="-1">
                                <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/aws_sagemaker_studio/sagemaker_studio_image_build/xgboost_bring_your_own/Batch_Transform_BYO_XGB.html" rel="noopener noreferrer" target="_blank"><span>How to Create a Custom XGBoost container?</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                        </td>
                        <td tabindex="-1">
                            <p>This notebook shows you how to build a custom XGBoost Container
                                with Amazon SageMaker Batch Transform.</p>
                        </td>
                    </tr>
                    <tr>
                        <td tabindex="-1">
                            <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_parquet_input_training.html" rel="noopener noreferrer" target="_blank"><span>Regression with XGBoost using Parquet</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                        </td>
                        <td tabindex="-1">
                            <p>This notebook shows you how to use the Abalone dataset in Parquet
                                to train a XGBoost model.</p>
                        </td>
                    </tr>
                    <tr>
                        <td tabindex="-1">
                            <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/xgboost_mnist/xgboost_mnist.html" rel="noopener noreferrer" target="_blank"><span>How to Train and Host a Multiclass Classification Model?
                                </span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                        </td>
                        <td tabindex="-1">
                            <p>This notebook shows how to use the MNIST dataset to train and host
                                a multiclass classification model.</p>
                        </td>
                    </tr>
                    <tr>
                        <td tabindex="-1">
                            <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_applying_machine_learning/xgboost_customer_churn/xgboost_customer_churn.html" rel="noopener noreferrer" target="_blank"><span>How to train a Model for Customer Churn
                                Prediction?</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                        </td>
                        <td tabindex="-1">
                            <p>This notebook shows you how to train a model to Predict Mobile
                                Customer Departure in an effort to identify unhappy
                                customers.</p>
                        </td>
                    </tr>
                    <tr>
                        <td tabindex="-1">
                            <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_managed_spot_training.html" rel="noopener noreferrer" target="_blank"><span>An Introduction to Amazon SageMaker Managed Spot infrastructure for
                                    XGBoost Training</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                        </td>
                        <td tabindex="-1">
                            <p>This notebook shows you how to use Spot Instances for training
                                with a XGBoost Container.</p>
                        </td>
                    </tr>
                    <tr>
                        <td tabindex="-1">
                            <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-debugger/xgboost_census_explanations/xgboost-census-debugger-rules.html" rel="noopener noreferrer" target="_blank"><span>How to use Amazon SageMaker Debugger to debug XGBoost Training
                                    Jobs?</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                        </td>
                        <td tabindex="-1">
                            <p>This notebook shows you how to use Amazon SageMaker Debugger to monitor
                                training jobs to detect inconsistencies using built-in debugging
                                rules.</p>
                        </td>
                    </tr>
                    <tr>
                        <td tabindex="-1">
                            <p><a href="https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-debugger/xgboost_realtime_analysis/xgboost-realtime-analysis.html" rel="noopener noreferrer" target="_blank"><span>How to use Amazon SageMaker Debugger to debug XGBoost Training Jobs in
                                    Real-Time?</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></p>
                        </td>
                        <td tabindex="-1">
                            <p>This notebook shows you how to use the MNIST dataset and Amazon SageMaker
                                Debugger to perform real-time analysis of XGBoost training jobs
                                while training jobs are running.</p>
                        </td>
                    </tr>
                </table></div></div>
        <p>For instructions on how to create and access Jupyter notebook instances that you can
            use to run the example in SageMaker, see <a href="nbi.html">Amazon SageMaker Notebook Instances</a>. After you
            have created a notebook instance and opened it, choose the <b>SageMaker
                Examples</b> tab to see a list of all of the SageMaker samples. The topic modeling
            example notebooks using the linear learning algorithm are located in the <b>Introduction to Amazon algorithms</b> section. To open a
            notebook, choose its <b>Use</b> tab and choose <b>Create copy</b>.</p>

    <awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="https://docs.aws.amazon.com/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./tabtransformer-tuning.html">Model Tuning</div><div id="next" class="next-link" accesskey="n" href="./xgboost-HowItWorks.html">How It Works</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/xgboost.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/xgboost.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>