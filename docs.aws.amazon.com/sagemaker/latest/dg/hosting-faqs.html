<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Model Hosting FAQs - Amazon SageMaker</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="hosting-faqs" /><meta name="default_state" content="hosting-faqs" /><link rel="icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="canonical" href="hosting-faqs.html" /><meta name="description" content="FAQs for SageMaker Inference Hosting, including all of the inference options." /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon SageMaker" /><meta name="guide" content="Developer Guide" /><meta name="abstract" content="Use Internet Monitor to build, train, and host machine learning models in AWS." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="hosting-faqs.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/hosting-faqs.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/hosting-faqs.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/hosting-faqs.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/hosting-faqs.html" hreflang="de" /><link rel="alternative" href="hosting-faqs.html" hreflang="en-us" /><link rel="alternative" href="hosting-faqs.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/hosting-faqs.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/hosting-faqs.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/hosting-faqs.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/hosting-faqs.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/hosting-faqs.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/hosting-faqs.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/hosting-faqs.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/hosting-faqs.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/hosting-faqs.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/hosting-faqs.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/hosting-faqs.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/hosting-faqs.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/hosting-faqs.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/hosting-faqs.html" hreflang="zh-tw" /><link rel="alternative" href="hosting-faqs.html" hreflang="x-default" /><meta name="feedback-item" content="SageMaker" /><meta name="this_doc_product" content="Amazon SageMaker" /><meta name="this_doc_guide" content="Developer Guide" /><script defer="" src="../../../assets/r/vendor4.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor3.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor1.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-common.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-doc-page.js@version=2021.12.02"></script><link href="../../../assets/r/vendor4.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-common.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-doc-page.css@version=2021.12.02.css" rel="stylesheet" /><script async="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'sagemaker'}"></script><meta id="panorama-serviceSubSection" value="Developer Guide" /><meta id="panorama-serviceConsolePage" value="Model Hosting FAQs" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>Model Hosting FAQs - Amazon SageMaker</title><meta name="pdf" content="/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#hosting-faqs" /><meta name="rss" content="amazon-sagemaker-release-notes.rss" /><meta name="forums" content="http://forums.aws.amazon.com/forum.jspa?forumID=285" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/hosting-faqs.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/hosting-faqs.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/hosting-faqs.html" /><meta name="keywords" content="SageMaker,Amazon SageMaker,machine learning,notebook instance,inference,deploy model,endpoint,prediction,ML application,serverless machine learning,multi model deployment,inference pipelines,ML model,troubleshooting,resources,reference,hosting,FAQs" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "Developer Guide",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Deploy models for inference",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html"
      },
      {
        "@type" : "ListItem",
        "position" : 5,
        "name" : "Resources",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/inference-resources.html"
      },
      {
        "@type" : "ListItem",
        "position" : 6,
        "name" : "Model Hosting FAQs",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/inference-resources.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#hosting-faqs" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="https://aws.amazon.com">AWS</a><a href="https://docs.aws.amazon.com/index.html">Documentation</a><a href="https://docs.aws.amazon.com/sagemaker/index.html">Amazon SageMaker</a><a href="whatis.html">Developer Guide</a></div><div id="page-toc-src"><a href="hosting-faqs.html#hosting-faqs-general">General Hosting</a><a href="hosting-faqs.html#hosting-faqs-real-time">Real-Time Inference</a><a href="hosting-faqs.html#hosting-faqs-serverless">Serverless Inference</a><a href="hosting-faqs.html#hosting-faqs-batch">Batch Transform</a><a href="hosting-faqs.html#hosting-faqs-async">Asynchronous Inference</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="hosting-faqs">Model Hosting FAQs</h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>Refer to the following FAQ items for answers to commonly asked questions about SageMaker Inference Hosting.</p>
     <h2 id="hosting-faqs-general">General Hosting</h2>
     <p>The following FAQ items answer common general questions for SageMaker Inference.</p>
     <div class="collapsible"><awsui-expandable-section variant="container" header="Q: What deployment options does Amazon SageMaker provide?" id="hosting-faqs-general-1" expanded="true"><p>A: After you build and train models, Amazon SageMaker provides four options to deploy
                    them so you can start making predictions. Real-Time Inference is suitable for
                    workloads with millisecond latency requirements, payload sizes up to 6 MB, and
                    processing times of up to 60 seconds. Batch Transform is ideal for offline
                    predictions on large batches of data that are available up front. Asynchronous Inference is
                    designed for workloads that do not have sub-second latency requirements, payload
                    sizes up to 1 GB, and processing times of up to 15 minutes. With Serverless Inference, you can
                    quickly deploy machine learning models for inference without having to configure
                    or manage the underlying infrastructure, and you pay only for the compute
                    capacity used to process inference requests, which is ideal for intermittent
                    workloads.</p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: How do I choose a model deployment option in SageMaker?" id="hosting-faqs-general-2" expanded="false"><p>A: The following diagram can help you choose a SageMaker Hosting model deployment option.</p><div class="mediaobject">
                     
                        <img src="../../../images/sagemaker/latest/dg/images/inference-hosting-options.png" class="aws-docs-img-whiteBg aws-docs-img-padding" alt="&#xA;                        Flowchart explaining how to choose a model deployment option in&#xA;                            SageMaker, described in the following paragraph.&#xA;                    " style="max-width:80%" />
                     
                     
                </div><p>The preceding diagram walks you through the following decision process. If you
                    want to process requests in batches, you might want to choose Batch Transform.
                    Otherwise, if you want to receive inference for each request to your model, you
                    might want to choose Asynchronous Inference, Serverless Inference, or Real-Time Inference. You can choose Asynchronous Inference
                    if you have long processing times or large payloads and want to queue requests.
                    You can choose Serverless Inference if your workload has unpredictable or intermittent traffic.
                    You can choose Real-Time Inference if you have sustained traffic and need lower
                    and consistent latency for your requests.</p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: I’ve heard SageMaker Inference is expensive. What’s the best way to optimize my cost when hosting models?" id="hosting-faqs-general-3" expanded="false"><p>A: To optimize your costs with SageMaker Inference, you should choose the right
                    hosting option for your use case. You can also use Inference features such as
                    <a href="http://aws.amazon.com/savingsplans/ml-pricing/" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker Savings Plans</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>,
                 model optimization with
                 <a href="neo.html">SageMaker Neo</a>,
                 <a href="multi-model-endpoints.html">Multi-Model Endpoints</a>
                 and <a href="multi-container-endpoints.html">Multi-Container Endpoints</a>,
                 or autoscaling. For tips on how to
                    optimize your Inference costs, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/inference-cost-optimization.html">Inference cost optimization best practices</a>.</p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: Why should I use Amazon SageMaker Inference Recommender?" id="hosting-faqs-general-4" expanded="false"><p>A: You should use Amazon SageMaker Inference Recommender if you need recommendations for the right
                    endpoint configuration to improve performance and reduce costs. Previously, data
                    scientists who wanted to deploy their models had to run manual benchmarks to
                    select the right endpoint configuration. First, they had to select the right
                    machine learning instance type out of more than 70 available instance types
                    based on the resource requirements of their models and sample payloads, and then
                    optimize the model to account for differing hardware. Then, they had to conduct
                    extensive load tests to validate that latency and throughput requirements were
                    met and that the costs were low. Inference Recommender eliminates this complexity by helping
                    you do the following: </p><div class="itemizedlist">
                     
                     
                     
                <ul class="itemizedlist"><li class="listitem">
                        <p>Get started in minutes with an instance recommendation.</p>
                    </li><li class="listitem">
                        <p>Conduct load tests across instance types to get recommendations on
                            your endpoint configuration within hours. </p>
                    </li><li class="listitem">
                        <p>Automatically tune container and model server parameters as well as
                            perform model optimizations for a given instance type.</p>
                    </li></ul></div></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: What is a model server?" id="hosting-faqs-general-5" expanded="false"><p>A: SageMaker endpoints are HTTP REST endpoints that use a containerized web server,
                    which includes a model server. These containers are responsible for loading up
                    and serving requests for a machine learning model. They implement a web server
                    that responds to <code class="code">/invocations</code> and <code class="code">/ping</code> on port
                    8080.</p><p>Common model servers include TensorFlow Serving, TorchServe and Multi Model Server.
                    SageMaker framework containers have these model servers built in.</p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: What is Bring Your Own Container with Amazon SageMaker?" id="hosting-faqs-general-6" expanded="false"><p>A: Everything in SageMaker Inference is containerized. SageMaker provides managed containers for popular frameworks
                 such as TensorFlow, SKlearn, and HuggingFace. For a comprehensive updated list of those images, see
                 <a href="https://github.com/aws/deep-learning-containers/blob/master/available_images.md" rel="noopener noreferrer" target="_blank"><span>Available Images</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p><p> Sometimes there are custom frameworks for which you might need to build a
                    container. This approach is known as <em>Bring Your Own
                        Container</em> or <em>BYOC</em>. With the
                    BYOC approach, you provide the Docker image to set up your framework or library.
                    Then, you push the image to Amazon Elastic Container Registry (Amazon ECR) so that you can use the image with
                    SageMaker. For an example of a BYOC approach, see <a href="https://sagemaker-workshop.com/custom/containers.html" rel="noopener noreferrer" target="_blank"><span>Overivew of
                        Containers for Amazon SageMaker</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p><p>Alternatively, instead of building an image from scratch, you can extend a container. You can take one of the
                 base images that SageMaker provides and add your dependencies on top of it in your Dockerfile.</p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: Do I need to train my models on SageMaker to host them on SageMaker endpoints?" id="hosting-faqs-general-7" expanded="false"><p>A: SageMaker offers the capacity to bring your own trained framework model that you've
                    trained outside of SageMaker and deploy it on any of the SageMaker hosting options.</p><p>SageMaker requires you to package the model in a <code class="code">model.tar.gz</code> file and
                    have a specific directory structure. Each framework has its own model structure
                    (see the following question for example structures). For more information, see
                    the SageMaker Python SDK documentation for <a href="https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html#deploying-directly-from-model-artifacts" rel="noopener noreferrer" target="_blank"><span>TensorFlow</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>, <a href="https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#bring-your-own-model" rel="noopener noreferrer" target="_blank"><span>PyTorch</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>, and <a href="https://sagemaker.readthedocs.io/en/stable/frameworks/mxnet/using_mxnet.html#deploy-endpoints-from-model-data" rel="noopener noreferrer" target="_blank"><span>MXNet</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p><p>While you can choose from prebuilt framework images such as TensorFlow, PyTorch, and
                    MXNet to host your trained model, you can also build your own container to host
                    your trained models on SageMaker endpoints. For a walkthrough, see the example Jupyter
                    notebook <a href="https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb" rel="noopener noreferrer" target="_blank"><span>Building your own algorithm container</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: How should I structure my model if I want to deploy on SageMaker but not train on SageMaker? " id="hosting-faqs-general-8" expanded="false"><p>A: SageMaker requires your model artifacts to be compressed in a <code class="code">.tar.gz</code> file, or a <em>tarball</em>. SageMaker automatically extracts this <code class="code">.tar.gz</code>
                 file into the <code class="code">/opt/ml/model/</code> directory in your container. The tarball shouldn't contain any symlinks or unncessary files. If you are making use of one of the framework containers,
                 such as TensorFlow, PyTorch, or MXNet, the container expects your TAR structure to be as follows: </p><p><b>TensorFlow</b></p><pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="">model.tar.gz/
             |--[model_version_number]/
                                       |--variables
                                       |--saved_model.pb
            code/
                |--inference.py
                |--requirements.txt</code></pre><p><b>PyTorch</b></p><pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="">model.tar.gz/
             |- model.pth
             |- code/
                     |- inference.py
                     |- requirements.txt  # only for versions 1.3.1 and higher</code></pre><p><b>MXNet</b></p><pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="">model.tar.gz/
            |- model-symbol.json
            |- model-shapes.json
            |- model-0000.params
            |- code/
                    |- inference.py
                    |- requirements.txt # only for versions 1.6.0 and higher</code></pre></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: When invoking a SageMaker endpoint, I can provide a ContentType&#xA;                 and Accept MIME Type. Which one is used to identify the data type being sent and received?" id="hosting-faqs-general-10" expanded="false"><p>A: <code class="code">ContentType</code> is the MIME type of the input data in the request body (the MIME type of the data you are sending to your endpoint).
                 The model server uses the <code class="code">ContentType</code> to determine if it can handle the type provided or not.</p><p><code class="code">Accept</code> is the MIME type of the inference response (the MIME type of the data your endpoint returns).
                 The model server uses the <code class="code">Accept</code> type to determine if it can handle returning the type provided or not.</p><p>Common MIME types include <code class="code">text/csv</code>, <code class="code">application/json</code>,
                    and <code class="code">application/jsonlines</code>.</p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: What are the supported data formats for SageMaker Inference?" id="hosting-faqs-general-12" expanded="false"><p>A: SageMaker passes any request onto the model container without modification. The container must contain the logic to deserialize the request.
                 For information about the formats defined for built-in algorithms, see <a href="cdf-inference.html">
                     Common Data Formats for Inference</a>. If you are building your own container or using a SageMaker Framework container, you can include the logic to accept a request format of your choice.</p><p>Similarly, SageMaker also returns the response without modification, and then the client must deserialize the response. In case of the built-in algorithms,
                 they return responses in specific formats. If you are building your own container or using a SageMaker Framework container, you can include the logic to return a response in the format you choose.</p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: How do I invoke my endpoint with binary data such as videos or images?" id="hosting-faqs-general-11" expanded="false"><p>Use the <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_runtime_InvokeEndpoint.html">Invoke Endpoint</a>
                 API call to make inference against your endpoint.</p><p>When passing your input as a payload to the <code class="code">InvokeEndpoint</code> API, you
                    must provide the correct type of input data that your model expects. When
                    passing a payload in the <code class="code">InvokeEndpoint</code> API call, the request bytes
                    are forwarded directly to the model container. For example, for an image, you
                    may use <code class="code">application/jpeg</code> for the <code class="code">ContentType</code>, and make
                    sure that your model can perform inference on this type of data. This applies
                    for JSON, CSV, video, or any other type of input with which you may be
                    dealing.</p><p>Another factor to consider is payload size limits. In terms of real-time and serverless endpoints, the payload limit is 6 MB.
                 You can split your video into multiple frames and invoke the endpoint with each frame individually. Alternatively, if your use case permits,
                 you can send the whole video in the payload using an asynchronous endpoint, which supports up to 1 GB payloads.</p><p>For an example that showcases how to run computer vision inference on large videos with Asynchronous Inference, see this
                 <a href="http://aws.amazon.com/blogs/machine-learning/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints/" rel="noopener noreferrer" target="_blank"><span>blog post</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p></awsui-expandable-section></div>
  
        <h2 id="hosting-faqs-real-time">Real-Time Inference</h2>
        <p>The following FAQ items answer common questions for SageMaker Real-Time Inference.</p>
        <div class="collapsible"><awsui-expandable-section variant="container" header="Q: How do I create a SageMaker endpoint?" id="hosting-faqs-real-time-1" expanded="true"><p>A: You can create a SageMaker endpoint through AWS-supported tooling such as the AWS SDKs, the SageMaker Python SDK,
                    the AWS Management Console, AWS CloudFormation, and the AWS Cloud Development Kit (AWS CDK).</p><p>There are three key entities in endpoint creation: a SageMaker model, a SageMaker endpoint configuration, and a SageMaker endpoint.
                    The SageMaker model points towards the model data and image you are using. The endpoint configuration defines your production variants,
                    which might include the instance type and instance count. You can then use either the
                    <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint" rel="noopener noreferrer" target="_blank"><span>create_endpoint</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>
                    API call or the <a href="https://sagemaker.readthedocs.io/en/stable/api/inference/model.html" rel="noopener noreferrer" target="_blank"><span>.deploy()</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> call for
                    SageMaker to create an endpoint using the metadata from your model and endpoint configuration.</p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: Do I need to use the SageMaker Python SDK to create/invoke endpoints? " id="hosting-faqs-real-time-2" expanded="false"><p>A: No, you can use the various AWS SDKs (see <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_runtime_InvokeEndpoint.html#API_runtime_InvokeEndpoint_SeeAlso">Invoke</a>/<a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateEndpoint.html#API_CreateEndpoint_SeeAlso">Create</a> for available SDKs) or even call the corresponding web APIs
                    directly.</p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: What is the difference between Multi-Model Endpoints (MME) and Multi Model Server (MMS)?" id="hosting-faqs-real-time-3" expanded="false"><p>A: A Multi-Model Endpoint is a Real-Time Inference option that SageMaker provides.
                    With Multi-Model Endpoints, you can host thousands of models behind one
                    endpoint. <a href="https://github.com/awslabs/multi-model-server" rel="noopener noreferrer" target="_blank"><span>Multi Model
                        Server</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> is an open-source framework for serving machine learning
                    models. It provides the HTTP front-end and model management capabilities
                    required by multi-model endpoints to host multiple models within a single
                    container, load models into and unload models out of the container dynamically,
                    and perform inference on a specified loaded model.</p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: What are the different model deployment architectures supported by Real-Time Inference?" id="hosting-faqs-real-time-4" expanded="false"><p>A: SageMaker Real-Time Inference supports various model deployment architecture
                    such as Multi-Model Endpoints, Multi-Container Endpoints, and Serial Inference Pipelines. </p><p><a href="multi-model-endpoints.html">Multi-Model Endpoints (MME)</a>
                    – MME allows customers to deploy 1000s of hyper‐personalized models
                    in a cost effective way. All the models are deployed on a shared‐resource
                    fleet. MME works best when the models are of similar size and latency and belong
                    to the same ML framework. These endpoints are ideal for when you have don’t need
                    to call the same model at all times. You can dynamically load respective models
                    onto the SageMaker endpoint to serve your request.</p><p><a href="multi-container-endpoints.html">Multi-Container
                        Endpoints (MCE)</a> – MCE allows customers to deploy 15 different
                    containers with diverse ML frameworks and functionalities with no cold starts
                    while only using one SageMaker endpoint. You can directly invoke these
                    containers. MCE is best for when you want to keep all the models in
                    memory.</p><p><a href="inference-pipelines.html">Serial Inference Pipelines (SIP)</a> – You can use SIP to chain
                    together 2‐15 containers on a single endpoint. SIP is mostly suitable for
                    combining preprocessing and model inference in one endpoint and for low latency
                    operations.</p></awsui-expandable-section></div>
     
        <h2 id="hosting-faqs-serverless">Serverless Inference</h2>
        <p>The following FAQ items answer common questions for Amazon SageMaker Serverless Inference.</p>
        <div class="collapsible"><awsui-expandable-section variant="container" header="Q: What is Amazon SageMaker Serverless Inference?" id="hosting-faqs-serverless-1" expanded="true"><p>A: <a href="serverless-endpoints.html">Serverless Inference</a>
                    is a purpose-built serverless model serving option that makes it easy to deploy and scale ML models.
                    Serverless Inference endpoints automatically start compute resources and scale them in and out depending on traffic,
                    eliminating the need for you to choose instance type, run provisioned capacity, or manage scaling.
                    You can optionally specify the memory requirements for your serverless endpoint.
                    You pay only for the duration of running the inference code and the amount of data processed, not for idle periods.</p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: Why should I use Serverless Inference?" id="hosting-faqs-serverless-2" expanded="false"><p>A: Serverless Inference simplifies the developer experience by eliminating the need to provision capacity
                    up front and manage scaling policies. Serverless Inference can scale instantly from tens to thousands of inferences
                    within seconds based on the usage patterns, making it ideal for ML applications with intermittent or unpredictable traffic.
                    For example, a chatbot service used by a payroll processing company experiences an increase in inquiries at the end of the
                    month while traffic is intermittent for rest of the month. Provisioning instances for the entire month in
                    such scenarios is not cost-effective, as you end up paying for idle periods.</p><p>Serverless Inference helps address these types of use cases by providing you automatic and fast scaling out of the box
                    without the need for you to forecast traffic up front or manage scaling policies. Additionally, you pay only for
                    the compute time to run your inference code and for data processing, making it ideal for workloads with intermittent traffic.</p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: How do I choose the right memory size for my serverless endpoint?" id="hosting-faqs-serverless-3" expanded="false"><p>A: Your serverless endpoint has a minimum RAM size of 1024 MB (1 GB), and the maximum RAM size you can choose is 6144 MB (6 GB).
                    The memory sizes you can choose are 1024 MB, 2048 MB, 3072 MB, 4096 MB, 5120 MB, or 6144 MB. Serverless Inference auto-assigns compute
                    resources proportional to the memory you select. If you choose a larger memory size, your container has access to more vCPUs.</p><p>Choose your endpoint’s memory size according to your model size. Generally, the memory size should be at least as large as your model size.
                    You may need to benchmark in order to choose the right memory selection for your model based on your latency SLAs.
                    The memory size increments have different pricing; see the <a href="http://aws.amazon.com/sagemaker/pricing/" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker pricing page</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> for more information.</p></awsui-expandable-section></div>
     
        <h2 id="hosting-faqs-batch">Batch Transform</h2>
        <p>The following FAQ items answer common questions for SageMaker Batch Transform.</p>
        <div class="collapsible"><awsui-expandable-section variant="container" header="Q: How does Batch Transform split my data? " id="hosting-faqs-batch-1" expanded="true"><p>A: For specific file formats such as CSV, RecordIO and TFRecord, SageMaker can
                    split your data into single-record or multi-record mini batches and send this as
                    a payload to your model container. When the value of <code class="code"><a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTransformJob.html#sagemaker-CreateTransformJob-request-BatchStrategy">BatchStrategy</a></code> is <code class="code">MultiRecord</code>, SageMaker sends
                    the maximum number of records in each request, up to the
                        <code class="code">MaxPayloadInMB</code> limit. When the value of
                        <code class="code">BatchStrategy</code> is <code class="code">SingleRecord</code>, SageMaker sends
                    individual records in each request.</p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: What is the maximum timeout for Batch Transform and payload limit for a single record?" id="hosting-faqs-batch-2" expanded="false"><p>A: The maximum timeout for Batch Transform is 3600 seconds. The
                    <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTransformJob.html#sagemaker-CreateTransformJob-request-MaxPayloadInMB">maximum payload size</a> for a record (per mini batch) is 100 MB.</p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: How do I speed up a Batch Transform job?" id="hosting-faqs-batch-3" expanded="false"><p>A: If you are using the <code class="code"><a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTransformJob.html">CreateTransformJob</a></code> API, you can reduce the time it takes
                    to complete batch transform jobs by using optimal values for parameters such as
                    <code class="code"><a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB">MaxPayloadInMB</a></code>,
                    <code class="code"><a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxConcurrentTransforms">MaxConcurrentTransforms</a></code>,
                    or <code class="code"><a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-BatchStrategy">BatchStrategy</a></code>.
                    The ideal value for <code class="code">MaxConcurrentTransforms</code> is equal to the number
                    of compute workers in the batch transform job. If you are using the SageMaker
                    console, you can specify these optimal parameter values in the
                        <b>Additional configuration</b> section of the <b>Batch
                        transform job configuration</b> page. SageMaker automatically finds the
                    optimal parameter settings for built-in algorithms. For custom algorithms,
                    provide these values through an <a href="your-algorithms-batch-code.html#your-algorithms-batch-code-how-containe-serves-requests">execution-parameters</a> endpoint.</p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: What are the data formats natively supported in Batch Transform?" id="hosting-faqs-batch-4" expanded="false"><p>A: Batch Transform supports CSV and JSON.</p></awsui-expandable-section></div>
     
        <h2 id="hosting-faqs-async">Asynchronous Inference</h2>
        <p>The following FAQ items answer common general questions for SageMaker Asynchronous Inference.</p>
        <div class="collapsible"><awsui-expandable-section variant="container" header="Q: What is Amazon SageMaker Asynchronous Inference?" id="hosting-faqs-async-1" expanded="true"><p>A: Asynchronous Inference queues incoming requests and processes them asynchronously. This
                    option is ideal for requests with large payload sizes or long processing times
                    that need to be processed as they arrive. Optionally, you can configure
                    auto-scaling settings to scale down the instance count to zero when not actively
                    processing requests. </p></awsui-expandable-section><awsui-expandable-section variant="container" header="Q: How do I scale my endpoints to 0 when there’s no traffic?" id="hosting-faqs-async-2" expanded="false"><p>A: Amazon SageMaker supports automatic scaling (autoscaling) your asynchronous
                    endpoint. Autoscaling dynamically adjusts the number of instances provisioned
                    for a model in response to changes in your workload. Unlike other hosted models
                    SageMaker supports, with Asynchronous Inference you can also scale down your asynchronous endpoints
                    instances to zero. Requests that are received when there are zero instances are
                    queued for processing once the endpoint scales up. For more information, see
                    <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-autoscale.html">Autoscale an asynchronous endpoint</a>.</p><p>Amazon SageMaker Serverless Inference also automatically scales down to zero. You won’t see this because SageMaker manages scaling your serverless endpoints,
                    but if you are not experiencing any traffic, the same infrastructure applies.</p></awsui-expandable-section></div>
    <awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="https://docs.aws.amazon.com/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./deploy-model-reference.html">Troubleshooting and reference</div><div id="next" class="next-link" accesskey="n" href="./mlops.html">Implement MLOps</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/hosting-faqs.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/hosting-faqs.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>