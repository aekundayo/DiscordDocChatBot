<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Model Monitor FAQs - Amazon SageMaker</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="model-monitor-faqs" /><meta name="default_state" content="model-monitor-faqs" /><link rel="icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="canonical" href="model-monitor-faqs.html" /><meta name="description" content="Use the following FAQs to learn more about Model Monitor." /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon SageMaker" /><meta name="guide" content="Developer Guide" /><meta name="abstract" content="Use Internet Monitor to build, train, and host machine learning models in AWS." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="model-monitor-faqs.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="de" /><link rel="alternative" href="model-monitor-faqs.html" hreflang="en-us" /><link rel="alternative" href="model-monitor-faqs.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/model-monitor-faqs.html" hreflang="zh-tw" /><link rel="alternative" href="model-monitor-faqs.html" hreflang="x-default" /><meta name="feedback-item" content="SageMaker" /><meta name="this_doc_product" content="Amazon SageMaker" /><meta name="this_doc_guide" content="Developer Guide" /><script defer="" src="../../../assets/r/vendor4.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor3.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor1.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-common.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-doc-page.js@version=2021.12.02"></script><link href="../../../assets/r/vendor4.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-common.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-doc-page.css@version=2021.12.02.css" rel="stylesheet" /><script async="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'sagemaker'}"></script><meta id="panorama-serviceSubSection" value="Developer Guide" /><meta id="panorama-serviceConsolePage" value="Model Monitor FAQs" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>Model Monitor FAQs - Amazon SageMaker</title><meta name="pdf" content="/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#model-monitor-faqs" /><meta name="rss" content="amazon-sagemaker-release-notes.rss" /><meta name="forums" content="http://forums.aws.amazon.com/forum.jspa?forumID=285" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/model-monitor-faqs.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/model-monitor-faqs.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/model-monitor-faqs.html" /><meta name="keywords" content="SageMaker,Amazon SageMaker,machine learning,notebook instance" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "Developer Guide",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Monitor data and model quality",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html"
      },
      {
        "@type" : "ListItem",
        "position" : 5,
        "name" : "Model Monitor FAQs",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#model-monitor-faqs" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="https://aws.amazon.com">AWS</a><a href="https://docs.aws.amazon.com/index.html">Documentation</a><a href="https://docs.aws.amazon.com/sagemaker/index.html">Amazon SageMaker</a><a href="whatis.html">Developer Guide</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="model-monitor-faqs">Model Monitor FAQs</h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>Refer to the following FAQs for more information about Amazon SageMaker Model Monitor.</p><p><b>Q: How do Model Monitor and SageMaker Clarify help customers monitor model
            behavior?</b></p><p>Customers can monitor model behavior along four dimensions - <a href="model-monitor-data-quality.html">Data quality</a>, <a href="model-monitor-model-quality.html">Model
            quality</a>, <a href="clarify-model-monitor-bias-drift.html">Bias drift</a>, and
            <a href="clarify-model-monitor-feature-attribution-drift.html">Feature
            Attribution drift</a> through Amazon SageMaker Model Monitor and SageMaker Clarify. <a href="http://aws.amazon.com/sagemaker/model-monitor/" rel="noopener noreferrer" target="_blank"><span>Model Monitor</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> continuously monitors the quality of
        Amazon SageMaker machine learning models in production. This includes monitoring drift in data
        quality and model quality metrics such as accuracy and RMSE. <a href="http://aws.amazon.com/sagemaker/clarify/?sagemaker-data-wrangler-whats-new.sort-by=item.additionalFields.postDateTime&amp;sagemaker-data-wrangler-whats-new.sort-order=desc" rel="noopener noreferrer" target="_blank"><span>SageMaker Clarify</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> bias monitoring helps data scientists and ML engineers monitor bias in
        model’s prediction and feature attribution drift.</p><p><b>Q: What happens in the background when Sagemaker Model monitor is
            enabled?</b></p><p>Amazon SageMaker Model Monitor automates model monitoring alleviating the need to monitor the models manually or
        building any additional tooling. In order to automate the process, Model Monitor provides you with
        the ability to create a set of baseline statistics and constraints using the data with which
        your model was trained, then set up a schedule to monitor the predictions made on your
        endpoint. Model Monitor uses rules to detect drift in your models and alerts you when it happens.
        The following steps describe what happens when you enable model monitoring:</p><div class="itemizedlist">
         
         
         
         
    <ul class="itemizedlist"><li class="listitem">
            <p><b>Enable model monitoring</b>: For a real-time
                endpoint, you have to enable the endpoint to capture data from incoming requests to
                a deployed ML model and the resulting model predictions. For a batch transform job, enable data capture of the batch transform inputs
                and outputs.</p>
        </li><li class="listitem">
            <p><b>Baseline processing job</b>: You then create a
                baseline from the dataset that was used to train the model. The baseline computes
                metrics and suggests constraints for the metrics. For example, the recall score for
                the model shouldn't regress and drop below 0.571, or the precision score shouldn't
                fall below 1.0. Real-time or batch predictions from your model are compared to the
                constraints and are reported as violations if they are outside the constrained
                values.</p>
        </li><li class="listitem">
            <p><b>Monitoring job</b>: Then, you create a monitoring
                schedule specifying what data to collect, how often to collect it, how to analyze
                it, and which reports to produce.</p>
        </li><li class="listitem">
            <p><b>Merge job</b>: This is only applicable if you are
                leveraging Amazon SageMaker Ground Truth. Model Monitor compares the predictions your model makes with Ground Truth
                labels to measure the quality of the model. For this to work, you periodically label
                data captured by your endpoint or batch transform job and upload it to Amazon S3. </p>
            <p>After you create and upload the Ground Truth labels, include the location of the labels
                as a parameter when you create the monitoring job. </p>
        </li></ul></div><p>When you use Model Monitor to monitor a batch transform job instead of a real-time endpoint,
        instead of receiving requests to an endpoint and tracking the predictions, Model Monitor monitors
        inference inputs and outputs. In a Model Monitor schedule, the customer provides the count and type
        of instances that are to be used in the processing job. These resources remain reserved
        until the schedule is deleted irrespective of the status of current execution.</p><p><b>Q: What is Data Capture, why is it required, and how can I enable
            it?</b></p><p>In order to log the inputs to the model endpoint and the inference outputs from the
        deployed model to Amazon S3, you can enable a feature called <a href="model-monitor-data-capture.html">Data Capture</a>. For more
        details about how to enable it for a real-time endpoint and batch transform job, see <a href="model-monitor-data-capture-endpoint.html">Capture data from real-time endpoint</a> and <a href="model-monitor-data-capture-batch.html">Capture data from batch transform job</a>.</p><p><b>Q: Does enabling Data Capture impact the performance of a real-time
            endpoint ?</b></p><p>Data Capture happens asynchronously without impacting production traffic. After you have
        enabled the data capture, then the request and response payload, along with some additional
        meta data, is saved in the Amazon S3 location that you specified in the
            <code class="code">DataCaptureConfig</code>. Note that there can be a delay in the propagation of the
        captured data to Amazon S3.</p><p>You can also view the captured data by listing the data capture files stored in Amazon S3. The
        format of the Amazon S3 path is:
            <code class="code">s3:///<span>{</span>endpoint-name}/<span>{</span>variant-name}/yyyy/mm/dd/hh/filename.jsonl</code>. Amazon S3
        Data Capture should be in the same region as the Model Monitor schedule. You should also ensure that
        the column names for the baseline dataset only have lowercase letters and an underscore
            (<code class="code">_</code>) as the only separator.</p><p><b>Q: Why is Ground Truth needed for model monitoring?</b></p><p>Ground Truth labels are required by the following features of Model Monitor:</p><div class="itemizedlist">
         
         
    <ul class="itemizedlist"><li class="listitem">
            <p><b>Model quality monitoring</b> compares the predictions
                your model makes with Ground Truth labels to measure the quality of the model.</p>
        </li><li class="listitem">
            <p><b>Model bias monitoring</b> monitors predictions for
                bias. One way bias can be introduced in deployed ML models is when the data used in
                training differs from the data used to generate predictions. This is especially
                pronounced if the data used for training changes over time (such as fluctuating
                mortgage rates), and the model prediction is not as accurate unless the model is
                retrained with updated data. For example, a model for predicting home prices can be
                biased if the mortgage rates used to train the model differ from the most current
                real-world mortgage rate.</p>
        </li></ul></div><p><b>Q: For customers leveraging Ground Truth for labeling, what are the steps I
            can take to monitor the quality of the model?</b></p><p>Model quality monitoring compares the predictions your model makes with Ground Truth labels to
        measure the quality of the model. For this to work, you periodically label data captured by
        your endpoint or batch transform job and upload it to Amazon S3. Besides captures, model bias
        monitoring execution also requires Ground Truth data. In real use cases, Ground Truth data should be
        regularly collected and uploaded to the designated Amazon S3 location. To match Ground Truth labels with
        captured prediction data, there must be a unique identifier for each record in the dataset.
        For the structure of each record for Ground Truth data, see <a href="model-monitor-model-quality-merge.html">Ingest Ground Truth Labels and
            Merge Them With Predictions</a>.</p><p>The following code example can be used for generating artificial Ground Truth data for a tabular
        dataset.</p><pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">import random

def ground_truth_with_id(inference_id):
    random.seed(inference_id)  # to get consistent results
    rand = random.random()
    # format required by the merge container
    return <span>{</span>
        "groundTruthData": <span>{</span>
            "data": "1" if rand &lt; 0.7 else "0",  # randomly generate positive labels 70% of the time
            "encoding": "CSV",
        },
        "eventMetadata": <span>{</span>
            "eventId": str(inference_id),
        },
        "eventVersion": "0",
    }


def upload_ground_truth(upload_time):
    records = [ground_truth_with_id(i) for i in range(test_dataset_size)]
    fake_records = [json.dumps(r) for r in records]
    data_to_upload = "\n".join(fake_records)
    target_s3_uri = f"<span>{</span>ground_truth_upload_path}/<span>{</span>upload_time:%Y/%m/%d/%H/%M%S}.jsonl"
    print(f"Uploading <span>{</span>len(fake_records)} records to", target_s3_uri)
    S3Uploader.upload_string_as_file_body(data_to_upload, target_s3_uri)
# Generate data for the last hour
upload_ground_truth(datetime.utcnow() - timedelta(hours=1))
# Generate data once a hour
def generate_fake_ground_truth(terminate_event):
    upload_ground_truth(datetime.utcnow())
    for _ in range(0, 60):
        time.sleep(60)
        if terminate_event.is_set():
            break


ground_truth_thread = WorkerThread(do_run=generate_fake_ground_truth)
ground_truth_thread.start()</code></pre><p>The following code example shows how to generate artificial traffic to send to the model
        endpoint. Notice the <code class="code">inferenceId</code> attribute used above to invoke. If this is
        present, it is used to join with Ground Truth data (otherwise, the <code class="code">eventId</code> is
        used).</p><pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">import threading

class WorkerThread(threading.Thread):
    def __init__(self, do_run, *args, **kwargs):
        super(WorkerThread, self).__init__(*args, **kwargs)
        self.__do_run = do_run
        self.__terminate_event = threading.Event()

    def terminate(self):
        self.__terminate_event.set()

    def run(self):
        while not self.__terminate_event.is_set():
            self.__do_run(self.__terminate_event)
def invoke_endpoint(terminate_event):
    with open(test_dataset, "r") as f:
        i = 0
        for row in f:
            payload = row.rstrip("\n")
            response = sagemaker_runtime_client.invoke_endpoint(
                EndpointName=endpoint_name,
                ContentType="text/csv",
                Body=payload,
                InferenceId=str(i),  # unique ID per row
            )
            i += 1
            response["Body"].read()
            time.sleep(1)
            if terminate_event.is_set():
                break


# Keep invoking the endpoint with test data
invoke_endpoint_thread = WorkerThread(do_run=invoke_endpoint)
invoke_endpoint_thread.start()</code></pre><p>You must upload Ground Truth data to an Amazon S3 bucket that has the same path format as captured
        data, which is in the following format:
            <code class="code">s3://&lt;bucket&gt;/&lt;prefix&gt;/yyyy/mm/dd/hh</code></p><div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>The date in this path is the date when the Ground Truth label is collected. It doesn't have
            to match the date when the inference was generated.</p></div></div><p><b>Q: How can customers customize monitoring
        schedules?</b></p><p>In addition to using the built-in monitoring mechanisms, you can create your own custom
        monitoring schedules and procedures using pre-processing and post-processing scripts, or by
        using or building your own container. It's important to note that pre-processing and
        post-processing scripts only work with data and model quality jobs.</p><p>Amazon SageMaker provides the capability for you to monitor and evaluate the data observed by the
        model endpoints. For this, you have to create a baseline with which you compare the
        real-time traffic. After a baseline is ready, set up a schedule to continuously evaluate and
        compare against the baseline. While creating a schedule, you can provide the pre-processing
        and post-processing script.</p><p>The following example shows how you can customize monitoring schedules with pre-processing
        and post-processing scripts.</p><pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">import boto3, osfrom sagemaker import get_execution_role, Sessionfrom sagemaker.model_monitor import CronExpressionGenerator, DefaultModelMonitor
# Upload pre and postprocessor scripts
session = Session()
bucket = boto3.Session().resource("s3").Bucket(session.default_bucket())
prefix = "demo-sagemaker-model-monitor"
pre_processor_script = bucket.Object(os.path.join(prefix, "preprocessor.py")).upload_file("preprocessor.py")
post_processor_script = bucket.Object(os.path.join(prefix, "postprocessor.py")).upload_file("postprocessor.py")
# Get execution role
role = get_execution_role() # can be an empty string
# Instance type
instance_type = "instance-type"
# instance_type = "ml.m5.xlarge" # Example
# Create a monitoring schedule with pre and post-processing
my_default_monitor = DefaultModelMonitor(
    role=role,
    instance_count=1,
    instance_type=instance_type,
    volume_size_in_gb=20,
    max_runtime_in_seconds=3600,
)

s3_report_path = "s3://<span>{</span>}/<span>{</span>}".format(bucket, "reports")
monitor_schedule_name = "monitor-schedule-name"
endpoint_name = "endpoint-name"
my_default_monitor.create_monitoring_schedule(
    post_analytics_processor_script=post_processor_script,
    record_preprocessor_script=pre_processor_script,
    monitor_schedule_name=monitor_schedule_name,
    # use endpoint_input for real-time endpoint
    endpoint_input=endpoint_name,
    # or use batch_transform_input for batch transform jobs
# batch_transform_input=batch_transform_name,
    output_s3_uri=s3_report_path,
    statistics=my_default_monitor.baseline_statistics(),
    constraints=my_default_monitor.suggested_constraints(),
    schedule_cron_expression=CronExpressionGenerator.hourly(),
    enable_cloudwatch_metrics=True,
)</code></pre><p><b>Q: What are some of the scenarios or use cases where I can leverage
            a pre-processing script?</b></p><p>You can use pre-processing scripts when you need to transform the inputs to your model
        monitor. Consider the following example scenarios:</p><div class="orderedlist">
         
         
         
         
    <ol><li>
            <p>Pre-processing script for data transformation.</p>
            <p>Suppose the output of your model is an array: <code class="code">[1.0, 2.1]</code>. The Model Monitor
                container only works with tabular or flattened JSON structures, such as
                    <code class="code"><span>{</span>“prediction0”: 1.0, “prediction1” : 2.1}</code>. You could use a
                pre-processing script like the following example to transform the array into the
                correct JSON structure.</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">def preprocess_handler(inference_record):
    input_data = inference_record.endpoint_input.data
    output_data = inference_record.endpoint_output.data.rstrip("\n")
    data = output_data + "," + input_data
    return <span>{</span> str(i).zfill(20) : d for i, d in enumerate(data.split(",")) }</code></pre>
        </li><li>
            <p>Exclude certain records from Model Monitor's metric calculations.</p>
            <p>Suppose your model has optional features and you use <code class="code">-1</code> to denote
                that the optional feature has a missing value. If you have a data quality monitor,
                you may want to remove the <code class="code">-1</code> from the input value array so that it
                isn't included in the monitor's metric calculations. You could use a script like the
                following to remove those values.</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">def preprocess_handler(inference_record):
    input_data = inference_record.endpoint_input.data
    return <span>{</span>i : None if x == -1 else x for i, x in enumerate(input_data.split(","))}</code></pre>
        </li><li>
            <p>Apply a custom sampling strategy.</p>
            <p>You can also apply a custom sampling strategy in your pre-processing script. To do
                this, configure Model Monitor's first-party, pre-built container to ignore a percentage of
                the records according to your specified sampling rate. In the following example, the
                handler samples 10% of the records by returning the record in 10% of handler calls
                and an empty list otherwise.</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">import random

def preprocess_handler(inference_record):
    # we set up a sampling rate of 0.1
    if random.random() &gt; 0.1:
        # return an empty list
        return []
    input_data = inference_record.endpoint_input.data
    return <span>{</span>i : None if x == -1 else x for i, x in enumerate(input_data.split(","))}</code></pre>
        </li><li>
            <p>Use custom logging.</p>
            <p>You can log any information you need from your script to Amazon CloudWatch. This can be
                useful when debugging your pre-processing script in case of an error. The following
                example shows how you can use the <code class="code">preprocess_handler</code> interface to log
                to CloudWatch.</p>
            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">
def preprocess_handler(inference_record, logger):
    logger.info(f"I'm a processing record: <span>{</span>inference_record}")
    logger.debug(f"I'm debugging a processing record: <span>{</span>inference_record}")
    logger.warning(f"I'm processing record with missing value: <span>{</span>inference_record}")
    logger.error(f"I'm a processing record with bad value: <span>{</span>inference_record}")
    return inference_record</code></pre>
        </li></ol></div><div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>When the pre-processing script is run on batch transform data, the input type is not
            always the <code class="code">CapturedData</code> object. For CSV data, the type is a string. For
            JSON data, the type is a Python dictionary.</p></div></div><p><b>Q: When can I leverage a post-processing script?</b></p><p>You can leverage a post-processing script as an extension following a successful
        monitoring run. The following is a simple example, but you can perform or call any business
        function that you need to perform after a successful monitoring run.</p><pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">def postprocess_handler(): 
    print("Hello from the post-processing script!")
</code></pre><p><b>Q: When should I consider bringing my own container for model
            monitoring?</b></p><p>SageMaker provides a pre-built container for analyzing data captured from endpoints or batch
        transform jobs for tabular datasets. However, there are scenarios where you might want to
        create your own container. Consider the following scenarios:</p><div class="itemizedlist">
         
         
         
         
         
    <ul class="itemizedlist"><li class="listitem">
            <p>You have regulatory and compliance requirements to only use the containers that
                are created and maintained internally in your organization.</p>
        </li><li class="listitem">
            <p>If you want to include a few third-party libraries, you can place a
                    <code class="code">requirements.txt</code> file in a local directory and reference it using
                the <code class="code">source_dir</code> parameter in the <a href="https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.Estimator" rel="noopener noreferrer" target="_blank"><span>SageMaker estimator</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>, which enables library installation at run-time.
                However, if you have lots of libraries or dependencies that increase the
                installation time while running the training job, you might want to leverage
                BYOC.</p>
        </li><li class="listitem">
            <p>Your environment forces no internet connectivity (or Silo), which prevents package
                download.</p>
        </li><li class="listitem">
            <p>You want to monitor data that's in data formats other than tabular, such as NLP or
                CV use cases.</p>
        </li><li class="listitem">
            <p>When you require additional monitoring metrics than the ones supported by
                Model Monitor.</p>
        </li></ul></div><p><b>Q: I have NLP and CV models. How do I monitor them for data
            drift?</b></p><p>Amazon SageMaker's prebuilt container supports tabular datasets. If you want to monitor NLP and CV
        models, you can bring your own container by leveraging the extension points provided by
        Model Monitor. For more details about the requirements, see <a href="model-monitor-byoc-containers.html">Bring your own
            containers</a>. Some of the following are more examples:</p><div class="itemizedlist">
         
         
    <ul class="itemizedlist"><li class="listitem">
            <p>For a detailed explanation of how to use Model Monitor for a computer vision use case, see
                    <a href="http://aws.amazon.com/blogs/machine-learning/detecting-and-analyzing-incorrect-model-predictions-with-amazon-sagemaker-model-monitor-and-debugger/" rel="noopener noreferrer" target="_blank"><span>Detecting and Analyzing incorrect predictions</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
        </li><li class="listitem">
            <p>For a scenario where Model Monitor can be leveraged for a NLP use case, see <a href="http://aws.amazon.com/blogs/machine-learning/detect-nlp-data-drift-using-custom-amazon-sagemaker-model-monitor/" rel="noopener noreferrer" target="_blank"><span>Detect NLP data drift using custom Amazon SageMaker Model Monitor</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
        </li></ul></div><p><b>Q: I want to delete the model endpoint for which Model Monitor was enabled,
            but I'm unable to do so since the monitoring schedule is still active. What should I
            do?</b></p><p>If you want to delete an inference endpoint hosted in SageMaker which has Model Monitor enabled, first
        you must delete the model monitoring schedule (with the
            <code class="code">DeleteMonitoringSchedule</code>
        <a href="https://docs.aws.amazon.com/cli/latest/reference/sagemaker/delete-monitoring-schedule.html">CLI</a> or
            <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DeleteMonitoringSchedule.html">API</a>).
        Then, delete the endpoint.</p><p><b>Q: Does SageMaker Model Monitor calculate metrics and statistics for
            input?</b></p><p>Model Monitor calculates metrics and statistics for output, not input.</p><p><b>Q: Does SageMaker Model Monitor support multi-model endpoints?</b></p><p>No, Model Monitor only supports endpoints that host a single model and doesn't support monitoring
        multi-model endpoints.</p><p><b>Q: Does SageMaker Model Monitor provide monitoring data about individual
            containers in an inference pipeline?</b></p><p>Model Monitor supports monitoring inference pipelines, but capturing and analyzing data is done
        for the entire pipeline, not for individual containers in the pipeline.</p><p><b>Q: What can I do to prevent impact to inference requests when data
            capture is set up?</b></p><p>To prevent impact to inference requests, Data Capture stops capturing requests at high
        levels of disk usage. It is recommended you keep your disk utilization below 75% in order to
        ensure data capture continues capturing requests.</p><p><b>Q: Can Amazon S3 Data Capture be in a different AWS region than the
            region in which the monitoring schedule was set up?</b></p><p>No, Amazon S3 Data Capture must be in the same region as the monitoring schedule.</p><p><b>Q: What is a baseline, and how do I create one? Can I create a
            custom baseline?</b></p><p>A baseline is used as a reference to compare real-time or batch predictions from the
        model. It computes statistics and metrics along with constraints on them. During monitoring,
        all of these are used in conjunction to identify violations.</p><p>To use the default solution of Amazon SageMaker Model Monitor, you can leverage the <a href="https://sagemaker.readthedocs.io/en/stable/api/inference/model_monitor.html" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker Python SDK</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. Specifically, use the <a href="https://sagemaker.readthedocs.io/en/stable/api/inference/model_monitor.html#sagemaker.model_monitor.model_monitoring.DefaultModelMonitor.suggest_baseline" rel="noopener noreferrer" target="_blank"><span>suggest_baseline</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> method of the <a href="https://sagemaker.readthedocs.io/en/stable/api/inference/model_monitor.html#sagemaker.model_monitor.model_monitoring.DefaultModelMonitor" rel="noopener noreferrer" target="_blank"><span>ModelMonitor</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> or the <a href="https://sagemaker.readthedocs.io/en/stable/api/inference/model_monitor.html#sagemaker.model_monitor.model_monitoring.ModelQualityMonitor" rel="noopener noreferrer" target="_blank"><span>ModelQualityMonitor</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> class to trigger a processing job that computes the metrics
        and constraints for the baseline.</p><p>The result of a baseline job are two files: <code class="code">statistics.json</code> and
            <code class="code">constraints.json</code>. <a href="model-monitor-byoc-statistics.html">Schema for
            statistics</a> and <a href="model-monitor-byoc-constraints.html">schema for
            constraints</a> contain the schema of the respective files. You can review the
        generated constraints and modify them before using them for monitoring. Based on your
        understanding of the domain and business problem, you can make a constraint more aggressive,
        or relax it to control the number and nature of the violations.</p><p><b>Q: What are the guidelines to create a baseline
        dataset?</b></p><p>The primary requirement for any kind of monitoring is to have a baseline dataset that is
        used to compute metrics and constraints. Typically, this is the training dataset used by the
        model, but in some cases you might choose to use some other reference dataset.</p><p>The column names of the baseline dataset should be compatible with Spark. In order to
        maintain the maximum compatibility between Spark, CSV, JSON and parquet it is advisable to
        use only lowercase letters, and only use <code class="code">_</code> as the separator. Special characters
        including <code class="code">“ ”</code> can cause issues.</p><p><b>Q: What are the <code class="code">StartTimeOffset</code> and
                <code class="code">EndTimeOffset</code> parameters, and when are they used?</b></p><p>When Amazon SageMaker Ground Truth is required for monitoring jobs like model quality, you need to ensure that
        a monitoring job only uses data for which Ground Truth is available. The
            <code class="code">start_time_offset</code> and <code class="code">end_time_offset</code> parameters of <a href="https://sagemaker.readthedocs.io/en/stable/api/inference/model_monitor.html#sagemaker.model_monitor.model_monitoring.EndpointInput" rel="noopener noreferrer" target="_blank"><span>EndpointInput</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> can be used to select the data that the monitoring job uses. The
        monitoring job uses the data in the time window that is defined by
            <code class="code">start_time_offset</code> and <code class="code">end_time_offset</code>. These parameters need
        to be specified in the <a href="https://en.wikipedia.org/wiki/ISO_8601#Durations" rel="noopener noreferrer" target="_blank"><span>ISO
            8601 duration format</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. The following are some examples:</p><div class="itemizedlist">
         
         
    <ul class="itemizedlist"><li class="listitem">
            <p>If your Ground Truth results arrive 3 days after the predictions have been made, set
                    <code class="code">start_time_offset="-P3D"</code> and <code class="code">end_time_offset="-P1D"</code>,
                which is 3 days and 1 day respectively.</p>
        </li><li class="listitem">
            <p>If the Ground Truth results arrive 6 hours after the predictions and you have an hourly
                schedule, set <code class="code">start_time_offset="-PT6H"</code> and
                    <code class="code">end_time_offset="-PT1H"</code>, which is 6 hours and 1 hour.</p>
        </li></ul></div><p><b>Q: Can I run ‘on demand' monitoring jobs?</b></p><p>Yes, you can run ‘on demand’ monitoring jobs by running a SageMaker Processing job. For Batch
        Transform, <a href="pipelines-sdk.html">SageMaker Pipelines</a> has a <a href="https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.monitor_batch_transform_step.MonitorBatchTransformStep" rel="noopener noreferrer" target="_blank"><span>MonitorBatchTransformStep</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> that you can use to create a SageMaker pipeline that runs
        monitoring jobs on demand. The SageMaker examples repository has code samples for running <a href="https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker_model_monitor/model_monitor_batch_transform/SageMaker-ModelMonitoring-Batch-Transform-Data-Quality-With-SageMaker-Pipelines-On-Demand.ipynb" rel="noopener noreferrer" target="_blank"><span>data quality</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> and <a href="https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker_model_monitor/model_monitor_batch_transform/SageMaker-ModelMonitoring-Batch-Transform-Model-Quality-With-SageMaker-Pipelines-On-Demand.ipynb" rel="noopener noreferrer" target="_blank"><span>model quality</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> monitoring jobs on demand.</p><p><b>Q: How do I set up Model Monitor?</b></p><p>You can set up Model Monitor in the following ways:</p><div class="itemizedlist">
         
         
         
         
    <ul class="itemizedlist"><li class="listitem">
            <p><b><a href="https://sagemaker.readthedocs.io/en/stable/index.html" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker Python
                        SDK</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></b> – There is a <a href="https://sagemaker.readthedocs.io/en/stable/api/inference/model_monitor.html" rel="noopener noreferrer" target="_blank"><span>Model Monitor module</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> which contains classes and functions that assist in
                suggesting baselines, creating monitoring schedules, and more. See the <a href="https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker_model_monitor" rel="noopener noreferrer" target="_blank"><span>Amazon SageMaker Model Monitor notebook examples</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> for detailed notebooks that leverage the SageMaker
                Python SDK for setting up Model Monitor.</p>
        </li><li class="listitem">
            <p><b><a href="pipelines-sdk.html">SageMaker Pipelines</a></b>
                – SageMaker Pipelines are integrated with Model Monitor through the <a href="build-and-manage-steps.html#step-type-quality-check">QualityCheck Step</a> and <a href="build-and-manage-steps.html#step-type-clarify-check">ClarifyCheckStep</a> APIs. You can create a SageMaker pipeline that contains
                these steps and that can be used to run monitoring jobs on demand whenever the
                pipeline is executed.</p>
        </li><li class="listitem">
            <p><b><a href="studio.html">Amazon SageMaker Studio</a></b>
                – You can create a data or model quality monitoring schedule along with model
                bias and explainability schedules directly from the UI by selecting an endpoint from
                the list of deployed model endpoints. Schedules for other types of monitoring can be
                created by selecting the relevant tab in the UI.</p>
        </li><li class="listitem">
            <p><b><a href="model-dashboard.html">SageMaker Model Dashboard</a></b>
                – You can enable monitoring on endpoints by selecting a model that has been
                deployed to an endpoint. In the following screenshot of the SageMaker console, a model
                named <code class="code">group1</code> has been selected from the <b>Models</b>
                section of the <b>Model dashboard</b>. On this page, you can create a
                monitoring schedule, and you can edit, activate or deactivate existing monitoring
                schedules and alerts. For a step by step guide on how to view alerts and model
                monitor schedules, see <a href="model-dashboard-schedule.html">View Model Monitor schedules and
                    alerts</a>.</p>
        </li></ul></div><div class="mediaobject">
         
            <img src="../../../images/sagemaker/latest/dg/images/model-monitoring-faqs-screenshot.png" class="aws-docs-img-whiteBg aws-docs-img-padding" alt="&#xA;            Screenshot of the Model dashboard, showing the option to create a monitoring schedule.&#xA;        " style="max-width:100%" />
         
         
    </div><p><b>Q: How does Model Monitor Integrate with SageMaker Model Dashboard</b></p><p><a href="model-dashboard.html">SageMaker Model Dashboard</a>
        gives you unified monitoring across all your models by providing automated alerts about
        deviations from expected behavior and troubleshooting to inspect models and analyze factors
        impacting model performance over time.</p><awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="https://docs.aws.amazon.com/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./model-monitor-cloudformation-monitoring-schedules.html">AWS CloudFormation Custom Resource for Real-time Endpoints</div><div id="next" class="next-link" accesskey="n" href="./model-explainability.html">Detect bias and understand explanations</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/model-monitor-faqs.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/model-monitor-faqs.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>