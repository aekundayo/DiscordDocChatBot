<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Run a custom load test - Amazon SageMaker</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="inference-recommender-load-test" /><meta name="default_state" content="inference-recommender-load-test" /><link rel="icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="canonical" href="inference-recommender-load-test.html" /><meta name="description" content="Learn how to create custom load test jobs, get results, and stop custom load test jobs with AWS SDK for Python (Boto3), AWS CLI, Amazon SageMaker Studio, and the SageMaker console." /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon SageMaker" /><meta name="guide" content="Developer Guide" /><meta name="abstract" content="Use Internet Monitor to build, train, and host machine learning models in AWS." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="inference-recommender-load-test.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="de" /><link rel="alternative" href="inference-recommender-load-test.html" hreflang="en-us" /><link rel="alternative" href="inference-recommender-load-test.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/inference-recommender-load-test.html" hreflang="zh-tw" /><link rel="alternative" href="inference-recommender-load-test.html" hreflang="x-default" /><meta name="feedback-item" content="SageMaker" /><meta name="this_doc_product" content="Amazon SageMaker" /><meta name="this_doc_guide" content="Developer Guide" /><script defer="" src="../../../assets/r/vendor4.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor3.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor1.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-common.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-doc-page.js@version=2021.12.02"></script><link href="../../../assets/r/vendor4.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-common.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-doc-page.css@version=2021.12.02.css" rel="stylesheet" /><script async="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'sagemaker'}"></script><meta id="panorama-serviceSubSection" value="Developer Guide" /><meta id="panorama-serviceConsolePage" value="Run a custom load test" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>Run a custom load test - Amazon SageMaker</title><meta name="pdf" content="/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#inference-recommender-load-test" /><meta name="rss" content="amazon-sagemaker-release-notes.rss" /><meta name="forums" content="http://forums.aws.amazon.com/forum.jspa?forumID=285" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/inference-recommender-load-test.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/inference-recommender-load-test.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/inference-recommender-load-test.html" /><meta name="keywords" content="SageMaker,Amazon SageMaker,machine learning,notebook instance,inference,deploy model,endpoint,prediction,ML application,serverless machine learning,multi model deployment,inference pipelines,ML model" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "Developer Guide",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Deploy models for inference",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html"
      },
      {
        "@type" : "ListItem",
        "position" : 5,
        "name" : "Amazon SageMaker Inference Recommender",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html"
      },
      {
        "@type" : "ListItem",
        "position" : 6,
        "name" : "Recommendation jobs",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-recommendation-jobs.html"
      },
      {
        "@type" : "ListItem",
        "position" : 7,
        "name" : "Run a custom load test",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-recommendation-jobs.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#inference-recommender-load-test" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="https://aws.amazon.com">AWS</a><a href="https://docs.aws.amazon.com/index.html">Documentation</a><a href="https://docs.aws.amazon.com/sagemaker/index.html">Amazon SageMaker</a><a href="whatis.html">Developer Guide</a></div><div id="page-toc-src"><a href="inference-recommender-load-test.html#load-test-create">Create a load test job</a><a href="inference-recommender-load-test.html#load-test-describe">Get your load test results</a><a href="inference-recommender-load-test.html#load-test-stop">Stop your load test</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="inference-recommender-load-test">Run a custom load test</h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>Amazon SageMaker Inference Recommender load tests conduct extensive benchmarks based on production
                requirements for latency and throughput, custom traffic patterns, and either
                serverless endpoints or real-time instances (up to 10) that you select.</p><p>The following sections demonstrate how to create, describe, and stop a load test
                programmatically using the AWS SDK for Python (Boto3) and the AWS CLI, or interactively using
                Amazon SageMaker Studio or the SageMaker console.</p>
                <h2 id="load-test-create">Create a load test job</h2>
                <p>Create a load test programmatically using the AWS SDK for Python (Boto3), with the AWS CLI,
                    or interactively using Studio or the SageMaker console. As with Inference Recommender inference
                    recommendations, specify a job name for your load test, an AWS IAM role ARN,
                    an input configuration, and your model package ARN from when you registered your
                    model with the model registry. Load tests require that you also specify a
                    traffic pattern and stopping conditions.</p>
                
                <awsdocs-tabs><dl style="display: none">
                    <dt>AWS SDK for Python (Boto3)</dt><dd tab-id="aws-sdk-for-python-(boto3)">
                            <p>Use the <code class="code">CreateInferenceRecommendationsJob</code> API to
                                create an Inference Recommender load test. Specify <code class="code">Advanced</code> for the
                                    <code class="code">JobType</code> field and provide: </p>
                            <div class="itemizedlist">
                                 
                                 
                                 
                                 
                            <ul class="itemizedlist"><li class="listitem"><p>A job name for your load test (<code class="code">JobName</code>). The job name must be unique within your
                                        AWS Region and within your AWS account.</p></li><li class="listitem"><p>The Amazon Resource Name (ARN) of an IAM role that enables Inference Recommender to perform tasks on your
                                        behalf. Define this for the <code class="code">RoleArn</code>
                                        field.</p></li><li class="listitem">
                                    <p>An endpoint configuration dictionary (<code class="code">InputConfig</code>) where you specify the
                                        following:</p>
                                    <div class="itemizedlist">
                                         
                                          
                                            
                                    <ul class="itemizedlist"><li class="listitem">
                                            <p>For <code class="code">TrafficPattern</code>, specify either
                                                the phases or stairs traffic pattern. With the
                                                phases traffic pattern, new users spawn every minute
                                                at the rate you specify. With the stairs traffic
                                                pattern, new users spawn at timed intervals (or
                                                  <em>steps</em>) at a
                                                rate you specify. Choose one of the
                                                following:</p>
                                            <div class="itemizedlist">
                                                 
                                                 
                                            <ul class="itemizedlist"><li class="listitem"><p>For <code class="code">TrafficType</code>, specify <code class="code">PHASES</code>. Then, for the <code class="code">Phases</code>
                                                  array, specify the
                                                  <code class="code">InitialNumberOfUsers</code> (how many
                                                  concurrent users to start with, with a minimum of
                                                  1 and a maximum of 3), <code class="code">SpawnRate</code> (the
                                                  number of users to be spawned in a minute for a
                                                  specific phase of load testing, with a minimum of
                                                  0 and maximum of 3), and
                                                  <code class="code">DurationInSeconds</code> (how long the
                                                  traffic phase should be, with a minimum of 120 and
                                                  maximum of 3600).</p></li><li class="listitem">
                                                    <p>For <code class="code">TrafficType</code>, specify
                                                  <code class="code">STAIRS</code>. Then, for the
                                                  <code class="code">Stairs</code> array, specify the
                                                  <code class="code">DurationInSeconds</code> (how long the
                                                  traffic phase should be, with a minimum of 120 and
                                                  maximum of 3600), <code class="code">NumberOfSteps</code> (how
                                                  many intervals are used during the phase), and
                                                  <code class="code">UsersPerStep</code> (how many users are
                                                  added during each interval). Note that the length
                                                  of each step is the value of
                                                  <code class="code">DurationInSeconds / NumberOfSteps</code>.
                                                  For example, if your
                                                  <code class="code">DurationInSeconds</code> is <code class="code">600</code>
                                                  and you specify <code class="code">5</code> steps, then each
                                                  step is 120 seconds long.</p>
                                                    <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>A user is defined as a system-generated actor that runs in a loop and invokes requests to an endpoint as part of Inference Recommender. For a typical XGBoost container
                                                        running on an <code class="code">ml.c5.large</code> instance, endpoints can reach 30,000 invocations per minute (500 tps) with just 15-20 users.</p></div></div>
                                                </li></ul></div>
                                        </li><li class="listitem">
                                            <p>For <code class="code">ResourceLimit</code>, specify <code class="code">MaxNumberOfTests</code> (the maximum number of benchmarking load tests for an Inference Recommender job, with a minimum of 1 and a maximum of 10) and
                                                <code class="code">MaxParallelOfTests</code> (the maximum number of parallel benchmarking load tests for an Inference Recommender job, with a minimum of 1 and a maximum of 10).</p>
                                        </li><li class="listitem">
                                            <p>For <code class="code">EndpointConfigurations</code>, you can
                                                specify one of the following:</p>
                                            <div class="itemizedlist">
                                                 
                                                 
                                            <ul class="itemizedlist"><li class="listitem"><p>The <code class="code">InstanceType</code> field, where you specify the
                                                    instance type on which you want to run your load
                                                    tests.</p></li><li class="listitem"><p>The <code class="code">ServerlessConfig</code>, in which you specify your ideal values for
                                                  <code class="code">MaxConcurrency</code> and
                                                  <code class="code">MemorySizeInMB</code> for a serverless
                                                  endpoint. For more information, see the <a href="serverless-endpoints.html">Serverless Inference documentation</a>.</p></li></ul></div>
                                        </li></ul></div>
                                </li><li class="listitem">
                                    <p>A stopping conditions dictionary (<code class="code">StoppingConditions</code>), where if any of the conditions are met, the Inference Recommender job stops. For this example, specify the following fields in the dictionary:</p>
                                    <div class="itemizedlist">
                                         
                                         
                                         
                                    <ul class="itemizedlist"><li class="listitem"><p>For <code class="code">MaxInvocations</code>, specify the maximum number of requests per minute expected for the endpoint, with a minimum of 1 and a maximum of 30,000.</p></li><li class="listitem"><p>For <code class="code">ModelLatencyThresholds</code>, specify <code class="code">Percentile</code> (the model latency percentile threshold) and <code class="code">ValueInMilliseconds</code> (the model latency percentile value in milliseconds).</p></li><li class="listitem"><p>(Optional) For <code class="code">FlatInvocations</code>, you can specify whether to continue the load test
                                                when the TPS (invocations per minute) rate flattens.
                                                A flattened TPS rate usually means that the endpoint
                                                has reached capacity. However, you might want to
                                                continue monitoring the endpoint under full capacity
                                                conditions. To continue the load test when this
                                                happens, specify this value as
                                                <code class="code">Continue</code>. Otherwise, the default value
                                                is <code class="code">Stop</code>.</p></li></ul></div>
                                </li></ul></div>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python "># Create a low-level SageMaker service client.
import boto3
aws_region=<code class="replaceable">&lt;INSERT&gt;</code>
sagemaker_client=boto3.client('sagemaker', region=aws_region) 
                
# Provide a name to your recommendation based on load testing
load_test_job_name=<code class="replaceable">"&lt;INSERT&gt;"</code>

# Provide the name of the sagemaker instance type
instance_type=<code class="replaceable">"&lt;INSERT&gt;"</code>

# Provide the IAM Role that gives SageMaker permission to access AWS services 
role_arn='arn:aws:iam::<code class="replaceable">&lt;account&gt;:role/*</code>'

# Provide your model package ARN that was created when you registered your 
# model with Model Registry
model_package_arn='arn:aws:sagemaker:<code class="replaceable">&lt;region&gt;:&lt;account&gt;:role/*</code>'

sagemaker_client.create_inference_recommendations_job(
                        JobName=load_test_job_name,
                        JobType="Advanced",
                        RoleArn=role_arn,
                        InputConfig=<span>{</span>
                            'ModelPackageVersionArn': model_package_arn,
                            "JobDurationInSeconds": 7200,
                            'TrafficPattern' : <span>{</span>
                                # Replace PHASES with STAIRS to use the stairs traffic pattern
                                'TrafficType': 'PHASES',
                                'Phases': [
                                    <span>{</span>
                                        'InitialNumberOfUsers': 1,
                                        'SpawnRate': 1,
                                        'DurationInSeconds': 120
                                    },
                                    <span>{</span>
                                        'InitialNumberOfUsers': 1,
                                        'SpawnRate': 1,
                                        'DurationInSeconds': 120
                                    }
                                ]
                                # Uncomment this section and comment out the Phases object above to use the stairs traffic pattern
                                # 'Stairs' : <span>{</span>
                                #   'DurationInSeconds': 240,
                                #   'NumberOfSteps': 2,
                                #   'UsersPerStep': 2
                                # }
                            },
                            'ResourceLimit': <span>{</span>
                                        'MaxNumberOfTests': 10,
                                        'MaxParallelOfTests': 3
                                },
                            "EndpointConfigurations" : [<span>{</span>
                                        'InstanceType': 'ml.c5.xlarge'
                                    },
                                    <span>{</span>
                                        'InstanceType': 'ml.m5.xlarge'
                                    },
                                    <span>{</span>
                                        'InstanceType': 'ml.r5.xlarge'
                                    }]
                                    # Uncomment the ServerlessConfig and comment out the InstanceType field if you want recommendations for a serverless endpoint
                                    # "ServerlessConfig": <span>{</span>
                                    #     "MaxConcurrency": <code class="replaceable">value</code>, 
                                    #     "MemorySizeInMB": <code class="replaceable">value</code> 
                                    # }
                        },
                        StoppingConditions=<span>{</span>
                            'MaxInvocations': 1000,
                            'ModelLatencyThresholds':[<span>{</span>
                                'Percentile': 'P95', 
                                'ValueInMilliseconds': 100
                            }],
                            # Change 'Stop' to 'Continue' to let the load test continue if invocations flatten 
                            'FlatInvocations': 'Stop'
                        }
                )</code></pre>
                            <p>See the <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/Welcome.html">Amazon SageMaker API Reference Guide</a> 
                                for a full list of optional and required arguments you can pass to <code class="code">CreateInferenceRecommendationsJob</code>.</p>
                        </dd>
                    <dt>AWS CLI</dt><dd tab-id="aws-cli">
                            <p>Use the <code class="code">create-inference-recommendations-job</code> API to
                                create an Inference Recommender load test. Specify <code class="code">Advanced</code> for the
                                    <code class="code">JobType</code> field and provide: </p>
                            <div class="itemizedlist">
                                 
                                 
                                 
                                 
                            <ul class="itemizedlist"><li class="listitem"><p>A job name for your load test (<code class="code">job-name</code>). The job name must be unique within your
                                        AWS Region and within your AWS account.</p></li><li class="listitem"><p>The Amazon Resource Name (ARN) of an IAM role that enables Inference Recommender to perform tasks on your
                                        behalf. Define this for the <code class="code">role-arn</code>
                                        field.</p></li><li class="listitem">
                                    <p>An endpoint configuration dictionary (<code class="code">input-config</code>) where you specify the
                                        following:</p>
                                    <div class="itemizedlist">
                                         
                                         
                                            
                                    <ul class="itemizedlist"><li class="listitem">
                                                <p>For <code class="code">TrafficPattern</code>, specify either
                                                    the phases or stairs traffic pattern. With the
                                                    phases traffic pattern, new users spawn every minute
                                                    at the rate you specify. With the stairs traffic
                                                    pattern, new users spawn at timed intervals (or
                                                    <em>steps</em>) at a
                                                    rate you specify. Choose one of the
                                                    following:</p>
                                                <div class="itemizedlist">
                                                     
                                                     
                                                <ul class="itemizedlist"><li class="listitem"><p>For <code class="code">TrafficType</code>, specify <code class="code">PHASES</code>. Then, for the <code class="code">Phases</code>
                                                        array, specify the
                                                        <code class="code">InitialNumberOfUsers</code> (how many
                                                        concurrent users to start with, with a minimum of
                                                        1 and a maximum of 3), <code class="code">SpawnRate</code> (the
                                                        number of users to be spawned in a minute for a
                                                        specific phase of load testing, with a minimum of
                                                        0 and maximum of 3), and
                                                        <code class="code">DurationInSeconds</code> (how long the
                                                        traffic phase should be, with a minimum of 120 and
                                                        maximum of 3600).</p></li><li class="listitem">
                                                        <p>For <code class="code">TrafficType</code>, specify
                                                            <code class="code">STAIRS</code>. Then, for the
                                                            <code class="code">Stairs</code> array, specify the
                                                            <code class="code">DurationInSeconds</code> (how long the
                                                            traffic phase should be, with a minimum of 120 and
                                                            maximum of 3600), <code class="code">NumberOfSteps</code> (how
                                                            many intervals are used during the phase), and
                                                            <code class="code">UsersPerStep</code> (how many users are
                                                            added during each interval). Note that the length
                                                            of each step is the value of
                                                            <code class="code">DurationInSeconds / NumberOfSteps</code>.
                                                            For example, if your
                                                            <code class="code">DurationInSeconds</code> is <code class="code">600</code>
                                                            and you specify <code class="code">5</code> steps, then each
                                                            step is 120 seconds long.</p>
                                                        <div class="awsdocs-note"><div class="awsdocs-note-title"><awsui-icon name="status-info" variant="link"></awsui-icon><h6>Note</h6></div><div class="awsdocs-note-text"><p>A user is defined as a system-generated actor that runs in a loop and invokes requests to an endpoint as part of Inference Recommender. For a typical XGBoost container
                                                            running on an <code class="code">ml.c5.large</code> instance, endpoints can reach 30,000 invocations per minute (500 tps) with just 15-20 users.</p></div></div>
                                                    </li></ul></div>
                                        </li><li class="listitem">
                                            <p>For <code class="code">ResourceLimit</code>, specify <code class="code">MaxNumberOfTests</code> (the maximum number of benchmarking load tests for an Inference Recommender job, with a minimum of 1 and a maximum of 10) and
                                                <code class="code">MaxParallelOfTests</code> (the maximum number of parallel benchmarking load tests for an Inference Recommender job, with a minimum of 1 and a maximum of 10).</p>
                                        </li><li class="listitem">
                                            <p>For <code class="code">EndpointConfigurations</code>, you can
                                                specify one of the following:</p>
                                            <div class="itemizedlist">
                                                 
                                                 
                                            <ul class="itemizedlist"><li class="listitem">
                                                  <p>The <code class="code">InstanceType</code> field, where
                                                  you specify the instance type on which you want to
                                                  run your load tests.</p>
                                                </li><li class="listitem">
                                                  <p>The <code class="code">ServerlessConfig</code>, in which
                                                  you specify your ideal values for
                                                  <code class="code">MaxConcurrency</code> and
                                                  <code class="code">MemorySizeInMB</code> for a serverless
                                                  endpoint.</p>
                                                </li></ul></div>
                                        </li></ul></div>
                                </li><li class="listitem">
                                    <p>A stopping conditions dictionary (<code class="code">stopping-conditions</code>), where if any of the conditions are met, the Inference Recommender job stops. For this example, specify the following fields in the dictionary:</p>
                                    <div class="itemizedlist">
                                         
                                         
                                         
                                    <ul class="itemizedlist"><li class="listitem"><p>For <code class="code">MaxInvocations</code>, specify the maximum number of requests per minute expected for the endpoint, with a minimum of 1 and a maximum of 30,000.</p></li><li class="listitem"><p>For <code class="code">ModelLatencyThresholds</code>, specify <code class="code">Percentile</code> (the model latency percentile threshold) and <code class="code">ValueInMilliseconds</code> (the model latency percentile value in milliseconds).</p></li><li class="listitem"><p>(Optional) For <code class="code">FlatInvocations</code>, you can specify whether to continue the load test
                                            when the TPS (invocations per minute) rate flattens.
                                            A flattened TPS rate usually means that the endpoint
                                            has reached capacity. However, you might want to
                                            continue monitoring the endpoint under full capacity
                                            conditions. To continue the load test when this
                                            happens, specify this value as
                                            <code class="code">Continue</code>. Otherwise, the default value
                                            is <code class="code">Stop</code>.</p></li></ul></div>
                                </li></ul></div>

                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="bash ">aws sagemaker create-inference-recommendations-job\
    --region <code class="replaceable">&lt;region&gt;</code>\
    --job-name <code class="replaceable">&lt;job-name&gt;</code>\
    --job-type ADVANCED\
    --role-arn arn:aws:iam::<code class="replaceable">&lt;account&gt;:role/*</code>\
    --input-config \"<span>{</span>
        \"ModelPackageVersionArn\": \"arn:aws:sagemaker:<code class="replaceable">&lt;region&gt;:&lt;account&gt;:role/*</code>\",
        \"JobDurationInSeconds\": 7200,                                
        \"TrafficPattern\" : <span>{</span>
                # Replace PHASES with STAIRS to use the stairs traffic pattern
                \"TrafficType\": \"PHASES\",
                \"Phases\": [
                    <span>{</span>
                        \"InitialNumberOfUsers\": 1,
                        \"SpawnRate\": 60,
                        \"DurationInSeconds\": 300
                    }
                ]
                # Uncomment this section and comment out the Phases object above to use the stairs traffic pattern
                # 'Stairs' : <span>{</span>
                #   'DurationInSeconds': 240,
                #   'NumberOfSteps': 2,
                #   'UsersPerStep': 2
                # }
            },
            \"ResourceLimit\": <span>{</span>
                \"MaxNumberOfTests\": 10,
                \"MaxParallelOfTests\": 3
            },
            \"EndpointConfigurations\" : [
                <span>{</span>
                    \"InstanceType\": \"ml.c5.xlarge\"
                },
                <span>{</span>
                    \"InstanceType\": \"ml.m5.xlarge\"
                },
                <span>{</span>
                    \"InstanceType\": \"ml.r5.xlarge\"
                }
                # Use the ServerlessConfig and leave out the InstanceType fields if you want recommendations for a serverless endpoint
                # \"ServerlessConfig\": <span>{</span>
                #     \"MaxConcurrency\": <code class="replaceable">value</code>, 
                #     \"MemorySizeInMB\": <code class="replaceable">value</code> 
                # }
            ]
        }\"
    --stopping-conditions \"<span>{</span>
        \"MaxInvocations\": 1000,
        \"ModelLatencyThresholds\":[
                <span>{</span>
                    \"Percentile\": \"P95\", 
                    \"ValueInMilliseconds\": 100
                }
        ],
        # Change 'Stop' to 'Continue' to let the load test continue if invocations flatten 
        \"FlatInvocations\": \"Stop\"
    }\"</code></pre>
                        </dd>
                    <dt>Amazon SageMaker Studio</dt><dd tab-id="amazon-sagemaker-studio">
                            <p>Create a load test with Studio.</p>
                            <div class="procedure"><ol><li>
                                    <p>In your Studio application, choose the home icon (<span class="inlinemediaobject">
                                    <img src="../../../images/sagemaker/latest/dg/images/studio/icons/house.png" class="aws-docs-img-whiteBg aws-docs-img-xs-padding" alt="Home icon in Studio" />
                                    </span>).</p>
                                </li><li>
                                    <p>In the left sidebar of Studio, choose
                                            <b>Deployments</b>.</p>
                                </li><li>
                                    <p>Choose <b>Inference recommender</b>
                                        from the dropdown list.</p>
                                </li><li>
                                        <p>Choose <b>Create inference recommender
                                            job</b>. A new tab titled <b>Create
                                            inference recommender job</b> opens.</p>
                                </li><li>
                                    <p>Select the name of your model group from the dropdown
                                            <b>Model group</b> field. The list
                                        includes all the model groups registered with the model
                                        registry in your account, including models registered
                                        outside of Studio.</p>
                                </li><li>
                                    <p>Select a model version from the dropdown <b>Model
                                            version</b> field.</p>
                                </li><li>
                                    <p>Choose <b>Continue</b>.</p>
                                </li><li>
                                    <p>Provide a name for the job in the
                                            <b>Name</b> field.</p>
                                </li><li>
                                    <p>(Optional) Provide a description of your job in the
                                            <b>Description</b> field.</p>
                                </li><li>
                                    <p>Choose an IAM role that grants Inference Recommender permission to
                                        access AWS services. You can create a role and attach the
                                            <code class="code">AmazonSageMakerFullAccess</code> IAM managed
                                        policy to accomplish this, or you can let Studio create
                                        a role for you.</p>
                                </li><li>
                                    <p>Choose <b>Stopping Conditions</b> to expand
                                        the available input fields. Provide a set of conditions for
                                        stopping a deployment recommendation. </p>
                                    <ol><li>
                                            <p>Specify the maximum number of requests per minute
                                                expected for the endpoint in the <b>Max
                                                  Invocations Per Minute</b> field.</p>
                                        </li><li>
                                            <p>Specify the model latency threshold in
                                                microseconds in the <b>Model Latency
                                                  Threshold</b> field. The <b>Model
                                                  Latency Threshold</b> depicts the interval
                                                of time taken by a model to respond as viewed from
                                                Inference Recommender. The interval includes the local
                                                communication time taken to send the request and to
                                                fetch the response from the model container and the
                                                time taken to complete the inference in the
                                                container.</p>
                                        </li></ol>
                                </li><li>
                                    <p>Choose <b>Traffic Pattern</b> to expand the
                                        available input fields.</p>
                                    <ol><li>
                                            <p>Set the initial number of virtual users by
                                                specifying an integer in the <b>Initial
                                                  Number of Users</b> field.</p>
                                        </li><li>
                                            <p>Provide an integer number for the <b>Spawn
                                                  Rate</b> field. The spawn rate sets the
                                                number of users created per second.</p>
                                        </li><li>
                                            <p>Set the duration for the phase in seconds by
                                                specifying an integer in the
                                                <b>Duration</b> field.</p>
                                        </li><li>
                                            <p>(Optional) Add additional traffic patterns. To do
                                                so, choose <b>Add</b>.</p>
                                        </li></ol>
                                </li><li>
                                    <p>Choose the <b>Additional</b> setting to
                                        reveal the <b>Max test duration</b> field.
                                        Specify, in seconds, the maximum time a test can take during
                                        a job. New jobs are not scheduled after the defined
                                        duration. This helps ensure jobs that are in progress are
                                        not stopped and that you only view completed jobs.</p>
                                </li><li>
                                    <p>Choose <b>Continue</b>.</p>
                                </li><li>
                                    <p>Choose <b>Selected Instances</b>.</p>
                                </li><li>
                                    <p>In the <b>Instances for benchmarking</b>
                                        field, choose <b>Add instances to test</b>.
                                        Select up to 10 instances for Inference Recommender to use for load
                                        testing.</p>
                                </li><li>
                                    <p>Choose <b>Additional settings</b>.</p>
                                    <ol><li>
                                            <p>Provide an integer that sets an upper limit on the
                                                number of tests a job can make for the <b>Max
                                                  number of tests field</b>. Note that each
                                                endpoint configuration results in a new load
                                                test.</p>
                                        </li><li>
                                            <p>Provide an integer for the <b>Max
                                                  parallel</b> test field. This setting
                                                defines an upper limit on the number of load tests
                                                that can run in parallel.</p>
                                        </li></ol>
                                </li><li>
                                    <p>Choose <b>Submit</b>.</p>
                                    <p>The load test can take up to 2 hours.</p>
                                    <div class="awsdocs-note awsdocs-warning"><div class="awsdocs-note-title"><awsui-icon name="status-warning" variant="error"></awsui-icon><h6>Warning</h6></div><div class="awsdocs-note-text"><p>Do not close
                                            this
                                            tab. If you close this tab, you
                                            cancel the Inference Recommender load test job.</p></div></div>
                                </li></ol></div>
                        </dd>
                    <dt>SageMaker console</dt><dd tab-id="sagemaker-console">
                            <p>Create a custom load test through the SageMaker console by doing the
                                following:</p>
                            <div class="procedure"><ol><li>
                                    <p>Go to the SageMaker console at <a href="https://console.aws.amazon.com/sagemaker/" rel="noopener noreferrer" target="_blank"><span>https://console.aws.amazon.com/sagemaker/</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                                </li><li>
                                    <p>In the left navigation pane, choose
                                        <b>Inference</b>, and then choose
                                        <b>Inference recommender</b>.</p>
                                </li><li>
                                    <p>On the <b>Inference recommender jobs</b>
                                        page, choose <b>Create job</b>.</p>
                                </li><li>
                                    <p>For <b>Step 1: Model
                                        configuration</b>, do the following:</p>
                                    <ol><li>
                                            <p>For <b>Job type</b>,
                                                choose <b>Advanced recommender
                                                    job</b>.</p>
                                        </li><li>
                                            <p>If you’re using a model registered in
                                                the SageMaker model registry, then turn on the
                                                <b>Choose a model from the model
                                                    registry</b> toggle and do the
                                                following:</p>
                                            <ol><li>
                                                    <p>For the <b>Model
                                                        group</b> dropdown list, choose the model
                                                        group in SageMaker model registry where your model
                                                        is.</p>
                                                </li><li>
                                                    <p>For the <b>Model
                                                        version</b> dropdown list, choose the
                                                        desired version of your model.</p>
                                                </li></ol>
                                        </li><li>
                                            <p>If you’re using a model that you’ve created in
                                                SageMaker, then turn off the <b>Choose a model
                                                  from the model registry</b> toggle and do
                                                the following:</p>
                                            <ol><li>
                                                    <p>For the <b>Model
                                                        name</b> field, enter the name of your
                                                        SageMaker model.</p>
                                                </li></ol>
                                        </li><li>
                                            <p>For <b>IAM role</b>, you can
                                                select an existing AWS IAM role that has the
                                                necessary permissions to create an instance
                                                recommendation job. Alternatively, if you don’t have
                                                an existing role, you can choose <b>Create a
                                                  new role</b> to open the role creation
                                                pop-up, and SageMaker adds the necessary permissions to
                                                the new role that you create.</p>
                                        </li><li>
                                            <p>For <b>S3 bucket for
                                                benchmarking payload</b>, enter the Amazon S3
                                                path to your sample payload archive, which should
                                                contain sample payload files that Inference Recommender uses to benchmark your model on
                                                different instance types.</p>
                                        </li><li>
                                            <p>For <b>Payload content type</b>,
                                                enter the MIME types of your sample payload
                                                data.</p>
                                        </li><li>
                                            <p>For <b>Traffic pattern</b>,
                                                configure phases for the load test by doing the
                                                following:</p>
                                            <ol><li><p>For <b>Initial number of users</b>,
                                                    specify how many concurrent users you want to start with (with
                                                    a minimum of 1 and a maximum of 3).</p></li><li><p>For <b>Spawn rate</b>, specify the number of users to be spawned in a minute for
                                                  the phase (with a minimum of 0 and a maximum of
                                                  3).</p></li><li><p>For <b>Duration (seconds)</b>, specify
                                                    how low the traffic phase should be in seconds (with a minimum of
                                                    120 and a maximum of 3600).</p></li></ol>
                                        </li><li>
                                            <p>(Optional) If you turned off the
                                                <b>Choose a model from the model registry
                                                    toggle</b> and specified a SageMaker model,
                                                then for <b>Container
                                                    configuration</b>, do the following:</p>
                                            <ol><li>
                                                    <p>For the
                                                        <b>Domain</b> dropdown list, select
                                                        the machine learning domain of the model, such as
                                                        computer vision, natural language processing, or
                                                        machine learning.</p>
                                                </li><li>
                                                    <p>For the
                                                        <b>Framework</b> dropdown list,
                                                        select the framework of your container, such as
                                                        TensorFlow or XGBoost.</p>
                                                </li><li>
                                                    <p>For
                                                        <b>Framework version</b>, enter the
                                                        framework version of your container image.</p>
                                                </li><li>
                                                    <p>For the
                                                        <b>Nearest model name</b> dropdown
                                                        list, select the pre-trained model that mostly
                                                        closely matches your own.</p>
                                                </li><li>
                                                    <p>For the
                                                        <b>Task</b> dropdown list, select
                                                        the machine learning task that the model
                                                        accomplishes, such as image classification or
                                                        regression.</p>
                                                </li></ol>
                                        </li><li>
                                            <p>(Optional) For <b>Model compilation using
                                                  SageMaker Neo</b>, you can configure the
                                                recommendation job for a model that you’ve compiled
                                                using SageMaker Neo. For <b>Data input
                                                  configuration</b>, enter the correct input
                                                data shape for your model in a format similar to
                                                  <code class="code"><span>{</span>'input':[1,1024,1024,3]}</code>.</p>
                                        </li><li>
                                            <p>Choose
                                                <b>Next</b>.</p>
                                        </li></ol>
                                </li><li>
                                    <p>For <b>Step 2: Instances and environment
                                        parameters</b>, do the following:</p>
                                    <ol><li>
                                            <p>For <b>Select instances for
                                                  benchmarking</b>, select up to 8 instance
                                                types that you want to benchmark against.</p>
                                        </li><li>
                                            <p>(Optional) For <b>Environment parameter
                                                  ranges</b>, you can specify environment
                                                parameters that help optimize your model. Specify
                                                the parameters as <b>Key</b> and
                                                  <b>Value</b> pairs.</p>
                                        </li><li>
                                            <p>Choose
                                                <b>Next</b>.</p>
                                        </li></ol>
                                </li><li>
                                    <p>For <b>Step 3: Job parameters</b>,
                                        do the following:</p>
                                    <ol><li>
                                            <p>(Optional) For the <b>Job name</b>
                                                field, enter a name for your instance recommendation
                                                job. When you create the job, SageMaker appends a
                                                timestamp to the end of this name.</p>
                                        </li><li>
                                            <p>(Optional) For the <b>Job
                                                description</b> field, enter a description
                                                for the job.</p>
                                        </li><li>
                                            <p>(Optional) For the
                                                <b>Encryption key</b> dropdown list,
                                                choose an AWS KMS key by name or enter its ARN to
                                                encrypt your data.</p>
                                        </li><li>
                                            <p>(Optional) For <b>Max number of
                                                tests</b>, enter the number of test that
                                                you want to run during the recommendation
                                                job.</p>
                                        </li><li>
                                            <p>(Optional) For <b>Max parallel
                                                tests</b>, enter the maximum number of
                                                parallel tests that you want to run during the
                                                recommendation job.</p>
                                        </li><li>
                                            <p>For <b>Max test
                                                duration (s)</b>, enter the maximum number of
                                                seconds you want each test to run for.</p>
                                        </li><li>
                                            <p>For <b>Max
                                                invocations per minute</b>, enter the
                                                maximum number of requests per minute the endpoint
                                                can reach before stopping the recommendation job.
                                                After reaching this limit, SageMaker ends the
                                                job.</p>
                                        </li><li>
                                            <p>For <b>P99 Model
                                                latency threshold (ms)</b>, enter the model
                                                latency percentile in milliseconds.</p>
                                        </li><li>
                                            <p>Choose
                                                <b>Next</b>.</p>
                                        </li></ol>
                                </li><li>
                                    <p>For <b>Step 4: Review job</b>, review your
                                        configurations and then choose
                                        <b>Submit</b>.</p>
                                </li></ol></div>
                        </dd>
                </dl></awsdocs-tabs>
             
                <h2 id="load-test-describe">Get your load test results</h2>
                <p>You can programmatically collect metrics across all load tests once the load
                    tests are done with AWS SDK for Python (Boto3), the AWS CLI, Studio, or the SageMaker
                    console.</p>
                <awsdocs-tabs><dl style="display: none">
                    <dt>AWS SDK for Python (Boto3)</dt><dd tab-id="aws-sdk-for-python-(boto3)">
                            <p>Collect metrics with the
                                    <code class="code">DescribeInferenceRecommendationsJob</code> API. Specify
                                the job name of the load test for the <code class="code">JobName</code>
                                field:</p>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">load_test_response = sagemaker_client.describe_inference_recommendations_job(
                                                        JobName=load_test_job_name
                                                        )</code></pre>
                            <p>Print the response object.</p>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">load_test_response['Status']</code></pre>
                            <p>This returns a JSON response similar to the following example.
                                Note that this example shows the recommended instance types for
                                real-time inference (for an example showing serverless inference
                                recommendations, see the example after this one).</p>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python "><span>{</span>
    'JobName': <code class="replaceable">'job-name'</code>, 
    'JobDescription': <code class="replaceable">'job-description'</code>, 
    'JobType': 'Advanced', 
    'JobArn': 'arn:aws:sagemaker:<code class="replaceable">region</code>:<code class="replaceable">account-id</code>:inference-recommendations-job/<code class="replaceable">resource-id</code>', 
    'Status': 'COMPLETED', 
    'CreationTime': datetime.datetime(2021, 10, 26, 19, 38, 30, 957000, tzinfo=tzlocal()), 
    'LastModifiedTime': datetime.datetime(2021, 10, 26, 19, 46, 31, 399000, tzinfo=tzlocal()), 
    'InputConfig': <span>{</span>
        'ModelPackageVersionArn': 'arn:aws:sagemaker:<code class="replaceable">region</code>:<code class="replaceable">account-id</code>:model-package/<code class="replaceable">resource-id</code>', 
        'JobDurationInSeconds': 7200, 
        'TrafficPattern': <span>{</span>
            'TrafficType': 'PHASES'
            }, 
        'ResourceLimit': <span>{</span>
            'MaxNumberOfTests': 100, 
            'MaxParallelOfTests': 100
            }, 
        'EndpointConfigurations': [<span>{</span>
            'InstanceType': 'ml.c5d.xlarge'
            }]
        }, 
    'StoppingConditions': <span>{</span>
        'MaxInvocations': 1000, 
        'ModelLatencyThresholds': [<span>{</span>
            'Percentile': 'P95', 
            'ValueInMilliseconds': 100}
            ]}, 
    'InferenceRecommendations': [<span>{</span>
        'Metrics': <span>{</span>
            'CostPerHour': 0.6899999976158142, 
            'CostPerInference': 1.0332434612791985e-05, 
            'MaximumInvocations': 1113, 
            'ModelLatency': 100000
            }, 
    'EndpointConfiguration': <span>{</span>
        'EndpointName': <code class="replaceable">'endpoint-name'</code>, 
        'VariantName': <code class="replaceable">'variant-name'</code>, 
        'InstanceType': 'ml.c5d.xlarge', 
        'InitialInstanceCount': 3
        }, 
    'ModelConfiguration': <span>{</span>
        'Compiled': False, 
        'EnvironmentParameters': []
        }
    }], 
    'ResponseMetadata': <span>{</span>
        'RequestId': <code class="replaceable">'request-id'</code>, 
        'HTTPStatusCode': 200, 
        'HTTPHeaders': <span>{</span>
            'x-amzn-requestid': <code class="replaceable">'x-amzn-requestid'</code>, 
            'content-type': <code class="replaceable">'content-type'</code>, 
            'content-length': '1199', 
            'date': 'Tue, 26 Oct 2021 19:57:42 GMT'
            }, 
        'RetryAttempts': 0}
    }</code></pre>
                            <p>The first few lines provide information about the load test job
                                itself. This includes the job name, role ARN, creation, and deletion
                                time. </p>
                            <p>The <code class="code">InferenceRecommendations</code> dictionary contains a
                                list of Inference Recommender inference recommendations.</p>
                            <p>The <code class="code">EndpointConfiguration</code> nested dictionary contains
                                the instance type (<code class="code">InstanceType</code>) recommendation along
                                with the endpoint and variant name (a deployed AWS machine
                                learning model) used during the recommendation job. You can use the
                                endpoint and variant name for monitoring in Amazon CloudWatch Events. See <a href="monitoring-cloudwatch.html">Monitor Amazon SageMaker with Amazon CloudWatch</a> for more
                                information.</p>
                            <p>The <code class="code">EndpointConfiguration</code> nested dictionary also
                                contains the instance count (<code class="code">InitialInstanceCount</code>)
                                recommendation. This is the number of instances that you should
                                provision in the endpoint to meet the <code class="code">MaxInvocations</code>
                                specified in the <code class="code">StoppingConditions</code>. For example, if
                                the <code class="code">InstanceType</code> is <code class="code">ml.m5.large</code> and the
                                    <code class="code">InitialInstanceCount</code> is <code class="code">2</code>, then you
                                should provision 2 <code class="code">ml.m5.large</code> instances for your
                                endpoint so that it can handle the TPS specified in the
                                    <code class="code">MaxInvocations</code> stopping condition.</p>
                            <p>The <code class="code">Metrics</code> nested dictionary contains information
                                about the estimated cost per hour (<code class="code">CostPerHour</code>) for
                                your real-time endpoint in US dollars, the estimated cost per
                                inference (<code class="code">CostPerInference</code>) for your real-time
                                endpoint, the maximum number of <code class="code">InvokeEndpoint</code> requests
                                sent to the endpoint, and the model latency
                                    (<code class="code">ModelLatency</code>), which is the interval of time (in
                                microseconds) that your model took to respond to SageMaker. The model
                                latency includes the local communication times taken to send the
                                request and to fetch the response from the model container and the
                                time taken to complete the inference in the container.</p>
                            <p>The following example shows the
                                    <code class="code">InferenceRecommendations</code> part of the response for a
                                load test job that was configured to return serverless inference
                                recommendations:</p>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="json ">"InferenceRecommendations": [ 
      <span>{</span> 
         "EndpointConfiguration": <span>{</span> 
            "EndpointName": "<code class="replaceable">value</code>",
            "InitialInstanceCount": <code class="replaceable">value</code>,
            "InstanceType": "<code class="replaceable">value</code>",
            "VariantName": "<code class="replaceable">value</code>",
            "ServerlessConfig": <span>{</span>
                "MaxConcurrency": <code class="replaceable">value</code>,
                "MemorySizeInMb": <code class="replaceable">value</code>
            }
         },
         "InvocationEndTime": <code class="replaceable">value</code>,
         "InvocationStartTime": <code class="replaceable">value</code>,
         "Metrics": <span>{</span> 
            "CostPerHour": <code class="replaceable">value</code>,
            "CostPerInference": <code class="replaceable">value</code>,
            "CpuUtilization": <code class="replaceable">value</code>,
            "MaxInvocations": <code class="replaceable">value</code>,
            "MemoryUtilization": <code class="replaceable">value</code>,
            "ModelLatency": <code class="replaceable">value</code>,
            "ModelSetupTime": <code class="replaceable">value</code>
         },
         "ModelConfiguration": <span>{</span> 
            "Compiled": "False",
            "EnvironmentParameters": [],
            "InferenceSpecificationName": "<code class="replaceable">value</code>"
         },
         "RecommendationId": "<code class="replaceable">value</code>"
      }
   ]</code></pre>
                            <p>You can interpret the recommendations for serverless inference
                                similarly to the results for real-time inference, with the exception
                                of the <code class="code">ServerlessConfig</code>, which tells you the values you
                                specified for <code class="code">MaxConcurrency</code> and <code class="code">MemorySizeInMB</code>
                                when setting up the load test. Serverless recommendations
                                also measure the metric <code class="code">ModelSetupTime</code>, which measures
                                (in microseconds) the time it takes to launch compute resources on a
                                serverless endpoint. For more information about setting up
                                serverless endpoints, see the <a href="serverless-endpoints.html">Serverless Inference
                                    documentation</a>.</p>
                        </dd>
                    <dt>AWS CLI</dt><dd tab-id="aws-cli">
                            <p>Collect metrics with the
                                    <code class="code">describe-inference-recommendations-job</code> API. Specify
                                the job name of the load test for the <code class="code">job-name</code>
                                flag:</p>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="bash ">aws sagemaker describe-inference-recommendations-job --job-name <code class="replaceable">&lt;job-name&gt;</code></code></pre>
                            <p>This returns a response similar to the following. Note that this
                                example shows the recommended instance types for real-time inference
                                (for an example showing serverless inference recommendations, see
                                the example after this one).</p>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="bash "><span>{</span>
    'JobName': <code class="replaceable">'job-name'</code>, 
    'JobDescription': <code class="replaceable">'job-description'</code>, 
    'JobType': 'Advanced', 
    'JobArn': 'arn:aws:sagemaker:<code class="replaceable">region</code>:<code class="replaceable">account-id</code>:inference-recommendations-job/<code class="replaceable">resource-id</code>', 
    'Status': 'COMPLETED', 
    'CreationTime': datetime.datetime(2021, 10, 26, 19, 38, 30, 957000, tzinfo=tzlocal()), 
    'LastModifiedTime': datetime.datetime(2021, 10, 26, 19, 46, 31, 399000, tzinfo=tzlocal()), 
    'InputConfig': <span>{</span>
        'ModelPackageVersionArn': 'arn:aws:sagemaker:<code class="replaceable">region</code>:<code class="replaceable">account-id</code>:model-package/<code class="replaceable">resource-id</code>', 
        'JobDurationInSeconds': 7200, 
        'TrafficPattern': <span>{</span>
            'TrafficType': 'PHASES'
            }, 
        'ResourceLimit': <span>{</span>
            'MaxNumberOfTests': 100, 
            'MaxParallelOfTests': 100
            }, 
        'EndpointConfigurations': [<span>{</span>
            'InstanceType': 'ml.c5d.xlarge'
            }]
        }, 
    'StoppingConditions': <span>{</span>
        'MaxInvocations': 1000, 
        'ModelLatencyThresholds': [<span>{</span>
            'Percentile': 'P95', 
            'ValueInMilliseconds': 100
            }]
        }, 
    'InferenceRecommendations': [<span>{</span>
        'Metrics': <span>{</span>
        'CostPerHour': 0.6899999976158142, 
        'CostPerInference': 1.0332434612791985e-05, 
        'MaximumInvocations': 1113, 
        'ModelLatency': 100000
        }, 
        'EndpointConfiguration': <span>{</span>
            'EndpointName': <code class="replaceable">'endpoint-name'</code>, 
            'VariantName': <code class="replaceable">'variant-name'</code>, 
            'InstanceType': 'ml.c5d.xlarge', 
            'InitialInstanceCount': 3
            }, 
        'ModelConfiguration': <span>{</span>
            'Compiled': False, 
            'EnvironmentParameters': []
            }
        }], 
    'ResponseMetadata': <span>{</span>
        'RequestId': <code class="replaceable">'request-id'</code>, 
        'HTTPStatusCode': 200, 
        'HTTPHeaders': <span>{</span>
            'x-amzn-requestid': <code class="replaceable">'x-amzn-requestid'</code>, 
            'content-type': <code class="replaceable">'content-type'</code>, 
            'content-length': '1199', 
            'date': 'Tue, 26 Oct 2021 19:57:42 GMT'
            }, 
        'RetryAttempts': 0
        }
    }</code></pre>
                            <p>The first few lines provide information about the load test job
                                itself. This includes the job name, role ARN, creation, and deletion
                                time. </p>
                            <p>The <code class="code">InferenceRecommendations</code> dictionary contains a
                                list of Inference Recommender inference recommendations.</p>
                            <p>The <code class="code">EndpointConfiguration</code> nested dictionary contains
                                the instance type (<code class="code">InstanceType</code>) recommendation along
                                with the endpoint and variant name (a deployed AWS machine
                                learning model) used during the recommendation job. You can use the
                                endpoint and variant name for monitoring in Amazon CloudWatch Events. See <a href="monitoring-cloudwatch.html">Monitor Amazon SageMaker with Amazon CloudWatch</a> for more
                                information.</p>
                            <p>The <code class="code">Metrics</code> nested dictionary contains information
                                about the estimated cost per hour (<code class="code">CostPerHour</code>) for
                                your real-time endpoint in US dollars, the estimated cost per
                                inference (<code class="code">CostPerInference</code>) for your real-time
                                endpoint, the maximum number of <code class="code">InvokeEndpoint</code> requests
                                sent to the endpoint, and the model latency
                                    (<code class="code">ModelLatency</code>), which is the interval of time (in
                                microseconds) that your model took to respond to SageMaker. The model
                                latency includes the local communication times taken to send the
                                request and to fetch the response from the model container and the
                                time taken to complete the inference in the container.</p>
                            <p>The following example shows the
                                    <code class="code">InferenceRecommendations</code> part of the response for a
                                load test job that was configured to return serverless inference
                                recommendations:</p>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="json ">"InferenceRecommendations": [ 
      <span>{</span> 
         "EndpointConfiguration": <span>{</span> 
            "EndpointName": "<code class="replaceable">value</code>",
            "InitialInstanceCount": <code class="replaceable">value</code>,
            "InstanceType": "<code class="replaceable">value</code>",
            "VariantName": "<code class="replaceable">value</code>",
            "ServerlessConfig": <span>{</span>
                "MaxConcurrency": <code class="replaceable">value</code>,
                "MemorySizeInMb": <code class="replaceable">value</code>
            }
         },
         "InvocationEndTime": <code class="replaceable">value</code>,
         "InvocationStartTime": <code class="replaceable">value</code>,
         "Metrics": <span>{</span> 
            "CostPerHour": <code class="replaceable">value</code>,
            "CostPerInference": <code class="replaceable">value</code>,
            "CpuUtilization": <code class="replaceable">value</code>,
            "MaxInvocations": <code class="replaceable">value</code>,
            "MemoryUtilization": <code class="replaceable">value</code>,
            "ModelLatency": <code class="replaceable">value</code>,
            "ModelSetupTime": <code class="replaceable">value</code>
         },
         "ModelConfiguration": <span>{</span> 
            "Compiled": "False",
            "EnvironmentParameters": [],
            "InferenceSpecificationName": "<code class="replaceable">value</code>"
         },
         "RecommendationId": "<code class="replaceable">value</code>"
      }
   ]</code></pre>
                            <p>You can interpret the recommendations for serverless inference
                                similarly to the results for real-time inference, with the exception
                                of the <code class="code">ServerlessConfig</code>, which tells you the values you
                                specified for <code class="code">MaxConcurrency</code> and
                                    <code class="code">MemorySizeInMB</code> when setting up the load test.
                                Serverless recommendations also measure the metric
                                    <code class="code">ModelSetupTime</code>, which measures (in microseconds)
                                the time it takes to launch computer resources on a serverless
                                endpoint. For more information about setting up serverless
                                endpoints, see the <a href="serverless-endpoints.html">Serverless Inference
                                    documentation</a>.</p>
                        </dd>
                    <dt>Amazon SageMaker Studio</dt><dd tab-id="amazon-sagemaker-studio">
                            <p>The recommendations populate in a new tab called
                                    <b>Inference recommendations</b> within
                                Studio. It can take up to 2 hours for the results to show up.
                                This tab contains <b>Results</b> and
                                    <b>Details</b> columns.</p>
                            <p>The <b>Details</b> column provides information about
                                the load test job, such as the name given to the load test job, when
                                the job was created (<b>Creation time</b>), and more.
                                It also contains <b>Settings</b> information, such as
                                the maximum number of invocation that occurred per minute and
                                information about the Amazon Resource Names used.</p>
                            <p>The <b>Results</b> column provides <b>
                                    Deployment goals</b> and <b>SageMaker
                                    recommendations</b> windows in which you can adjust the
                                order in which results are displayed based on deployment importance.
                                There are three dropdown menus in which you can provide the level of
                                importance of the <b>Cost</b>,
                                    <b>Latency</b>, and
                                    <b>Throughput</b> for your use case. For each goal
                                (cost, latency, and throughput), you can set the level of
                                importance: <b>Lowest Importance</b>, <b>Low
                                    Importance</b>, <b>Moderate importance</b>,
                                    <b>High importance</b>, or <b>Highest
                                    importance</b>. </p>
                            <p>Based on your selections of importance for each goal, Inference Recommender
                                displays its top recommendation in the <b>SageMaker
                                    recommendation</b> field on the right of the panel, along
                                with the estimated cost per hour and inference request. It also
                                provides Information about the expected model latency, maximum
                                number of invocations, and the number of instances.</p>
                            <p>In addition to the top recommendation displayed, you can also see
                                the same information displayed for all instances that Inference Recommender tested
                                in the <b>All runs</b> section.</p>
                        </dd>
                    <dt>SageMaker console</dt><dd tab-id="sagemaker-console">
                            <p>You can view your custom load test job results in the SageMaker console
                                by doing the following:</p>
                            <div class="procedure"><ol><li>
                                    <p>Go to the SageMaker console at <a href="https://console.aws.amazon.com/sagemaker/" rel="noopener noreferrer" target="_blank"><span>https://console.aws.amazon.com/sagemaker/</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                                </li><li>
                                    <p>In the left navigation pane, choose <b>Inference</b>, and
                                        then choose <b>Inference recommender</b>.</p>
                                </li><li>
                                    <p>On the <b>Inference recommender jobs</b>
                                        page, choose the name of your inference recommendation
                                        job.</p>
                                </li></ol></div>
                            <p>On the details page for your job, you can view the
                                    <b>Inference recommendations</b>, which are the
                                instance types SageMaker recommends for your model, as shown in the
                                following screenshot.</p>
                            <div class="mediaobject">
                                 
                                    <img src="../../../images/sagemaker/latest/dg/images/inf-rec-instant-recs.png" class="aws-docs-img-whiteBg aws-docs-img-padding" alt="&#xA;                                    Screenshot of the inference recommendations list on the job details page in the SageMaker console.&#xA;                                " style="max-width:80%" />
                                 
                                 
                            </div>
                            <p>In this section, you can compare the instance types by various
                                factors such as <b>Model latency</b>,
                                <b>Cost per hour</b>, <b>Cost per inference</b>,
                                and <b>Invocations per minute</b>.</p>
                            <p>On this page, you can also view the configurations you specified
                                for your job. In the <b>Monitor</b>
                                section, you can view the Amazon CloudWatch metrics that were logged
                                for each instance type. To learn more about interpreting these
                                metrics, see <a href="inference-recommender-interpret-results.html">Interpret results</a>.</p>
                        </dd>
                </dl></awsdocs-tabs>
             
                <h2 id="load-test-stop">Stop your load test</h2>
                <p>Stop your load test jobs programmatically with the
                        <code class="code">StopInferenceRecommendationsJob</code> API, or through Studio or the SageMaker console.</p>
                
                <awsdocs-tabs><dl style="display: none">
                    <dt>AWS SDK for Python (Boto3)</dt><dd tab-id="aws-sdk-for-python-(boto3)"><p>Specify the job name of the load test for the <code class="code">JobName</code> field:</p>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="python ">sagemaker_client.stop_inference_recommendations_job(
                                    JobName=<code class="replaceable">'&lt;INSERT&gt;'</code>
                                    )</code></pre>
                        </dd>
                    <dt>AWS CLI</dt><dd tab-id="aws-cli"><p>Specify the job name of the load test for the <code class="code">job-name</code> flag:</p>
                            <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="bash ">aws sagemaker stop-inference-recommendations-job --job-name <code class="replaceable">&lt;job-name&gt;</code></code></pre>
                        </dd>
                    <dt>Amazon SageMaker Studio</dt><dd tab-id="amazon-sagemaker-studio"><p>Close the tab where you initiated your custom load job to stop your Inference Recommender load test.</p></dd>
                    <dt>SageMaker console</dt><dd tab-id="sagemaker-console">
                            <p>To stop your load test job through the SageMaker console, do the following:</p>
                            <div class="procedure"><ol><li>
                                    <p>Go to the SageMaker console at <a href="https://console.aws.amazon.com/sagemaker/" rel="noopener noreferrer" target="_blank"><span>https://console.aws.amazon.com/sagemaker/</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                                </li><li>
                                    <p>In the left navigation pane, choose <b>Inference</b>, and
                                        then choose <b>Inference recommender</b>.</p>
                                </li><li>
                                    <p>On the <b>Inference recommender jobs</b>
                                        page, select your load test job.</p>
                                </li><li>
                                    <p>Choose <b>Stop job</b>.</p>
                                </li><li>
                                    <p>In the dialog box that pops up, choose <b>Confirm</b>.</p>
                                </li></ol></div>
                            <p>After stopping your job, the job’s <b>Status</b> should change to <b>Stopping</b>.</p>
                        </dd>
                </dl></awsdocs-tabs>
            <awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="https://docs.aws.amazon.com/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./inference-recommender-autoscaling.html">Get autoscaling policy recommendations</div><div id="next" class="next-link" accesskey="n" href="./inference-recommender-troubleshooting.html">Troubleshoot Inference Recommender errors</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/inference-recommender-load-test.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/inference-recommender-load-test.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>