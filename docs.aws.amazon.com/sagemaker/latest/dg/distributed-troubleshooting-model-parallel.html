<!DOCTYPE html>
    <html xmlns="http://www.w3.org/1999/xhtml" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Model Parallel Troubleshooting - Amazon SageMaker</title><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="assets_root" content="/assets" /><meta name="target_state" content="distributed-troubleshooting-model-parallel" /><meta name="default_state" content="distributed-troubleshooting-model-parallel" /><link rel="icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="shortcut icon" type="image/ico" href="../../../assets/images/favicon.ico" /><link rel="canonical" href="distributed-troubleshooting-model-parallel.html" /><meta name="description" content="Troubleshooting information for distributed training in Amazon SageMaker." /><meta name="deployment_region" content="IAD" /><meta name="product" content="Amazon SageMaker" /><meta name="guide" content="Developer Guide" /><meta name="abstract" content="Use Internet Monitor to build, train, and host machine learning models in AWS." /><meta name="guide-locale" content="en_us" /><meta name="tocs" content="toc-contents.json" /><link rel="canonical" href="distributed-troubleshooting-model-parallel.html" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="id-id" /><link rel="alternative" href="https://docs.aws.amazon.com/id_id/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="id" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="de-de" /><link rel="alternative" href="https://docs.aws.amazon.com/de_de/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="de" /><link rel="alternative" href="distributed-troubleshooting-model-parallel.html" hreflang="en-us" /><link rel="alternative" href="distributed-troubleshooting-model-parallel.html" hreflang="en" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="es-es" /><link rel="alternative" href="https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="es" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="fr-fr" /><link rel="alternative" href="https://docs.aws.amazon.com/fr_fr/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="fr" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="it-it" /><link rel="alternative" href="https://docs.aws.amazon.com/it_it/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="it" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="ja-jp" /><link rel="alternative" href="https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="ja" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="ko-kr" /><link rel="alternative" href="https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="ko" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="pt-br" /><link rel="alternative" href="https://docs.aws.amazon.com/pt_br/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="pt" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="zh-cn" /><link rel="alternative" href="https://docs.aws.amazon.com/zh_tw/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" hreflang="zh-tw" /><link rel="alternative" href="distributed-troubleshooting-model-parallel.html" hreflang="x-default" /><meta name="feedback-item" content="SageMaker" /><meta name="this_doc_product" content="Amazon SageMaker" /><meta name="this_doc_guide" content="Developer Guide" /><script defer="" src="../../../assets/r/vendor4.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor3.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/vendor1.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-common.js@version=2021.12.02"></script><script defer="" src="../../../assets/r/awsdocs-doc-page.js@version=2021.12.02"></script><link href="../../../assets/r/vendor4.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-common.css@version=2021.12.02.css" rel="stylesheet" /><link href="../../../assets/r/awsdocs-doc-page.css@version=2021.12.02.css" rel="stylesheet" /><script async="" id="awsc-panorama-bundle" type="text/javascript" src="https://prod.pa.cdn.uis.awsstatic.com/panorama-nav-init.js" data-config="{'appEntity':'aws-documentation','region':'us-east-1','service':'sagemaker'}"></script><meta id="panorama-serviceSubSection" value="Developer Guide" /><meta id="panorama-serviceConsolePage" value="Model Parallel Troubleshooting" /></head><body class="awsdocs awsui"><div class="awsdocs-container"><awsdocs-header></awsdocs-header><awsui-app-layout id="app-layout" class="awsui-util-no-gutters" ng-controller="ContentController as $ctrl" header-selector="awsdocs-header" navigation-hide="false" navigation-width="$ctrl.navWidth" navigation-open="$ctrl.navOpen" navigation-change="$ctrl.onNavChange($event)" tools-hide="$ctrl.hideTools" tools-width="$ctrl.toolsWidth" tools-open="$ctrl.toolsOpen" tools-change="$ctrl.onToolsChange($event)"><div id="guide-toc" dom-region="navigation"><awsdocs-toc></awsdocs-toc></div><div id="main-column" dom-region="content" tabindex="-1"><awsdocs-view class="awsdocs-view"><div id="awsdocs-content"><head><title>Model Parallel Troubleshooting - Amazon SageMaker</title><meta name="pdf" content="/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#distributed-troubleshooting-model-parallel" /><meta name="rss" content="amazon-sagemaker-release-notes.rss" /><meta name="forums" content="http://forums.aws.amazon.com/forum.jspa?forumID=285" /><meta name="feedback" content="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" /><meta name="feedback-yes" content="feedbackyes.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" /><meta name="feedback-no" content="feedbackno.html?topic_url=http://docs.aws.amazon.com/en_us/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html" /><meta name="keywords" content="SageMaker,Amazon SageMaker,machine learning,notebook instance" /><script type="application/ld+json">
{
    "@context" : "https://schema.org",
    "@type" : "BreadcrumbList",
    "itemListElement" : [
      {
        "@type" : "ListItem",
        "position" : 1,
        "name" : "AWS",
        "item" : "https://aws.amazon.com"
      },
      {
        "@type" : "ListItem",
        "position" : 2,
        "name" : "Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/index.html"
      },
      {
        "@type" : "ListItem",
        "position" : 3,
        "name" : "Developer Guide",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg"
      },
      {
        "@type" : "ListItem",
        "position" : 4,
        "name" : "Train machine learning models",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/train-model.html"
      },
      {
        "@type" : "ListItem",
        "position" : 5,
        "name" : "Distributed training in Amazon SageMaker",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html"
      },
      {
        "@type" : "ListItem",
        "position" : 6,
        "name" : "SageMaker's Model Parallelism Library",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html"
      },
      {
        "@type" : "ListItem",
        "position" : 7,
        "name" : "Model Parallel Troubleshooting",
        "item" : "https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html"
      }
    ]
}
</script></head><body><div id="main"><div style="display: none"><a href="https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#distributed-troubleshooting-model-parallel" target="_blank" rel="noopener noreferrer" title="Open PDF"></a></div><div id="breadcrumbs" class="breadcrumb"><a href="https://aws.amazon.com">AWS</a><a href="https://docs.aws.amazon.com/index.html">Documentation</a><a href="https://docs.aws.amazon.com/sagemaker/index.html">Amazon SageMaker</a><a href="whatis.html">Developer Guide</a></div><div id="page-toc-src"><a href="distributed-troubleshooting-model-parallel.html#distributed-ts-model-parallel-debugger">Considerations for Using SageMaker
                Debugger with the SageMaker Model Parallelism Library</a><a href="distributed-troubleshooting-model-parallel.html#distributed-ts-model-parallel-checkpoints">Saving Checkpoints</a><a href="distributed-troubleshooting-model-parallel.html#distributed-ts-model-parallel-tf-convergence">Convergence Using Model
                Parallel and TensorFlow</a><a href="distributed-troubleshooting-model-parallel.html#distributed-ts-model-parallel-training-issues">Stalling or Crashing
                Distributed Training Jobs</a><a href="distributed-troubleshooting-model-parallel.html#distributed-ts-model-parallel-nccl-error">Receiving NCCL Error for a
                PyTorch Training Job</a><a href="distributed-troubleshooting-model-parallel.html#distributed-ts-model-parallel-super-forward-not-supported">Receiving
                    RecursionError for a PyTorch Training Job</a></div><div id="main-content" class="awsui-util-container"><div id="main-col-body"><awsdocs-language-banner data-service="$ctrl.pageService"></awsdocs-language-banner><h1 class="topictitle" id="distributed-troubleshooting-model-parallel">Model Parallel
            Troubleshooting</h1><div class="awsdocs-page-header-container"><awsdocs-page-header></awsdocs-page-header><awsdocs-filter-selector id="awsdocs-filter-selector"></awsdocs-filter-selector></div><p>If you run into an error, you can use the following list to try to troubleshoot your
        training job. If the problem persists, contact <a href="http://aws.amazon.com/premiumsupport" rel="noopener noreferrer" target="_blank"><span>AWS Support</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>. </p><div class="highlights" id="inline-topiclist"><h6>Topics</h6><ul><li><a href="distributed-troubleshooting-model-parallel.html#distributed-ts-model-parallel-debugger">Considerations for Using SageMaker
                Debugger with the SageMaker Model Parallelism Library</a></li><li><a href="distributed-troubleshooting-model-parallel.html#distributed-ts-model-parallel-checkpoints">Saving Checkpoints</a></li><li><a href="distributed-troubleshooting-model-parallel.html#distributed-ts-model-parallel-tf-convergence">Convergence Using Model
                Parallel and TensorFlow</a></li><li><a href="distributed-troubleshooting-model-parallel.html#distributed-ts-model-parallel-training-issues">Stalling or Crashing
                Distributed Training Jobs</a></li><li><a href="distributed-troubleshooting-model-parallel.html#distributed-ts-model-parallel-nccl-error">Receiving NCCL Error for a
                PyTorch Training Job</a></li><li><a href="distributed-troubleshooting-model-parallel.html#distributed-ts-model-parallel-super-forward-not-supported">Receiving
                    RecursionError for a PyTorch Training Job</a></li></ul></div>
        <h2 id="distributed-ts-model-parallel-debugger">Considerations for Using SageMaker
                Debugger with the SageMaker Model Parallelism Library</h2>
        <p>SageMaker Debugger is not available for the SageMaker model parallelism library. Debugger is enabled
            by default for all SageMaker TensorFlow and PyTorch training jobs, and you might see an error
            that looks like the following: </p>
        <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="txt ">FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/checkpoints/metadata.json.sagemaker-uploading</code></pre>
        <p>To fix this issue, disable Debugger by passing <code class="code">debugger_hook_config=False</code>
            when creating a framework <code class="code">estimator</code> as shown in the following
            example.</p>
        <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="py ">bucket=sagemaker.Session().default_bucket()
base_job_name="sagemaker-checkpoint-test"
checkpoint_in_bucket="checkpoints"

# The S3 URI to store the checkpoints
checkpoint_s3_bucket="s3://<span>{</span>}/<span>{</span>}/<span>{</span>}".format(bucket, base_job_name, checkpoint_in_bucket)

estimator = TensorFlow(
    ...

    distribution=<span>{</span>"smdistributed": <span>{</span>"modelparallel": <span>{</span> "enabled": True }}},
    checkpoint_s3_uri=checkpoint_s3_bucket,
    checkpoint_local_path="/opt/ml/checkpoints",
    debugger_hook_config=False
)</code></pre>
     
        <h2 id="distributed-ts-model-parallel-checkpoints">Saving Checkpoints</h2>
        <p>You might run into the following error when saving checkpoints of a large model on
            SageMaker: </p>
        <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="txt ">InternalServerError: We encountered an internal error. Please try again</code></pre>
        <p>This could be caused by a SageMaker limitation while uploading the local checkpoint to Amazon S3
            during training. To disable checkpointing in SageMaker, use the following example to
            explicitly upload the checkpoints.</p>
        <p>If you run into the preceding error, do not use <code class="code">checkpoint_s3_uri</code> with
            the SageMaker <code class="code">estimator</code> call. While saving checkpoints for larger models, we
            recommend saving checkpoints to a custom directory and passing the same to the helper
            function (as a <code class="code">local_path</code> argument).</p>
        <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="py ">import os

def aws_s3_sync(source, destination):
    """aws s3 sync in quiet mode and time profile"""
    import time, subprocess
    cmd = ["aws", "s3", "sync", "--quiet", source, destination]
    print(f"Syncing files from <span>{</span>source} to <span>{</span>destination}")
    start_time = time.time()
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    p.wait()
    end_time = time.time()
    print("Time Taken to Sync: ", (end_time-start_time))
    return

def sync_local_checkpoints_to_s3(local_path="/opt/ml/checkpoints", s3_uri=os.path.dirname(os.path.dirname(os.getenv('SM_MODULE_DIR', '')))+'/checkpoints'):
    """ sample function to sync checkpoints from local path to s3 """

    import boto3
    #check if local path exists
    if not os.path.exists(local_path):
        raise RuntimeError("Provided local path <span>{</span>local_path} does not exist. Please check")

    #check if s3 bucket exists
    s3 = boto3.resource('s3')
    if not s3_uri.startswith("s3://"):
        raise ValueError(f"Provided s3 uri <span>{</span>s3_uri} is not valid.")

    s3_bucket = s3_uri.replace('s3://','').split('/')[0]
    print(f"S3 Bucket: <span>{</span>s3_bucket}")
    try:
        s3.meta.client.head_bucket(Bucket=s3_bucket)
    except Exception as e:
        raise e
    aws_s3_sync(local_path, s3_uri)
    return

def sync_s3_checkpoints_to_local(local_path="/opt/ml/checkpoints", s3_uri=os.path.dirname(os.path.dirname(os.getenv('SM_MODULE_DIR', '')))+'/checkpoints'):
    """ sample function to sync checkpoints from s3 to local path """

    import boto3
    #try to create local path if it does not exist
    if not os.path.exists(local_path):
        print(f"Provided local path <span>{</span>local_path} does not exist. Creating...")
        try:
            os.makedirs(local_path)
        except Exception as e:
            raise RuntimeError(f"Failed to create <span>{</span>local_path}")

    #check if s3 bucket exists
    s3 = boto3.resource('s3')
    if not s3_uri.startswith("s3://"):
        raise ValueError(f"Provided s3 uri <span>{</span>s3_uri} is not valid.")

    s3_bucket = s3_uri.replace('s3://','').split('/')[0]
    print(f"S3 Bucket: <span>{</span>s3_bucket}")
    try:
        s3.meta.client.head_bucket(Bucket=s3_bucket)
    except Exception as e:
        raise e
    aws_s3_sync(s3_uri, local_path)
    return
</code></pre>
        <p>Usage of helper functions:</p>
        <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="py ">#base_s3_uri - user input s3 uri or save to model directory (default)
#curr_host - to save checkpoints of current host
#iteration - current step/epoch during which checkpoint is saved

# save checkpoints on every node using local_rank
if smp.local_rank() == 0:
    base_s3_uri = os.path.dirname(os.path.dirname(os.getenv('SM_MODULE_DIR', '')))
    curr_host = os.environ['SM_CURRENT_HOST']
    full_s3_uri = f'<span>{</span>base_s3_uri}/checkpoints/<span>{</span>curr_host}/<span>{</span>iteration}'
    sync_local_checkpoints_to_s3(local_path=checkpoint_dir, s3_uri=full_s3_uri)</code></pre>
     
        <h2 id="distributed-ts-model-parallel-tf-convergence">Convergence Using Model
                Parallel and TensorFlow</h2>
        <p>When you use SageMaker multi-node training with TensorFlow and the model parallelism
            library, the loss may not converge as expected because the order of training input files
            may be different on each node. This may cause different ranks in the same model parallel
            group to work on different input files, causing inconsistencies. To prevent this, ensure
            the input files are ordered the same way in all the ranks before they get converted to
            TensorFlow datasets. One way to achieve this is to sort the input file names in the
            training script.</p>
     
        <h2 id="distributed-ts-model-parallel-training-issues">Stalling or Crashing
                Distributed Training Jobs</h2>
        <p>If your training job has stalling, crashing, or not responding issues, read the
            following troubleshooting items to identify what's the cause of the issue. If you need
            any further support, reach out to the SageMaker distributed training team through <a href="http://aws.amazon.com/premiumsupport" rel="noopener noreferrer" target="_blank"><span>AWS Support</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
        <div class="itemizedlist">
             
             
             
             
             
             
             
             
        <ul class="itemizedlist"><li class="listitem">
                <p> If you see <b>a distributed training job stalling at the
                        NCCL initialization step</b>, consider the following: </p>
                <div class="itemizedlist">
                     
                <ul class="itemizedlist"><li class="listitem">
                        <p>If you are using one of the EFA-enabled instances (
                                <code class="code">ml.p4d</code> or <code class="code">ml.p3dn</code> instances) with a custom
                            VPC and its subnet, ensure that the security group used has inbound and
                            outbound connections for all ports to and from the same SG. You also
                            generally need outbound connections to any IP as a separate rule (for
                            internet access). To find instructions on how to add inbound and
                            outbound rules for EFA communication, refer to <a href="distributed-troubleshooting-data-parallel.html#distributed-ts-data-parallel-efa-sg">SageMaker Distributed Training Job
                Stalling During Initialization</a>.</p>
                    </li></ul></div>
            </li><li class="listitem">
                <p>If you see a <b>distributed training job stalling when
                        checkpointing</b> the full model, this might be because the
                        <code class="code">state_dict()</code> call on the model or optimizer was not made on all
                    ranks with <code class="code">rdp_rank()==0</code> (when using tensor parallelism) or
                        <code class="code">dp_rank()==0</code> (when using only pipeline parallelism). These
                    ranks need to communicate to construct the checkpoint to be saved. Similar
                    stalling issues can also happen when checkpointing partial optimizer if
                        <code class="code">shard_optimizer_state</code> is enabled. </p>
                <p>For more information about checkpointing a model with model parallelism, see
                        <a href="https://sagemaker.readthedocs.io/en/stable/api/training/smp_versions/latest/smd_model_parallel_pytorch.html#general-instruction-for-saving-and-loading" rel="noopener noreferrer" target="_blank"><span>General Instruction for Saving and Loading</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a> and <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-model-parallel-checkpointing-and-finetuning.html#model-parallel-extended-features-pytorch-saving-loading-checkpoints">Checkpointing a distributed PyTorch model (for the SageMaker model parallelism
                    library between v1.6.0 and v1.9.0)</a>.</p>
            </li><li class="listitem">
                <p>If the training job crashes with a <b>CUDA Out of Memory
                        error</b>, this means that the distributed training configuration
                    needs to be adjusted to fit the model on the GPU cluster. For more information
                    and best practices, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-best-practices.html#model-parallel-best-practices-configuration">Setting Up the Right
                Configuration for a Given Model</a>.</p>
            </li><li class="listitem">
                <p>If the training job crashes with an <b>uncorrectable <a href="https://docs.nvidia.com/deploy/a100-gpu-mem-error-mgmt/index.html" rel="noopener noreferrer" target="_blank"><span>ECC error</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a></b>, this means that one of the GPUs in the
                    cluster has gone bad. If you need technical support, share the job ARN with the
                    AWS team and restart your training job from a checkpoint if possible.</p>
            </li><li class="listitem">
                <p>In rare cases, a job configuration that worked previously but is close to the
                    limits of GPU memory might fail later with a different cluster due to a
                        <b>CUDA Out of Memory error</b>. This could be
                    because some GPU has lower available memory than usual due to ECC errors.</p>
            </li><li class="listitem">
                <p><b>Network timeout crash</b> might happen when
                    running a multinode job which doesnâ€™t use all GPUs in the node. To get around
                    this, use all GPUs on the node by ensuring that the
                        <code class="code">processes_per_host</code> parameter is set to the number of GPUs in
                    each instance. For example, this is <code class="code">processes_per_host=8</code> for
                        <code class="code">ml.p3.16xlarge</code>, <code class="code">ml.p3dn.24xlarge</code>, and
                        <code class="code">ml.p4d.24xlarge</code> instances.</p>
            </li><li class="listitem">
                <p>If you find that your training job takes a long time during the data
                    downloading stage, make sure the Amazon S3 path you provided to
                        <code class="code">checkpoint_s3_uri</code> for the SageMaker <code class="code">Estimator</code> class is
                    unique for the current training job. If this path is reused across multiple
                    training jobs running simultaneously, all those checkpoints are uploaded and
                    downloaded to the same Amazon S3 path and might significantly increase checkpoint
                    loading time.</p>
            </li><li class="listitem">
                <p>Use FSx for Lustre when you deal with large data and models.</p>
                <div class="itemizedlist">
                     
                     
                    
                     
                <ul class="itemizedlist"><li class="listitem">
                        <p>If your dataset is large and fetching it takes a long time, we
                            recommend keeping your dataset in <a href="http://aws.amazon.com/fsx/lustre/" rel="noopener noreferrer" target="_blank"><span>FSx for Lustre</span><awsui-icon class="awsdocs-link-icon" name="external"></awsui-icon></a>.</p>
                    </li><li class="listitem">
                        <p>When training models are beyond 10 billion parameters, we recommend
                            using FSx for Lustre for checkpointing.</p>
                    </li><li class="listitem">
                        <p>After you create a file system, make sure to wait for the status to
                            become <b>available</b> before starting a
                            training job using it. </p>
                    </li></ul></div>
            </li></ul></div>
     
        <h2 id="distributed-ts-model-parallel-nccl-error">Receiving NCCL Error for a
                PyTorch Training Job</h2>
        <p>If you encountered the following error, it might be due to a process running out of
            GPU memory.</p>
        <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="">NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:825, unhandled system error, NCCL version 2.7.8
ncclSystemError: System call (socket, malloc, munmap, etc) failed.</code></pre>
        <p>You can resolve this by reducing the batch size or <code class="code">active_microbatches</code>.
            If auto partitioning is not resulting in a well-balanced partitioning, you might have to
            consider manual partitioning. For more information, see <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-best-practices.html#model-parallel-best-practices-configuration-pipeline-across-nodes">Pipeline parallelism across nodes</a>.</p>
     
        <h2 id="distributed-ts-model-parallel-super-forward-not-supported">Receiving
                    <code class="code">RecursionError</code> for a PyTorch Training Job</h2>
        <p>The library does not support calling <code class="code">super.forward()</code> inside a module's
            forward call. If you use <code class="code">super.forward()</code>, you might receive the following
            error message. </p>
        <pre class="programlisting"><div class="code-btn-container"><div class="btn-copy-code" title="Copy"><awsui-icon name="copy"></awsui-icon></div></div><code class="">RecursionError: maximum recursion depth exceeded</code></pre>
        <p>To fix the error, instead of calling <code class="code">super.forward()</code>, you should call
                <code class="code">super()._orig_forward()</code>. </p>
    <awsdocs-copyright class="copyright-print"></awsdocs-copyright><awsdocs-thumb-feedback right-edge="{{$ctrl.thumbFeedbackRightEdge}}"></awsdocs-thumb-feedback></div><noscript><div><div><div><div id="js_error_message"><p><img src="https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png" alt="Warning" /> <strong>Javascript is disabled or is unavailable in your browser.</strong></p><p>To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.</p></div></div></div></div></noscript><div id="main-col-footer" class="awsui-util-font-size-0"><div id="doc-conventions"><a target="_top" href="https://docs.aws.amazon.com/general/latest/gr/docconventions.html">Document Conventions</a></div><div class="prev-next"><div id="previous" class="prev-link" accesskey="p" href="./model-parallel-customize-tips-pitfalls.html">Configuration Tips and Pitfalls</div><div id="next" class="next-link" accesskey="n" href="./distributed-training-notebook-examples.html">SageMaker Distributed Training Notebook Examples</div></div></div><awsdocs-page-utilities></awsdocs-page-utilities></div><div id="quick-feedback-yes" style="display: none;"><div class="title">Did this page help you? - Yes</div><div class="content"><p>Thanks for letting us know we're doing a good job!</p><p>If you've got a moment, please tell us what we did right so we can do more of it.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html"></awsui-button></p></div></div><div id="quick-feedback-no" style="display: none;"><div class="title">Did this page help you? - No</div><div class="content"><p>Thanks for letting us know this page needs work. We're sorry we let you down.</p><p>If you've got a moment, please tell us how we can make the documentation better.</p><p><awsui-button id="fblink" rel="noopener noreferrer" target="_blank" text="Feedback" click="linkClick($event)" href="https://docs.aws.amazon.com/forms/aws-doc-feedback?hidden_service_name=SageMaker&amp;topic_url=https://docs.aws.amazon.com/en_us/sagemaker/latest/dg/distributed-troubleshooting-model-parallel.html"></awsui-button></p></div></div></div></body></div></awsdocs-view><div class="page-loading-indicator" id="page-loading-indicator"><awsui-spinner size="large"></awsui-spinner></div></div><div id="tools-panel" dom-region="tools"><awsdocs-tools-panel id="awsdocs-tools-panel"></awsdocs-tools-panel></div></awsui-app-layout><awsdocs-cookie-banner class="doc-cookie-banner"></awsdocs-cookie-banner></div></body></html>